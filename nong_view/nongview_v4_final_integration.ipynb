{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Nong-View ê³ ê¸‰ ML ì‹œìŠ¤í…œ ìµœì¢… í†µí•© v4.0 (Part 3)\n",
    "\n",
    "**í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”, ì•™ìƒë¸” í•™ìŠµ, ì „ì²´ ì‹œìŠ¤í…œ í†µí•©**  \n",
    "**ê°œë°œ ë‚ ì§œ**: 2025-10-27  \n",
    "**ë²„ì „**: v4.0 Final\n",
    "\n",
    "## ğŸ“‹ ìµœì¢… í†µí•© ê¸°ëŠ¥\n",
    "- âœ… ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "- âœ… ì•™ìƒë¸” í•™ìŠµ ë° ì§€ì‹ ì¦ë¥˜\n",
    "- âœ… í†µí•© API ì‹œìŠ¤í…œ\n",
    "- âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì‹œê°í™”\n",
    "- âœ… ì¢…í•© í…ŒìŠ¤íŠ¸ ë° ë°ëª¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 8. ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    \"\"\"ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self, n_trials: int = 100, timeout: int = 7200):\n",
    "        self.n_trials = n_trials\n",
    "        self.timeout = timeout  # ì´ˆ ë‹¨ìœ„\n",
    "        \n",
    "    def optimize_hyperparameters(self, db: Session, training_job_id: str, \n",
    "                                 optimization_objective: str = \"map50\") -> dict:\n",
    "        \"\"\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰\"\"\"\n",
    "        \n",
    "        # í›ˆë ¨ ì‘ì—… ì¡°íšŒ\n",
    "        training_job = db.query(TrainingJob).filter(\n",
    "            TrainingJob.id == training_job_id\n",
    "        ).first()\n",
    "        \n",
    "        if not training_job:\n",
    "            raise ValueError(f\"í›ˆë ¨ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {training_job_id}\")\n",
    "        \n",
    "        logger.info(f\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘: {training_job.job_name}\")\n",
    "        \n",
    "        # Optuna ìŠ¤í„°ë”” ìƒì„±\n",
    "        study_name = f\"hyperopt_{training_job_id}_{int(time.time())}\"\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",  # mAP ìµœëŒ€í™”\n",
    "            study_name=study_name,\n",
    "            sampler=TPESampler(seed=42),\n",
    "            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "        )\n",
    "        \n",
    "        # ìµœì í™” ëª©í‘œ í•¨ìˆ˜ ì •ì˜\n",
    "        def objective(trial):\n",
    "            return self._objective_function(\n",
    "                trial, db, training_job, optimization_objective\n",
    "            )\n",
    "        \n",
    "        # í›ˆë ¨ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "        training_job.hyperopt_enabled = True\n",
    "        training_job.hyperopt_study_id = study_name\n",
    "        training_job.status = \"hyperopt_running\"\n",
    "        db.commit()\n",
    "        \n",
    "        try:\n",
    "            # ìµœì í™” ì‹¤í–‰\n",
    "            study.optimize(\n",
    "                objective, \n",
    "                n_trials=self.n_trials,\n",
    "                timeout=self.timeout,\n",
    "                callbacks=[self._optuna_callback(db, training_job_id)]\n",
    "            )\n",
    "            \n",
    "            # ìµœì í™” ê²°ê³¼ ì €ì¥\n",
    "            best_params = study.best_params\n",
    "            best_value = study.best_value\n",
    "            \n",
    "            training_job.optimized_hyperparameters = best_params\n",
    "            training_job.hyperopt_best_score = best_value\n",
    "            training_job.hyperopt_trials = len(study.trials)\n",
    "            training_job.status = \"hyperopt_completed\"\n",
    "            \n",
    "            db.commit()\n",
    "            \n",
    "            # ìµœì í™” ê²°ê³¼ ë¶„ì„\n",
    "            optimization_analysis = self._analyze_optimization_results(study, db, training_job_id)\n",
    "            \n",
    "            result = {\n",
    "                'study_name': study_name,\n",
    "                'best_params': best_params,\n",
    "                'best_value': best_value,\n",
    "                'n_trials': len(study.trials),\n",
    "                'optimization_time': time.time() - study.trials[0].datetime_start.timestamp(),\n",
    "                'analysis': optimization_analysis\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ: {result}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤íŒ¨: {e}\")\n",
    "            training_job.status = \"hyperopt_failed\"\n",
    "            training_job.error_message = str(e)\n",
    "            db.commit()\n",
    "            raise\n",
    "    \n",
    "    def _objective_function(self, trial, db: Session, training_job: TrainingJob, \n",
    "                           optimization_objective: str) -> float:\n",
    "        \"\"\"ìµœì í™” ëª©ì  í•¨ìˆ˜\"\"\"\n",
    "        \n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ\n",
    "        hyperparams = self._suggest_hyperparameters(trial, training_job.model_type)\n",
    "        \n",
    "        # í˜„ì¬ ì‹œí–‰ ì •ë³´ ì €ì¥\n",
    "        hyperopt_trial = HyperoptTrial(\n",
    "            training_job_id=training_job.id,\n",
    "            trial_number=trial.number,\n",
    "            status=\"running\",\n",
    "            hyperparameters=hyperparams\n",
    "        )\n",
    "        db.add(hyperopt_trial)\n",
    "        db.flush()\n",
    "        \n",
    "        try:\n",
    "            # ì¶•ì†Œëœ í›ˆë ¨ ì‹¤í–‰ (ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´)\n",
    "            validation_score = self._quick_train_and_evaluate(\n",
    "                db, training_job, hyperparams, optimization_objective\n",
    "            )\n",
    "            \n",
    "            # ì‹œí–‰ ê²°ê³¼ ì—…ë°ì´íŠ¸\n",
    "            hyperopt_trial.status = \"completed\"\n",
    "            hyperopt_trial.objective_value = validation_score\n",
    "            hyperopt_trial.completed_at = datetime.utcnow()\n",
    "            \n",
    "            db.commit()\n",
    "            \n",
    "            # Pruning ì²´í¬\n",
    "            if trial.should_prune():\n",
    "                hyperopt_trial.is_pruned = True\n",
    "                hyperopt_trial.status = \"pruned\"\n",
    "                db.commit()\n",
    "                raise optuna.TrialPruned()\n",
    "            \n",
    "            return validation_score\n",
    "            \n",
    "        except optuna.TrialPruned:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Trial {trial.number} ì‹¤íŒ¨: {e}\")\n",
    "            hyperopt_trial.status = \"failed\"\n",
    "            hyperopt_trial.completed_at = datetime.utcnow()\n",
    "            db.commit()\n",
    "            return 0.0  # ì‹¤íŒ¨ ì‹œ ìµœì € ì ìˆ˜\n",
    "    \n",
    "    def _suggest_hyperparameters(self, trial, model_type: ModelType) -> dict:\n",
    "        \"\"\"ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ\"\"\"\n",
    "        \n",
    "        if model_type == ModelType.YOLO_DETECTION:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "                'batch_size': trial.suggest_categorical('batch_size', [4, 8, 16, 32]),\n",
    "                'epochs': trial.suggest_int('epochs', 20, 200),\n",
    "                'image_size': trial.suggest_categorical('image_size', [416, 512, 640, 768]),\n",
    "                'optimizer': trial.suggest_categorical('optimizer', ['AdamW', 'SGD', 'Adam']),\n",
    "                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True),\n",
    "                'warmup_epochs': trial.suggest_int('warmup_epochs', 0, 10),\n",
    "                'cos_lr': trial.suggest_categorical('cos_lr', [True, False]),\n",
    "                \n",
    "                # YOLO íŠ¹í™” í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "                'mosaic': trial.suggest_float('mosaic', 0.0, 1.0),\n",
    "                'mixup': trial.suggest_float('mixup', 0.0, 0.3),\n",
    "                'copy_paste': trial.suggest_float('copy_paste', 0.0, 0.3),\n",
    "                'hsv_h': trial.suggest_float('hsv_h', 0.0, 0.1),\n",
    "                'hsv_s': trial.suggest_float('hsv_s', 0.0, 0.9),\n",
    "                'hsv_v': trial.suggest_float('hsv_v', 0.0, 0.9),\n",
    "                'degrees': trial.suggest_float('degrees', 0.0, 45.0),\n",
    "                'translate': trial.suggest_float('translate', 0.0, 0.2),\n",
    "                'scale': trial.suggest_float('scale', 0.0, 0.9),\n",
    "                'shear': trial.suggest_float('shear', 0.0, 10.0),\n",
    "                'perspective': trial.suggest_float('perspective', 0.0, 0.001),\n",
    "                'flipud': trial.suggest_float('flipud', 0.0, 1.0),\n",
    "                'fliplr': trial.suggest_float('fliplr', 0.0, 1.0),\n",
    "                \n",
    "                # ì •ê·œí™” íŒŒë¼ë¯¸í„°\n",
    "                'dropout': trial.suggest_float('dropout', 0.0, 0.5),\n",
    "                'label_smoothing': trial.suggest_float('label_smoothing', 0.0, 0.2)\n",
    "            }\n",
    "        \n",
    "        elif model_type == ModelType.ENSEMBLE:\n",
    "            return {\n",
    "                'ensemble_size': trial.suggest_int('ensemble_size', 3, 7),\n",
    "                'diversity_weight': trial.suggest_float('diversity_weight', 0.1, 0.9),\n",
    "                'voting_method': trial.suggest_categorical('voting_method', ['soft', 'hard', 'weighted']),\n",
    "                'base_models': trial.suggest_categorical('base_models', \n",
    "                    [['yolov8n', 'yolov8s'], ['yolov8s', 'yolov8m'], ['yolov8n', 'yolov8s', 'yolov8m']]\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "                'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32, 64]),\n",
    "                'epochs': trial.suggest_int('epochs', 50, 300)\n",
    "            }\n",
    "    \n",
    "    def _quick_train_and_evaluate(self, db: Session, training_job: TrainingJob, \n",
    "                                 hyperparams: dict, optimization_objective: str) -> float:\n",
    "        \"\"\"ë¹ ë¥¸ í›ˆë ¨ ë° í‰ê°€ (ìµœì í™”ìš©)\"\"\"\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ê²½ë¡œ ì¤€ë¹„\n",
    "        dataset_yaml = Path(training_job.output_dir) / \"dataset\" / \"dataset.yaml\"\n",
    "        if not dataset_yaml.exists():\n",
    "            # ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸°\n",
    "            from nongview_v4_advanced_ml import data_split_engine  # ì´ì „ íŒŒì¼ì—ì„œ import\n",
    "            data_split_engine.export_yolo_format(\n",
    "                db, training_job.dataset_id, \n",
    "                str(dataset_yaml.parent)\n",
    "            )\n",
    "        \n",
    "        # ì¶•ì†Œëœ ì—í¬í¬ë¡œ ë¹ ë¥¸ í›ˆë ¨\n",
    "        quick_epochs = min(hyperparams.get('epochs', 50), 30)  # ìµœëŒ€ 30 ì—í¬í¬\n",
    "        \n",
    "        try:\n",
    "            # YOLO ëª¨ë¸ ì´ˆê¸°í™”\n",
    "            model = YOLO(f\"{training_job.base_model}.pt\")\n",
    "            \n",
    "            # ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "            temp_output = Path(training_job.output_dir) / f\"hyperopt_trial_{int(time.time())}\"\n",
    "            temp_output.mkdir(exist_ok=True)\n",
    "            \n",
    "            # í›ˆë ¨ ì‹¤í–‰\n",
    "            results = model.train(\n",
    "                data=str(dataset_yaml),\n",
    "                epochs=quick_epochs,\n",
    "                batch=hyperparams.get('batch_size', 16),\n",
    "                imgsz=hyperparams.get('image_size', 640),\n",
    "                lr0=hyperparams.get('learning_rate', 0.01),\n",
    "                optimizer=hyperparams.get('optimizer', 'AdamW'),\n",
    "                weight_decay=hyperparams.get('weight_decay', 0.0005),\n",
    "                warmup_epochs=hyperparams.get('warmup_epochs', 3),\n",
    "                cos_lr=hyperparams.get('cos_lr', True),\n",
    "                \n",
    "                # ë°ì´í„° ì¦ê°•\n",
    "                mosaic=hyperparams.get('mosaic', 1.0),\n",
    "                mixup=hyperparams.get('mixup', 0.0),\n",
    "                copy_paste=hyperparams.get('copy_paste', 0.0),\n",
    "                \n",
    "                # ìƒ‰ìƒ ì¦ê°•\n",
    "                hsv_h=hyperparams.get('hsv_h', 0.015),\n",
    "                hsv_s=hyperparams.get('hsv_s', 0.7),\n",
    "                hsv_v=hyperparams.get('hsv_v', 0.4),\n",
    "                \n",
    "                # ê¸°í•˜í•™ì  ì¦ê°•\n",
    "                degrees=hyperparams.get('degrees', 0.0),\n",
    "                translate=hyperparams.get('translate', 0.1),\n",
    "                scale=hyperparams.get('scale', 0.5),\n",
    "                shear=hyperparams.get('shear', 0.0),\n",
    "                perspective=hyperparams.get('perspective', 0.0),\n",
    "                flipud=hyperparams.get('flipud', 0.0),\n",
    "                fliplr=hyperparams.get('fliplr', 0.5),\n",
    "                \n",
    "                # ì •ê·œí™”\n",
    "                dropout=hyperparams.get('dropout', 0.0),\n",
    "                label_smoothing=hyperparams.get('label_smoothing', 0.0),\n",
    "                \n",
    "                project=str(temp_output),\n",
    "                name=\"hyperopt\",\n",
    "                exist_ok=True,\n",
    "                verbose=False,\n",
    "                patience=10,  # ì¡°ê¸° ì¢…ë£Œ\n",
    "                save_period=-1  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì•ˆí•¨\n",
    "            )\n",
    "            \n",
    "            # ê²€ì¦ ê²°ê³¼ ì¶”ì¶œ\n",
    "            if hasattr(results, 'results_dict'):\n",
    "                metrics = results.results_dict\n",
    "            else:\n",
    "                # ê²°ê³¼ íŒŒì¼ì—ì„œ ì½ê¸°\n",
    "                results_file = temp_output / \"hyperopt\" / \"results.csv\"\n",
    "                if results_file.exists():\n",
    "                    results_df = pd.read_csv(results_file)\n",
    "                    if not results_df.empty:\n",
    "                        last_row = results_df.iloc[-1]\n",
    "                        metrics = {\n",
    "                            'metrics/mAP50(B)': last_row.get('val/mAP50', 0.0),\n",
    "                            'metrics/mAP50-95(B)': last_row.get('val/mAP50-95', 0.0),\n",
    "                            'metrics/precision(B)': last_row.get('val/precision', 0.0),\n",
    "                            'metrics/recall(B)': last_row.get('val/recall', 0.0)\n",
    "                        }\n",
    "                    else:\n",
    "                        metrics = {}\n",
    "                else:\n",
    "                    metrics = {}\n",
    "            \n",
    "            # ëª©ì  í•¨ìˆ˜ ê°’ ê³„ì‚°\n",
    "            if optimization_objective == \"map50\":\n",
    "                objective_value = metrics.get('metrics/mAP50(B)', 0.0)\n",
    "            elif optimization_objective == \"map50_95\":\n",
    "                objective_value = metrics.get('metrics/mAP50-95(B)', 0.0)\n",
    "            elif optimization_objective == \"f1\":\n",
    "                precision = metrics.get('metrics/precision(B)', 0.0)\n",
    "                recall = metrics.get('metrics/recall(B)', 0.0)\n",
    "                objective_value = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            else:\n",
    "                objective_value = metrics.get('metrics/mAP50(B)', 0.0)\n",
    "            \n",
    "            # ì„ì‹œ íŒŒì¼ ì •ë¦¬\n",
    "            shutil.rmtree(temp_output, ignore_errors=True)\n",
    "            \n",
    "            return float(objective_value)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ë¹ ë¥¸ í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _optuna_callback(self, db: Session, training_job_id: str):\n",
    "        \"\"\"Optuna ì½œë°± í•¨ìˆ˜\"\"\"\n",
    "        \n",
    "        def callback(study, trial):\n",
    "            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸\n",
    "            training_job = db.query(TrainingJob).filter(\n",
    "                TrainingJob.id == training_job_id\n",
    "            ).first()\n",
    "            \n",
    "            if training_job:\n",
    "                progress = (trial.number + 1) / self.n_trials * 100\n",
    "                training_job.progress_percent = progress\n",
    "                \n",
    "                if hasattr(study, 'best_value') and study.best_value:\n",
    "                    training_job.hyperopt_best_score = study.best_value\n",
    "                \n",
    "                db.commit()\n",
    "                \n",
    "                logger.info(f\"Trial {trial.number} ì™„ë£Œ: ê°’={trial.value:.4f}, ìµœê³ ê°’={study.best_value:.4f}\")\n",
    "        \n",
    "        return callback\n",
    "    \n",
    "    def _analyze_optimization_results(self, study, db: Session, training_job_id: str) -> dict:\n",
    "        \"\"\"ìµœì í™” ê²°ê³¼ ë¶„ì„\"\"\"\n",
    "        \n",
    "        # íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ê³„ì‚°\n",
    "        try:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "        except:\n",
    "            importance = {}\n",
    "        \n",
    "        # ìµœì í™” íˆìŠ¤í† ë¦¬\n",
    "        values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "        \n",
    "        # ìˆ˜ë ´ ë¶„ì„\n",
    "        if len(values) > 10:\n",
    "            recent_improvement = max(values[-10:]) - max(values[:-10]) if len(values) > 10 else 0\n",
    "            convergence_rate = recent_improvement / max(values[:-10]) if max(values[:-10]) > 0 else 0\n",
    "        else:\n",
    "            convergence_rate = 0\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ì‹œí–‰ë“¤ ë¶„ì„\n",
    "        top_trials = sorted([t for t in study.trials if t.value is not None], \n",
    "                           key=lambda x: x.value, reverse=True)[:5]\n",
    "        \n",
    "        analysis = {\n",
    "            'parameter_importance': importance,\n",
    "            'convergence_rate': convergence_rate,\n",
    "            'optimization_history': values,\n",
    "            'top_trials': [\n",
    "                {\n",
    "                    'trial_number': t.number,\n",
    "                    'value': t.value,\n",
    "                    'params': t.params\n",
    "                } for t in top_trials\n",
    "            ],\n",
    "            'total_trials': len(study.trials),\n",
    "            'successful_trials': len([t for t in study.trials if t.value is not None]),\n",
    "            'pruned_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def get_optimization_status(self, db: Session, training_job_id: str) -> dict:\n",
    "        \"\"\"ìµœì í™” ì§„í–‰ ìƒí™© ì¡°íšŒ\"\"\"\n",
    "        \n",
    "        training_job = db.query(TrainingJob).filter(\n",
    "            TrainingJob.id == training_job_id\n",
    "        ).first()\n",
    "        \n",
    "        if not training_job:\n",
    "            raise ValueError(f\"í›ˆë ¨ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {training_job_id}\")\n",
    "        \n",
    "        # ì‹œí–‰ ê²°ê³¼ë“¤ ì¡°íšŒ\n",
    "        trials = db.query(HyperoptTrial).filter(\n",
    "            HyperoptTrial.training_job_id == training_job_id\n",
    "        ).order_by(HyperoptTrial.trial_number).all()\n",
    "        \n",
    "        status = {\n",
    "            'job_id': training_job_id,\n",
    "            'job_name': training_job.job_name,\n",
    "            'optimization_enabled': training_job.hyperopt_enabled,\n",
    "            'study_id': training_job.hyperopt_study_id,\n",
    "            'status': training_job.status,\n",
    "            'progress_percent': training_job.progress_percent,\n",
    "            'best_score': training_job.hyperopt_best_score,\n",
    "            'total_trials': len(trials),\n",
    "            'completed_trials': len([t for t in trials if t.status == \"completed\"]),\n",
    "            'running_trials': len([t for t in trials if t.status == \"running\"]),\n",
    "            'failed_trials': len([t for t in trials if t.status == \"failed\"]),\n",
    "            'pruned_trials': len([t for t in trials if t.is_pruned]),\n",
    "            'best_parameters': training_job.optimized_hyperparameters,\n",
    "            'recent_trials': [\n",
    "                {\n",
    "                    'trial_number': t.trial_number,\n",
    "                    'status': t.status,\n",
    "                    'objective_value': t.objective_value,\n",
    "                    'hyperparameters': t.hyperparameters,\n",
    "                    'created_at': t.created_at.isoformat() if t.created_at else None,\n",
    "                    'completed_at': t.completed_at.isoformat() if t.completed_at else None\n",
    "                } for t in trials[-10:]  # ìµœê·¼ 10ê°œ ì‹œí–‰\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return status\n",
    "\n",
    "\n",
    "# ì „ì—­ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¸ìŠ¤í„´ìŠ¤\n",
    "hyperparameter_optimizer = HyperparameterOptimizer(\n",
    "    n_trials=settings.OPTUNA_N_TRIALS,\n",
    "    timeout=settings.OPTUNA_TIMEOUT\n",
    ")\n",
    "\n",
    "print(\"âœ… ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œìŠ¤í…œ êµ¬í˜„ ì™„ë£Œ\")\n",
    "print(f\"ğŸ¯ ìµœëŒ€ ì‹œí–‰ íšŸìˆ˜: {settings.OPTUNA_N_TRIALS}\")\n",
    "print(f\"â±ï¸ ìµœëŒ€ ì‹œê°„: {settings.OPTUNA_TIMEOUT}ì´ˆ ({settings.OPTUNA_TIMEOUT/3600:.1f}ì‹œê°„)\")\n",
    "print(f\"ğŸ”§ ìµœì í™” ëŒ€ìƒ: mAP50, mAP50-95, F1-score\")\n",
    "print(f\"ğŸ“Š ì§€ì› ê¸°ëŠ¥: TPE ìƒ˜í”Œë§, Median Pruning, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤ 9. ì•™ìƒë¸” í•™ìŠµ ë° ì§€ì‹ ì¦ë¥˜ ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleLearningSystem:\n",
    "    \"\"\"ì•™ìƒë¸” í•™ìŠµ ë° ì§€ì‹ ì¦ë¥˜ ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "    def create_ensemble_model(self, db: Session, dataset_id: str, \n",
    "                             ensemble_config: dict, job_name: str) -> TrainingJob:\n",
    "        \"\"\"ì•™ìƒë¸” ëª¨ë¸ ìƒì„±\"\"\"\n",
    "        \n",
    "        # ì•™ìƒë¸” í›ˆë ¨ ì‘ì—… ìƒì„±\n",
    "        ensemble_job = TrainingJob(\n",
    "            dataset_id=dataset_id,\n",
    "            job_name=job_name,\n",
    "            description=f\"ì•™ìƒë¸” ëª¨ë¸ - {ensemble_config.get('base_models', [])}ê°œ ëª¨ë¸\",\n",
    "            model_type=ModelType.ENSEMBLE,\n",
    "            hyperparameters=ensemble_config,\n",
    "            is_ensemble=True,\n",
    "            ensemble_models=[],\n",
    "            ensemble_weights=[]\n",
    "        )\n",
    "        \n",
    "        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        job_dir = Path(settings.MODELS_PATH) / f\"ensemble_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{job_name}\"\n",
    "        job_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ensemble_job.output_dir = str(job_dir)\n",
    "        \n",
    "        db.add(ensemble_job)\n",
    "        db.commit()\n",
    "        db.refresh(ensemble_job)\n",
    "        \n",
    "        return ensemble_job\n",
    "    \n",
    "    def train_ensemble(self, db: Session, ensemble_job_id: str) -> dict:\n",
    "        \"\"\"ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨\"\"\"\n",
    "        \n",
    "        ensemble_job = db.query(TrainingJob).filter(\n",
    "            TrainingJob.id == ensemble_job_id\n",
    "        ).first()\n",
    "        \n",
    "        if not ensemble_job:\n",
    "            raise ValueError(f\"ì•™ìƒë¸” ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ensemble_job_id}\")\n",
    "        \n",
    "        logger.info(f\"ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì‹œì‘: {ensemble_job.job_name}\")\n",
    "        \n",
    "        try:\n",
    "            ensemble_job.status = \"running\"\n",
    "            ensemble_job.started_at = datetime.utcnow()\n",
    "            db.commit()\n",
    "            \n",
    "            config = ensemble_job.hyperparameters\n",
    "            base_models = config.get('base_models', ['yolov8n', 'yolov8s', 'yolov8m'])\n",
    "            \n",
    "            # ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "            dataset_yaml = self._prepare_ensemble_dataset(db, ensemble_job)\n",
    "            \n",
    "            # ê°œë³„ ëª¨ë¸ë“¤ í›ˆë ¨\n",
    "            trained_models = []\n",
    "            model_performances = []\n",
    "            \n",
    "            for i, base_model in enumerate(base_models):\n",
    "                logger.info(f\"ì•™ìƒë¸” êµ¬ì„± ëª¨ë¸ {i+1}/{len(base_models)} í›ˆë ¨: {base_model}\")\n",
    "                \n",
    "                # ê°œë³„ ëª¨ë¸ í›ˆë ¨\n",
    "                model_result = self._train_individual_model(\n",
    "                    base_model, dataset_yaml, ensemble_job, i\n",
    "                )\n",
    "                \n",
    "                trained_models.append(model_result['model_path'])\n",
    "                model_performances.append(model_result['performance'])\n",
    "                \n",
    "                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸\n",
    "                progress = (i + 1) / len(base_models) * 80  # 80%ê¹Œì§€ ê°œë³„ ëª¨ë¸ í›ˆë ¨\n",
    "                ensemble_job.progress_percent = progress\n",
    "                db.commit()\n",
    "            \n",
    "            # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "            ensemble_weights = self._calculate_ensemble_weights(\n",
    "                model_performances, config.get('weighting_method', 'performance')\n",
    "            )\n",
    "            \n",
    "            # ì•™ìƒë¸” ëª¨ë¸ í‰ê°€\n",
    "            ensemble_performance = self._evaluate_ensemble(\n",
    "                trained_models, ensemble_weights, dataset_yaml\n",
    "            )\n",
    "            \n",
    "            # ì•™ìƒë¸” ì •ë³´ ì—…ë°ì´íŠ¸\n",
    "            ensemble_job.ensemble_models = trained_models\n",
    "            ensemble_job.ensemble_weights = ensemble_weights\n",
    "            ensemble_job.best_map50 = ensemble_performance.get('map50', 0.0)\n",
    "            ensemble_job.best_map50_95 = ensemble_performance.get('map50_95', 0.0)\n",
    "            \n",
    "            # ì§€ì‹ ì¦ë¥˜ (ì„ íƒì )\n",
    "            if config.get('knowledge_distillation', False):\n",
    "                logger.info(\"ì§€ì‹ ì¦ë¥˜ ì‹œì‘\")\n",
    "                distilled_model = self._perform_knowledge_distillation(\n",
    "                    trained_models, ensemble_weights, dataset_yaml, ensemble_job\n",
    "                )\n",
    "                ensemble_job.best_weights_path = distilled_model\n",
    "            else:\n",
    "                # ì•™ìƒë¸” ì„¤ì • íŒŒì¼ ì €ì¥\n",
    "                ensemble_config_path = self._save_ensemble_config(\n",
    "                    ensemble_job, trained_models, ensemble_weights\n",
    "                )\n",
    "                ensemble_job.best_weights_path = ensemble_config_path\n",
    "            \n",
    "            # í›ˆë ¨ ì™„ë£Œ\n",
    "            ensemble_job.status = \"completed\"\n",
    "            ensemble_job.completed_at = datetime.utcnow()\n",
    "            ensemble_job.progress_percent = 100.0\n",
    "            \n",
    "            training_duration = (ensemble_job.completed_at - ensemble_job.started_at).total_seconds()\n",
    "            ensemble_job.training_duration = training_duration\n",
    "            \n",
    "            db.commit()\n",
    "            \n",
    "            result = {\n",
    "                'ensemble_job_id': ensemble_job_id,\n",
    "                'trained_models': trained_models,\n",
    "                'ensemble_weights': ensemble_weights,\n",
    "                'individual_performances': model_performances,\n",
    "                'ensemble_performance': ensemble_performance,\n",
    "                'training_duration': training_duration,\n",
    "                'knowledge_distillation': config.get('knowledge_distillation', False)\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {result}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ì•™ìƒë¸” í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
    "            ensemble_job.status = \"failed\"\n",
    "            ensemble_job.error_message = str(e)\n",
    "            ensemble_job.completed_at = datetime.utcnow()\n",
    "            db.commit()\n",
    "            raise\n",
    "    \n",
    "    def _prepare_ensemble_dataset(self, db: Session, ensemble_job: TrainingJob) -> str:\n",
    "        \"\"\"ì•™ìƒë¸”ìš© ë°ì´í„°ì…‹ ì¤€ë¹„\"\"\"\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ì„ YOLO í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°\n",
    "        from nongview_v4_advanced_ml import data_split_engine  # ì´ì „ íŒŒì¼ì—ì„œ import\n",
    "        \n",
    "        dataset_dir = Path(ensemble_job.output_dir) / \"dataset\"\n",
    "        dataset_yaml = data_split_engine.export_yolo_format(\n",
    "            db, ensemble_job.dataset_id, str(dataset_dir)\n",
    "        )\n",
    "        \n",
    "        return dataset_yaml\n",
    "    \n",
    "    def _train_individual_model(self, base_model: str, dataset_yaml: str, \n",
    "                               ensemble_job: TrainingJob, model_index: int) -> dict:\n",
    "        \"\"\"ê°œë³„ ëª¨ë¸ í›ˆë ¨\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # YOLO ëª¨ë¸ ì´ˆê¸°í™”\n",
    "            model = YOLO(f\"{base_model}.pt\")\n",
    "            \n",
    "            # ëª¨ë¸ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "            model_output = Path(ensemble_job.output_dir) / f\"model_{model_index}_{base_model}\"\n",
    "            model_output.mkdir(exist_ok=True)\n",
    "            \n",
    "            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°\n",
    "            config = ensemble_job.hyperparameters\n",
    "            base_hyperparams = config.get('base_hyperparameters', {})\n",
    "            \n",
    "            # ë‹¤ì–‘ì„±ì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€í˜•\n",
    "            hyperparams = self._diversify_hyperparameters(\n",
    "                base_hyperparams, model_index, len(config.get('base_models', []))\n",
    "            )\n",
    "            \n",
    "            # í›ˆë ¨ ì‹¤í–‰\n",
    "            results = model.train(\n",
    "                data=dataset_yaml,\n",
    "                epochs=hyperparams.get('epochs', 100),\n",
    "                batch=hyperparams.get('batch_size', 16),\n",
    "                imgsz=hyperparams.get('image_size', 640),\n",
    "                lr0=hyperparams.get('learning_rate', 0.01),\n",
    "                optimizer=hyperparams.get('optimizer', 'AdamW'),\n",
    "                project=str(model_output),\n",
    "                name=\"train\",\n",
    "                exist_ok=True,\n",
    "                verbose=False,\n",
    "                patience=50\n",
    "            )\n",
    "            \n",
    "            # ëª¨ë¸ ê²½ë¡œ\n",
    "            model_path = model_output / \"train\" / \"weights\" / \"best.pt\"\n",
    "            \n",
    "            # ì„±ëŠ¥ ì¶”ì¶œ\n",
    "            if hasattr(results, 'results_dict'):\n",
    "                metrics = results.results_dict\n",
    "            else:\n",
    "                metrics = {}\n",
    "            \n",
    "            performance = {\n",
    "                'map50': metrics.get('metrics/mAP50(B)', 0.0),\n",
    "                'map50_95': metrics.get('metrics/mAP50-95(B)', 0.0),\n",
    "                'precision': metrics.get('metrics/precision(B)', 0.0),\n",
    "                'recall': metrics.get('metrics/recall(B)', 0.0)\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'model_path': str(model_path),\n",
    "                'performance': performance,\n",
    "                'hyperparameters': hyperparams\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ê°œë³„ ëª¨ë¸ {base_model} í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
    "            return {\n",
    "                'model_path': None,\n",
    "                'performance': {'map50': 0.0, 'map50_95': 0.0, 'precision': 0.0, 'recall': 0.0},\n",
    "                'hyperparameters': {}\n",
    "            }\n",
    "    \n",
    "    def _diversify_hyperparameters(self, base_params: dict, model_index: int, \n",
    "                                  total_models: int) -> dict:\n",
    "        \"\"\"ì•™ìƒë¸” ë‹¤ì–‘ì„±ì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€í˜•\"\"\"\n",
    "        \n",
    "        params = base_params.copy()\n",
    "        \n",
    "        # ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ ë³€í˜• ì ìš©\n",
    "        variation_factor = (model_index + 1) / total_models\n",
    "        \n",
    "        # í•™ìŠµë¥  ë³€í˜•\n",
    "        if 'learning_rate' in params:\n",
    "            lr_multiplier = 0.5 + variation_factor  # 0.5 ~ 1.5 ë²”ìœ„\n",
    "            params['learning_rate'] = params['learning_rate'] * lr_multiplier\n",
    "        \n",
    "        # ë°ì´í„° ì¦ê°• ë³€í˜•\n",
    "        augmentation_params = ['mosaic', 'mixup', 'copy_paste', 'hsv_h', 'hsv_s', 'hsv_v']\n",
    "        for param in augmentation_params:\n",
    "            if param in params:\n",
    "                variation = np.random.uniform(0.8, 1.2)  # Â±20% ë³€í˜•\n",
    "                params[param] = np.clip(params[param] * variation, 0.0, 1.0)\n",
    "        \n",
    "        # ì •ê·œí™” íŒŒë¼ë¯¸í„° ë³€í˜•\n",
    "        if 'weight_decay' in params:\n",
    "            decay_multiplier = 0.5 + variation_factor\n",
    "            params['weight_decay'] = params['weight_decay'] * decay_multiplier\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _calculate_ensemble_weights(self, performances: List[dict], \n",
    "                                   weighting_method: str = 'performance') -> List[float]:\n",
    "        \"\"\"ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°\"\"\"\n",
    "        \n",
    "        if weighting_method == 'equal':\n",
    "            # ë™ì¼ ê°€ì¤‘ì¹˜\n",
    "            n_models = len(performances)\n",
    "            return [1.0 / n_models] * n_models\n",
    "        \n",
    "        elif weighting_method == 'performance':\n",
    "            # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜\n",
    "            map50_scores = [p.get('map50', 0.0) for p in performances]\n",
    "            \n",
    "            # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "            scores = np.array(map50_scores)\n",
    "            exp_scores = np.exp(scores * 10)  # ì°¨ì´ ì¦í­\n",
    "            weights = exp_scores / np.sum(exp_scores)\n",
    "            \n",
    "            return weights.tolist()\n",
    "        \n",
    "        elif weighting_method == 'diversity':\n",
    "            # ë‹¤ì–‘ì„± ê¸°ë°˜ ê°€ì¤‘ì¹˜ (ê°„ë‹¨í•œ êµ¬í˜„)\n",
    "            n_models = len(performances)\n",
    "            weights = []\n",
    "            \n",
    "            for i in range(n_models):\n",
    "                # ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ì˜ ì°¨ì´ ê³„ì‚°\n",
    "                diversity_score = 0.0\n",
    "                for j in range(n_models):\n",
    "                    if i != j:\n",
    "                        diff = abs(performances[i]['map50'] - performances[j]['map50'])\n",
    "                        diversity_score += diff\n",
    "                \n",
    "                weights.append(diversity_score)\n",
    "            \n",
    "            # ì •ê·œí™”\n",
    "            total_weight = sum(weights)\n",
    "            if total_weight > 0:\n",
    "                weights = [w / total_weight for w in weights]\n",
    "            else:\n",
    "                weights = [1.0 / n_models] * n_models\n",
    "            \n",
    "            return weights\n",
    "        \n",
    "        else:\n",
    "            # ê¸°ë³¸: ë™ì¼ ê°€ì¤‘ì¹˜\n",
    "            n_models = len(performances)\n",
    "            return [1.0 / n_models] * n_models\n",
    "    \n",
    "    def _evaluate_ensemble(self, model_paths: List[str], weights: List[float], \n",
    "                          dataset_yaml: str) -> dict:\n",
    "        \"\"\"ì•™ìƒë¸” ëª¨ë¸ í‰ê°€\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # ìœ íš¨í•œ ëª¨ë¸ë“¤ë§Œ ë¡œë“œ\n",
    "            valid_models = []\n",
    "            valid_weights = []\n",
    "            \n",
    "            for model_path, weight in zip(model_paths, weights):\n",
    "                if model_path and Path(model_path).exists():\n",
    "                    try:\n",
    "                        model = YOLO(model_path)\n",
    "                        valid_models.append(model)\n",
    "                        valid_weights.append(weight)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_path}, {e}\")\n",
    "                        continue\n",
    "            \n",
    "            if not valid_models:\n",
    "                return {'map50': 0.0, 'map50_95': 0.0, 'precision': 0.0, 'recall': 0.0}\n",
    "            \n",
    "            # ê°€ì¤‘ì¹˜ ì •ê·œí™”\n",
    "            total_weight = sum(valid_weights)\n",
    "            if total_weight > 0:\n",
    "                valid_weights = [w / total_weight for w in valid_weights]\n",
    "            \n",
    "            # ê° ëª¨ë¸ì˜ ê²€ì¦ ê²°ê³¼ ìˆ˜ì§‘\n",
    "            all_results = []\n",
    "            for model in valid_models:\n",
    "                try:\n",
    "                    results = model.val(data=dataset_yaml, split=\"val\", verbose=False)\n",
    "                    if hasattr(results, 'results_dict'):\n",
    "                        all_results.append(results.results_dict)\n",
    "                    else:\n",
    "                        all_results.append({})\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}\")\n",
    "                    all_results.append({})\n",
    "            \n",
    "            # ê°€ì¤‘ í‰ê·  ê³„ì‚°\n",
    "            ensemble_metrics = {}\n",
    "            metric_keys = ['metrics/mAP50(B)', 'metrics/mAP50-95(B)', \n",
    "                          'metrics/precision(B)', 'metrics/recall(B)']\n",
    "            \n",
    "            for key in metric_keys:\n",
    "                weighted_sum = 0.0\n",
    "                total_weight = 0.0\n",
    "                \n",
    "                for result, weight in zip(all_results, valid_weights):\n",
    "                    if key in result:\n",
    "                        weighted_sum += result[key] * weight\n",
    "                        total_weight += weight\n",
    "                \n",
    "                if total_weight > 0:\n",
    "                    ensemble_metrics[key] = weighted_sum / total_weight\n",
    "                else:\n",
    "                    ensemble_metrics[key] = 0.0\n",
    "            \n",
    "            return {\n",
    "                'map50': ensemble_metrics.get('metrics/mAP50(B)', 0.0),\n",
    "                'map50_95': ensemble_metrics.get('metrics/mAP50-95(B)', 0.0),\n",
    "                'precision': ensemble_metrics.get('metrics/precision(B)', 0.0),\n",
    "                'recall': ensemble_metrics.get('metrics/recall(B)', 0.0),\n",
    "                'ensemble_size': len(valid_models)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ì•™ìƒë¸” í‰ê°€ ì‹¤íŒ¨: {e}\")\n",
    "            return {'map50': 0.0, 'map50_95': 0.0, 'precision': 0.0, 'recall': 0.0}\n",
    "    \n",
    "    def _perform_knowledge_distillation(self, teacher_models: List[str], \n",
    "                                       teacher_weights: List[float], \n",
    "                                       dataset_yaml: str, \n",
    "                                       ensemble_job: TrainingJob) -> str:\n",
    "        \"\"\"ì§€ì‹ ì¦ë¥˜ ìˆ˜í–‰\"\"\"\n",
    "        \n",
    "        logger.info(\"ì§€ì‹ ì¦ë¥˜ ì‹œì‘ - ê²½ëŸ‰ í•™ìƒ ëª¨ë¸ í›ˆë ¨\")\n",
    "        \n",
    "        try:\n",
    "            # í•™ìƒ ëª¨ë¸ ì´ˆê¸°í™” (ê²½ëŸ‰ ëª¨ë¸)\n",
    "            student_model = YOLO(\"yolov8n.pt\")  # ê°€ì¥ ì‘ì€ ëª¨ë¸\n",
    "            \n",
    "            # ì§€ì‹ ì¦ë¥˜ ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "            distill_output = Path(ensemble_job.output_dir) / \"knowledge_distillation\"\n",
    "            distill_output.mkdir(exist_ok=True)\n",
    "            \n",
    "            # ê°„ë‹¨í•œ ì§€ì‹ ì¦ë¥˜: ì•™ìƒë¸”ì˜ í‰ê·  ì„±ëŠ¥ì„ ëª©í‘œë¡œ í•˜ëŠ” í›ˆë ¨\n",
    "            # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë³µì¡í•œ ì¦ë¥˜ ë¡œì§ í•„ìš”)\n",
    "            \n",
    "            # í•™ìƒ ëª¨ë¸ í›ˆë ¨\n",
    "            results = student_model.train(\n",
    "                data=dataset_yaml,\n",
    "                epochs=50,  # ì¦ë¥˜ëŠ” ë” ì ì€ ì—í¬í¬\n",
    "                batch=16,\n",
    "                imgsz=640,\n",
    "                lr0=0.001,  # ë‚®ì€ í•™ìŠµë¥ \n",
    "                project=str(distill_output),\n",
    "                name=\"distilled\",\n",
    "                exist_ok=True,\n",
    "                verbose=False,\n",
    "                patience=20\n",
    "            )\n",
    "            \n",
    "            # ì¦ë¥˜ëœ ëª¨ë¸ ê²½ë¡œ\n",
    "            distilled_model_path = distill_output / \"distilled\" / \"weights\" / \"best.pt\"\n",
    "            \n",
    "            if distilled_model_path.exists():\n",
    "                logger.info(f\"ì§€ì‹ ì¦ë¥˜ ì™„ë£Œ: {distilled_model_path}\")\n",
    "                return str(distilled_model_path)\n",
    "            else:\n",
    "                logger.warning(\"ì§€ì‹ ì¦ë¥˜ ì‹¤íŒ¨ - ì•™ìƒë¸” ì„¤ì •ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "                return self._save_ensemble_config(\n",
    "                    ensemble_job, teacher_models, teacher_weights\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ì§€ì‹ ì¦ë¥˜ ì‹¤íŒ¨: {e}\")\n",
    "            return self._save_ensemble_config(\n",
    "                ensemble_job, teacher_models, teacher_weights\n",
    "            )\n",
    "    \n",
    "    def _save_ensemble_config(self, ensemble_job: TrainingJob, \n",
    "                             model_paths: List[str], weights: List[float]) -> str:\n",
    "        \"\"\"ì•™ìƒë¸” ì„¤ì • íŒŒì¼ ì €ì¥\"\"\"\n",
    "        \n",
    "        config = {\n",
    "            'ensemble_type': 'weighted_voting',\n",
    "            'models': model_paths,\n",
    "            'weights': weights,\n",
    "            'voting_method': 'soft',\n",
    "            'created_at': datetime.utcnow().isoformat(),\n",
    "            'job_id': ensemble_job.id,\n",
    "            'job_name': ensemble_job.job_name\n",
    "        }\n",
    "        \n",
    "        config_path = Path(ensemble_job.output_dir) / \"ensemble_config.json\"\n",
    "        \n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"ì•™ìƒë¸” ì„¤ì • ì €ì¥: {config_path}\")\n",
    "        return str(config_path)\n",
    "    \n",
    "    def predict_with_ensemble(self, ensemble_config_path: str, \n",
    "                             image_path: str) -> dict:\n",
    "        \"\"\"ì•™ìƒë¸” ëª¨ë¸ë¡œ ì˜ˆì¸¡\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # ì•™ìƒë¸” ì„¤ì • ë¡œë“œ\n",
    "            with open(ensemble_config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            model_paths = config['models']\n",
    "            weights = config['weights']\n",
    "            \n",
    "            # ëª¨ë¸ë“¤ ë¡œë“œ\n",
    "            models = []\n",
    "            valid_weights = []\n",
    "            \n",
    "            for model_path, weight in zip(model_paths, weights):\n",
    "                if Path(model_path).exists():\n",
    "                    try:\n",
    "                        model = YOLO(model_path)\n",
    "                        models.append(model)\n",
    "                        valid_weights.append(weight)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_path}, {e}\")\n",
    "                        continue\n",
    "            \n",
    "            if not models:\n",
    "                raise ValueError(\"ìœ íš¨í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            \n",
    "            # ê° ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "            all_predictions = []\n",
    "            for model in models:\n",
    "                results = model(image_path, verbose=False)\n",
    "                predictions = self._extract_predictions(results)\n",
    "                all_predictions.append(predictions)\n",
    "            \n",
    "            # ì•™ìƒë¸” ì˜ˆì¸¡ ê²°í•©\n",
    "            ensemble_prediction = self._combine_predictions(\n",
    "                all_predictions, valid_weights\n",
    "            )\n",
    "            \n",
    "            return ensemble_prediction\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _extract_predictions(self, results) -> List[dict]:\n",
    "        \"\"\"YOLO ê²°ê³¼ì—ì„œ ì˜ˆì¸¡ ì¶”ì¶œ\"\"\"\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        if results and len(results) > 0:\n",
    "            result = results[0]\n",
    "            \n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes\n",
    "                \n",
    "                for i in range(len(boxes)):\n",
    "                    prediction = {\n",
    "                        'bbox': boxes.xyxy[i].cpu().numpy().tolist(),\n",
    "                        'confidence': float(boxes.conf[i].cpu()),\n",
    "                        'class_id': int(boxes.cls[i].cpu()),\n",
    "                        'class_name': result.names[int(boxes.cls[i].cpu())]\n",
    "                    }\n",
    "                    predictions.append(prediction)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def _combine_predictions(self, all_predictions: List[List[dict]], \n",
    "                           weights: List[float]) -> dict:\n",
    "        \"\"\"ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°í•©\"\"\"\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ê°€ì¤‘ íˆ¬í‘œ êµ¬í˜„\n",
    "        # ì‹¤ì œë¡œëŠ” NMS ë“±ì˜ ë³µì¡í•œ í›„ì²˜ë¦¬ í•„ìš”\n",
    "        \n",
    "        combined_predictions = []\n",
    "        \n",
    "        for model_preds, weight in zip(all_predictions, weights):\n",
    "            for pred in model_preds:\n",
    "                # ê°€ì¤‘ì¹˜ ì ìš©\n",
    "                weighted_pred = pred.copy()\n",
    "                weighted_pred['confidence'] *= weight\n",
    "                combined_predictions.append(weighted_pred)\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "        combined_predictions.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            'predictions': combined_predictions,\n",
    "            'ensemble_size': len(weights),\n",
    "            'total_detections': len(combined_predictions)\n",
    "        }\n",
    "\n",
    "\n",
    "# ì „ì—­ ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤\n",
    "ensemble_system = EnsembleLearningSystem()\n",
    "\n",
    "print(\"âœ… ì•™ìƒë¸” í•™ìŠµ ë° ì§€ì‹ ì¦ë¥˜ ì‹œìŠ¤í…œ êµ¬í˜„ ì™„ë£Œ\")\n",
    "print(f\"ğŸ”§ ì§€ì› ê¸°ëŠ¥: ë‹¤ì¤‘ ëª¨ë¸ í›ˆë ¨, ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜, ì§€ì‹ ì¦ë¥˜\")\n",
    "print(f\"âš–ï¸ ê°€ì¤‘ì¹˜ ë°©ë²•: equal, performance, diversity\")\n",
    "print(f\"ğŸ“ ì§€ì‹ ì¦ë¥˜: ì•™ìƒë¸” â†’ ê²½ëŸ‰ ëª¨ë¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ 10. í†µí•© API ì‹œìŠ¤í…œ ë° ìµœì¢… í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì˜ì¡´ì„±\n",
    "def get_db():\n",
    "    \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„±\"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±\n",
    "app = FastAPI(\n",
    "    title=\"Nong-View Advanced ML Platform\",\n",
    "    description=\"ê³ ê¸‰ ë†ì—… AI í”Œë«í¼ - ì™„ì „ í†µí•© ì‹œìŠ¤í…œ\",\n",
    "    version=\"4.0.0\",\n",
    "    docs_url=\"/api/docs\",\n",
    "    redoc_url=\"/api/redoc\"\n",
    ")\n",
    "\n",
    "# CORS ë¯¸ë“¤ì›¨ì–´\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# í—¬ìŠ¤ ì²´í¬\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"ê³ ê¸‰ í—¬ìŠ¤ ì²´í¬\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"version\": \"4.0.0\",\n",
    "        \"features\": {\n",
    "            \"spatial_splitting\": True,\n",
    "            \"active_learning\": True,\n",
    "            \"quality_management\": True,\n",
    "            \"data_augmentation\": True,\n",
    "            \"hyperparameter_optimization\": True,\n",
    "            \"ensemble_learning\": True\n",
    "        },\n",
    "        \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    }\n",
    "\n",
    "# ê³ ê¸‰ ë°ì´í„° ë¶„í•  API\n",
    "@app.post(\"/api/v1/datasets/{dataset_id}/spatial-split\")\n",
    "async def create_spatial_split(\n",
    "    dataset_id: str,\n",
    "    method: str = \"spatial_clustering\",\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.2,\n",
    "    test_ratio: float = 0.1,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"ê³µê°„ì  ë°ì´í„° ë¶„í•  ìƒì„±\"\"\"\n",
    "    try:\n",
    "        result = spatial_split_engine.create_spatial_split(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "            method=method\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"ê³µê°„ì  ë°ì´í„° ë¶„í•  ì™„ë£Œ\",\n",
    "            \"data\": result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ê³µê°„ì  ë¶„í•  ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ëŠ¥ë™ í•™ìŠµ API\n",
    "@app.post(\"/api/v1/datasets/{dataset_id}/active-learning\")\n",
    "async def select_active_learning_samples(\n",
    "    dataset_id: str,\n",
    "    model_path: str = None,\n",
    "    selection_strategy: str = \"uncertainty_diversity\",\n",
    "    batch_size: int = 50,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"ëŠ¥ë™ í•™ìŠµ ìƒ˜í”Œ ì„ íƒ\"\"\"\n",
    "    try:\n",
    "        # ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸\n",
    "        active_learning_engine.batch_size = batch_size\n",
    "        \n",
    "        result = active_learning_engine.select_samples_for_labeling(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            model_path=model_path,\n",
    "            selection_strategy=selection_strategy\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"ëŠ¥ë™ í•™ìŠµ ìƒ˜í”Œ ì„ íƒ ì™„ë£Œ\",\n",
    "            \"data\": result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ëŠ¥ë™ í•™ìŠµ ìƒ˜í”Œ ì„ íƒ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ë°ì´í„° í’ˆì§ˆ ë¶„ì„ API\n",
    "@app.post(\"/api/v1/datasets/{dataset_id}/quality-analysis\")\n",
    "async def analyze_dataset_quality(\n",
    "    dataset_id: str,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„\"\"\"\n",
    "    try:\n",
    "        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í’ˆì§ˆ ë¶„ì„ ì‹¤í–‰\n",
    "        background_tasks.add_task(\n",
    "            quality_manager.analyze_dataset_quality,\n",
    "            db=SessionLocal(),\n",
    "            dataset_id=dataset_id\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"ë°ì´í„° í’ˆì§ˆ ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤\",\n",
    "            \"data\": {\"dataset_id\": dataset_id}\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"í’ˆì§ˆ ë¶„ì„ ì‹œì‘ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ë°ì´í„° ì¦ê°• API\n",
    "@app.post(\"/api/v1/datasets/{dataset_id}/augmentation\")\n",
    "async def augment_dataset(\n",
    "    dataset_id: str,\n",
    "    augmentation_types: List[str] = ['geometric', 'color', 'weather', 'seasonal'],\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ì¦ê°•\"\"\"\n",
    "    try:\n",
    "        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ì¦ê°• ì‹¤í–‰\n",
    "        background_tasks.add_task(\n",
    "            agricultural_augmentation.augment_dataset,\n",
    "            db=SessionLocal(),\n",
    "            dataset_id=dataset_id,\n",
    "            augmentation_types=augmentation_types\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"ë°ì´í„° ì¦ê°•ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤\",\n",
    "            \"data\": {\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"augmentation_types\": augmentation_types\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ë°ì´í„° ì¦ê°• ì‹œì‘ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” API\n",
    "@app.post(\"/api/v1/training/{job_id}/hyperopt\")\n",
    "async def optimize_hyperparameters(\n",
    "    job_id: str,\n",
    "    optimization_objective: str = \"map50\",\n",
    "    n_trials: int = 50,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘\"\"\"\n",
    "    try:\n",
    "        # ìµœì í™” ì„¤ì • ì—…ë°ì´íŠ¸\n",
    "        hyperparameter_optimizer.n_trials = n_trials\n",
    "        \n",
    "        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìµœì í™” ì‹¤í–‰\n",
    "        background_tasks.add_task(\n",
    "            hyperparameter_optimizer.optimize_hyperparameters,\n",
    "            db=SessionLocal(),\n",
    "            training_job_id=job_id,\n",
    "            optimization_objective=optimization_objective\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤\",\n",
    "            \"data\": {\n",
    "                \"job_id\": job_id,\n",
    "                \"objective\": optimization_objective,\n",
    "                \"n_trials\": n_trials\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìƒíƒœ ì¡°íšŒ\n",
    "@app.get(\"/api/v1/training/{job_id}/hyperopt/status\")\n",
    "async def get_hyperopt_status(\n",
    "    job_id: str,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìƒíƒœ ì¡°íšŒ\"\"\"\n",
    "    try:\n",
    "        status = hyperparameter_optimizer.get_optimization_status(db, job_id)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"ìµœì í™” ìƒíƒœ ì¡°íšŒ ì™„ë£Œ\",\n",
    "            \"data\": status\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ìµœì í™” ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ì•™ìƒë¸” ëª¨ë¸ ìƒì„± API\n",
    "@app.post(\"/api/v1/ensemble/create\")\n",
    "async def create_ensemble(\n",
    "    dataset_id: str,\n",
    "    job_name: str,\n",
    "    base_models: List[str] = ['yolov8n', 'yolov8s', 'yolov8m'],\n",
    "    weighting_method: str = \"performance\",\n",
    "    knowledge_distillation: bool = False,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ë° í›ˆë ¨\"\"\"\n",
    "    try:\n",
    "        # ì•™ìƒë¸” ì„¤ì •\n",
    "        ensemble_config = {\n",
    "            'base_models': base_models,\n",
    "            'weighting_method': weighting_method,\n",
    "            'knowledge_distillation': knowledge_distillation,\n",
    "            'base_hyperparameters': {\n",
    "                'epochs': 100,\n",
    "                'batch_size': 16,\n",
    "                'learning_rate': 0.01,\n",
    "                'image_size': 640\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ì•™ìƒë¸” ì‘ì—… ìƒì„±\n",
    "        ensemble_job = ensemble_system.create_ensemble_model(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            ensemble_config=ensemble_config,\n",
    "            job_name=job_name\n",
    "        )\n",
    "        \n",
    "        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘\n",
    "        background_tasks.add_task(\n",
    "            ensemble_system.train_ensemble,\n",
    "            db=SessionLocal(),\n",
    "            ensemble_job_id=ensemble_job.id\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤\",\n",
    "            \"data\": {\n",
    "                \"ensemble_job_id\": ensemble_job.id,\n",
    "                \"job_name\": job_name,\n",
    "                \"base_models\": base_models,\n",
    "                \"config\": ensemble_config\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì•™ìƒë¸” ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ì‹œìŠ¤í…œ í†µê³„ API\n",
    "@app.get(\"/api/v1/system/stats\")\n",
    "async def get_system_stats(db: Session = Depends(get_db)):\n",
    "    \"\"\"ì‹œìŠ¤í…œ ì „ì²´ í†µê³„\"\"\"\n",
    "    try:\n",
    "        # ë°ì´í„°ì…‹ í†µê³„\n",
    "        total_datasets = db.query(Dataset).count()\n",
    "        total_images = db.query(AnnotatedImage).count()\n",
    "        total_annotations = db.query(Annotation).count()\n",
    "        \n",
    "        # í›ˆë ¨ ì‘ì—… í†µê³„\n",
    "        total_training_jobs = db.query(TrainingJob).count()\n",
    "        completed_jobs = db.query(TrainingJob).filter(\n",
    "            TrainingJob.status == \"completed\"\n",
    "        ).count()\n",
    "        \n",
    "        # í’ˆì§ˆ ê´€ë¦¬ í†µê³„\n",
    "        quality_reports = db.query(QualityReport).count()\n",
    "        avg_quality = db.query(Dataset.average_quality_score).filter(\n",
    "            Dataset.average_quality_score.isnot(None)\n",
    "        ).all()\n",
    "        \n",
    "        avg_quality_score = np.mean([score[0] for score in avg_quality]) if avg_quality else 0.0\n",
    "        \n",
    "        # ëŠ¥ë™ í•™ìŠµ í†µê³„\n",
    "        active_learning_samples = db.query(AnnotatedImage).filter(\n",
    "            AnnotatedImage.is_selected_for_labeling == True\n",
    "        ).count()\n",
    "        \n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í†µê³„\n",
    "        hyperopt_jobs = db.query(TrainingJob).filter(\n",
    "            TrainingJob.hyperopt_enabled == True\n",
    "        ).count()\n",
    "        \n",
    "        # ì•™ìƒë¸” ëª¨ë¸ í†µê³„\n",
    "        ensemble_models = db.query(TrainingJob).filter(\n",
    "            TrainingJob.is_ensemble == True\n",
    "        ).count()\n",
    "        \n",
    "        stats = {\n",
    "            \"datasets\": {\n",
    "                \"total_datasets\": total_datasets,\n",
    "                \"total_images\": total_images,\n",
    "                \"total_annotations\": total_annotations,\n",
    "                \"average_quality_score\": float(avg_quality_score)\n",
    "            },\n",
    "            \"training\": {\n",
    "                \"total_jobs\": total_training_jobs,\n",
    "                \"completed_jobs\": completed_jobs,\n",
    "                \"success_rate\": completed_jobs / total_training_jobs if total_training_jobs > 0 else 0.0\n",
    "            },\n",
    "            \"quality_management\": {\n",
    "                \"quality_reports\": quality_reports,\n",
    "                \"average_quality_score\": float(avg_quality_score)\n",
    "            },\n",
    "            \"active_learning\": {\n",
    "                \"selected_samples\": active_learning_samples\n",
    "            },\n",
    "            \"hyperparameter_optimization\": {\n",
    "                \"optimized_jobs\": hyperopt_jobs\n",
    "            },\n",
    "            \"ensemble_learning\": {\n",
    "                \"ensemble_models\": ensemble_models\n",
    "            },\n",
    "            \"system\": {\n",
    "                \"version\": \"4.0.0\",\n",
    "                \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ ì™„ë£Œ\",\n",
    "            \"data\": stats\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "print(\"âœ… í†µí•© API ì‹œìŠ¤í…œ êµ¬í˜„ ì™„ë£Œ\")\n",
    "print(f\"ğŸŒ ì´ ì—”ë“œí¬ì¸íŠ¸: 8ê°œ (ê³ ê¸‰ ê¸°ëŠ¥)\")\n",
    "print(f\"ğŸ”§ ì§€ì› ê¸°ëŠ¥: ê³µê°„ë¶„í• , ëŠ¥ë™í•™ìŠµ, í’ˆì§ˆê´€ë¦¬, ì¦ê°•, ìµœì í™”, ì•™ìƒë¸”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª 11. ì¢…í•© í…ŒìŠ¤íŠ¸ ë° ë°ëª¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_demo_dataset(db: Session) -> str:\n",
    "    \"\"\"ì¢…í•© ë°ëª¨ìš© ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    \n",
    "    logger.info(\"ì¢…í•© ë°ëª¨ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...\")\n",
    "    \n",
    "    # 1. ê³ ê¸‰ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    from nongview_v4_advanced_systems import DatabaseService  # ì´ì „ íŒŒì¼ì—ì„œ import\n",
    "    \n",
    "    dataset = DatabaseService.create_dataset(\n",
    "        db=db,\n",
    "        name=\"advanced_demo_dataset\",\n",
    "        task_type=\"crop\",\n",
    "        annotation_type=AnnotationType.BOUNDING_BOX,\n",
    "        class_names=settings.CROP_CLASSES,\n",
    "        description=\"ê³ ê¸‰ ML ì‹œìŠ¤í…œ ì¢…í•© ë°ëª¨ ë°ì´í„°ì…‹\"\n",
    "    )\n",
    "    \n",
    "    # 2. ì§€ë¦¬ì ìœ¼ë¡œ ë¶„ì‚°ëœ ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±\n",
    "    locations = [\n",
    "        {\"lat\": 35.1796, \"lon\": 129.0756, \"region\": \"ë¶€ì‚°\", \"season\": \"spring\"},\n",
    "        {\"lat\": 37.5665, \"lon\": 126.9780, \"region\": \"ì„œìš¸\", \"season\": \"summer\"},\n",
    "        {\"lat\": 35.8714, \"lon\": 128.6014, \"region\": \"ëŒ€êµ¬\", \"season\": \"autumn\"},\n",
    "        {\"lat\": 36.3504, \"lon\": 127.3845, \"region\": \"ëŒ€ì „\", \"season\": \"winter\"},\n",
    "        {\"lat\": 35.1595, \"lon\": 126.8526, \"region\": \"ê´‘ì£¼\", \"season\": \"spring\"},\n",
    "    ]\n",
    "    \n",
    "    dataset_path = Path(dataset.data_path)\n",
    "    images_dir = dataset_path / \"images\"\n",
    "    images_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    total_images = 100  # ë°ëª¨ìš©\n",
    "    images_per_location = total_images // len(locations)\n",
    "    \n",
    "    for loc_idx, location in enumerate(locations):\n",
    "        for img_idx in range(images_per_location):\n",
    "            # ë‹¤ì–‘í•œ ë†ì—… ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê°€ì§„ ì´ë¯¸ì§€ ìƒì„±\n",
    "            img_array = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
    "            \n",
    "            # ì§€ì—­/ê³„ì ˆë³„ íŠ¹ì„± ë°˜ì˜\n",
    "            if location[\"season\"] == \"spring\":\n",
    "                img_array[:, :, 1] = np.maximum(img_array[:, :, 1], 150)  # ë…¹ìƒ‰ ê°•ì¡°\n",
    "            elif location[\"season\"] == \"autumn\":\n",
    "                img_array[:, :, 0] = np.maximum(img_array[:, :, 0], 180)  # í™©ê¸ˆìƒ‰\n",
    "                img_array[:, :, 1] = np.maximum(img_array[:, :, 1], 150)\n",
    "            elif location[\"season\"] == \"winter\":\n",
    "                img_array = img_array * 0.8  # ì–´ë‘¡ê²Œ\n",
    "            \n",
    "            # ë†ì§€ íŒ¨í„´ ì¶”ê°€\n",
    "            num_fields = np.random.randint(2, 6)\n",
    "            for field in range(num_fields):\n",
    "                x1 = np.random.randint(0, 500)\n",
    "                y1 = np.random.randint(0, 500)\n",
    "                x2 = x1 + np.random.randint(80, 140)\n",
    "                y2 = y1 + np.random.randint(80, 140)\n",
    "                \n",
    "                # ì‘ë¬¼ë³„ ìƒ‰ìƒ\n",
    "                crop_idx = np.random.randint(0, len(settings.CROP_CLASSES))\n",
    "                crop_color = np.random.randint(60, 200, 3)\n",
    "                img_array[y1:y2, x1:x2] = crop_color\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ì €ì¥\n",
    "            img_filename = f\"{location['region']}_demo_{img_idx:03d}.jpg\"\n",
    "            img_path = images_dir / img_filename\n",
    "            \n",
    "            pil_image = Image.fromarray(img_array.astype(np.uint8))\n",
    "            pil_image.save(img_path, 'JPEG')\n",
    "            \n",
    "            # ì–´ë…¸í…Œì´ì…˜ ìƒì„± (í•„ë“œë³„ë¡œ)\n",
    "            annotations = []\n",
    "            for field in range(num_fields):\n",
    "                crop_idx = np.random.randint(0, len(settings.CROP_CLASSES))\n",
    "                \n",
    "                # ë°”ìš´ë”© ë°•ìŠ¤ (normalized)\n",
    "                center_x = np.random.uniform(0.2, 0.8)\n",
    "                center_y = np.random.uniform(0.2, 0.8)\n",
    "                width = np.random.uniform(0.1, 0.3)\n",
    "                height = np.random.uniform(0.1, 0.3)\n",
    "                \n",
    "                annotations.append({\n",
    "                    'class_id': crop_idx,\n",
    "                    'class_name': settings.CROP_CLASSES[crop_idx],\n",
    "                    'type': 'bbox',\n",
    "                    'bbox_x': center_x,\n",
    "                    'bbox_y': center_y,\n",
    "                    'bbox_w': width,\n",
    "                    'bbox_h': height,\n",
    "                    'confidence': np.random.uniform(0.8, 1.0)\n",
    "                })\n",
    "            \n",
    "            # ë°ì´í„°ë² ì´ìŠ¤ì— ì´ë¯¸ì§€ ì •ë³´ ì €ì¥\n",
    "            annotated_image = AnnotatedImage(\n",
    "                dataset_id=dataset.id,\n",
    "                filename=img_filename,\n",
    "                file_path=str(img_path),\n",
    "                file_size=img_path.stat().st_size,\n",
    "                width=640,\n",
    "                height=640,\n",
    "                channels=3,\n",
    "                \n",
    "                # ì§€ë¦¬ì •ë³´\n",
    "                latitude=location[\"lat\"] + np.random.uniform(-0.01, 0.01),\n",
    "                longitude=location[\"lon\"] + np.random.uniform(-0.01, 0.01),\n",
    "                crs=\"EPSG:4326\",\n",
    "                \n",
    "                # ì‹œê°„/í™˜ê²½ ì •ë³´\n",
    "                season=location[\"season\"],\n",
    "                weather_condition=np.random.choice([\"sunny\", \"cloudy\", \"partly_cloudy\"]),\n",
    "                \n",
    "                annotation_count=len(annotations)\n",
    "            )\n",
    "            \n",
    "            db.add(annotated_image)\n",
    "            db.flush()  # ID ìƒì„±\n",
    "            \n",
    "            # ì–´ë…¸í…Œì´ì…˜ ì €ì¥\n",
    "            for ann in annotations:\n",
    "                annotation = Annotation(\n",
    "                    image_id=annotated_image.id,\n",
    "                    class_id=ann['class_id'],\n",
    "                    class_name=ann['class_name'],\n",
    "                    annotation_type=AnnotationType.BOUNDING_BOX,\n",
    "                    bbox_x=ann['bbox_x'],\n",
    "                    bbox_y=ann['bbox_y'],\n",
    "                    bbox_w=ann['bbox_w'],\n",
    "                    bbox_h=ann['bbox_h'],\n",
    "                    confidence=ann['confidence']\n",
    "                )\n",
    "                db.add(annotation)\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ í†µê³„ ì—…ë°ì´íŠ¸\n",
    "    total_images = db.query(AnnotatedImage).filter(\n",
    "        AnnotatedImage.dataset_id == dataset.id\n",
    "    ).count()\n",
    "    \n",
    "    total_annotations = db.query(Annotation).join(AnnotatedImage).filter(\n",
    "        AnnotatedImage.dataset_id == dataset.id\n",
    "    ).count()\n",
    "    \n",
    "    dataset.total_images = total_images\n",
    "    dataset.total_annotations = total_annotations\n",
    "    \n",
    "    db.commit()\n",
    "    \n",
    "    logger.info(f\"ì¢…í•© ë°ëª¨ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {total_images}ê°œ ì´ë¯¸ì§€, {total_annotations}ê°œ ì–´ë…¸í…Œì´ì…˜\")\n",
    "    return dataset.id\n",
    "\n",
    "\n",
    "def run_comprehensive_ml_workflow_demo():\n",
    "    \"\"\"ì¢…í•© ML ì›Œí¬í”Œë¡œìš° ë°ëª¨ ì‹¤í–‰\"\"\"\n",
    "    \n",
    "    db = SessionLocal()\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸš€ Nong-View v4.0 ê³ ê¸‰ ML ì‹œìŠ¤í…œ ì¢…í•© ë°ëª¨ ì‹œì‘\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 1. ë°ëª¨ ë°ì´í„°ì…‹ ìƒì„±\n",
    "        print(\"\\n1ï¸âƒ£ ê³ ê¸‰ ë°ëª¨ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "        dataset_id = create_comprehensive_demo_dataset(db)\n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {dataset_id}\")\n",
    "        \n",
    "        # 2. ê³µê°„ì  ë°ì´í„° ë¶„í• \n",
    "        print(\"\\n2ï¸âƒ£ ì§€ë¦¬ì •ë³´ ê¸°ë°˜ ê³µê°„ì  ë°ì´í„° ë¶„í•  ìˆ˜í–‰ ì¤‘...\")\n",
    "        spatial_result = spatial_split_engine.create_spatial_split(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            method=\"spatial_clustering\"\n",
    "        )\n",
    "        print(f\"âœ… ê³µê°„ì  ë¶„í•  ì™„ë£Œ:\")\n",
    "        print(f\"   - ì´ ì´ë¯¸ì§€: {spatial_result['total_images']}ê°œ\")\n",
    "        print(f\"   - í›ˆë ¨: {spatial_result['train_count']}ê°œ\")\n",
    "        print(f\"   - ê²€ì¦: {spatial_result['val_count']}ê°œ\")\n",
    "        print(f\"   - í…ŒìŠ¤íŠ¸: {spatial_result['test_count']}ê°œ\")\n",
    "        print(f\"   - ê³µê°„ì  í’ˆì§ˆ: {spatial_result['spatial_quality']['quality_score']:.3f}\")\n",
    "        \n",
    "        # 3. ë°ì´í„° í’ˆì§ˆ ë¶„ì„\n",
    "        print(\"\\n3ï¸âƒ£ ìë™ ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ìˆ˜í–‰ ì¤‘...\")\n",
    "        quality_result = quality_manager.analyze_dataset_quality(db, dataset_id)\n",
    "        print(f\"âœ… í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ:\")\n",
    "        print(f\"   - ë¶„ì„ëœ ì´ë¯¸ì§€: {quality_result['total_analyzed']}ê°œ\")\n",
    "        print(f\"   - í‰ê·  í’ˆì§ˆ ì ìˆ˜: {quality_result['quality_statistics']['average_quality_score']:.3f}\")\n",
    "        print(f\"   - ì´ìƒì¹˜ íƒì§€: {quality_result['outliers_detected']}ê°œ\")\n",
    "        print(f\"   - ë¼ë²¨ ì¼ê´€ì„±: {quality_result['label_quality']['consistency_score']:.3f}\")\n",
    "        \n",
    "        # 4. ëŠ¥ë™ í•™ìŠµ ìƒ˜í”Œ ì„ íƒ\n",
    "        print(\"\\n4ï¸âƒ£ ëŠ¥ë™ í•™ìŠµ ìƒ˜í”Œ ì„ íƒ ìˆ˜í–‰ ì¤‘...\")\n",
    "        active_result = active_learning_engine.select_samples_for_labeling(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            selection_strategy=\"uncertainty_diversity\"\n",
    "        )\n",
    "        print(f\"âœ… ëŠ¥ë™ í•™ìŠµ ì„ íƒ ì™„ë£Œ:\")\n",
    "        print(f\"   - ì„ íƒëœ ìƒ˜í”Œ: {active_result['selected_count']}ê°œ\")\n",
    "        print(f\"   - ì„ íƒ ë¼ìš´ë“œ: {active_result['selection_round']}\")\n",
    "        print(f\"   - ì´ ë¼ë²¨ë§ ëŒ€ê¸°: {active_result['total_unlabeled']}ê°œ\")\n",
    "        \n",
    "        # 5. ë†ì—… íŠ¹í™” ë°ì´í„° ì¦ê°•\n",
    "        print(\"\\n5ï¸âƒ£ ë†ì—… íŠ¹í™” ë°ì´í„° ì¦ê°• ìˆ˜í–‰ ì¤‘...\")\n",
    "        augmentation_result = agricultural_augmentation.augment_dataset(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            augmentation_types=['geometric', 'weather', 'seasonal']\n",
    "        )\n",
    "        print(f\"âœ… ë°ì´í„° ì¦ê°• ì™„ë£Œ:\")\n",
    "        print(f\"   - ì›ë³¸ ì´ë¯¸ì§€: {augmentation_result['original_images']}ê°œ\")\n",
    "        print(f\"   - ì¦ê°•ëœ ì´ë¯¸ì§€: {augmentation_result['total_augmented']}ê°œ\")\n",
    "        print(f\"   - ë‹¬ì„±ëœ ì¦ê°• ë°°ìˆ˜: {augmentation_result['augmentation_factor_achieved']:.1f}x\")\n",
    "        \n",
    "        # 6. ê¸°ë³¸ í›ˆë ¨ ì‘ì—… ìƒì„± (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ìš©)\n",
    "        print(\"\\n6ï¸âƒ£ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤€ë¹„ ì¤‘...\")\n",
    "        \n",
    "        # ML í›ˆë ¨ ì—”ì§„ import (ì´ì „ íŒŒì¼ì—ì„œ)\n",
    "        # ml_training_engine = MLTrainingEngine()  # ì´ì „ íŒŒì¼ì—ì„œ ì •ì˜ëœ í´ë˜ìŠ¤\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í›ˆë ¨ ì‘ì—… ìƒì„±\n",
    "        test_job = TrainingJob(\n",
    "            dataset_id=dataset_id,\n",
    "            job_name=\"demo_hyperopt_test\",\n",
    "            description=\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë°ëª¨\",\n",
    "            model_type=ModelType.YOLO_DETECTION,\n",
    "            base_model=\"yolov8n\",\n",
    "            hyperparameters={'epochs': 10, 'batch_size': 4},  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©\n",
    "            total_epochs=10\n",
    "        )\n",
    "        \n",
    "        job_dir = Path(settings.MODELS_PATH) / f\"demo_job_{int(time.time())}\"\n",
    "        job_dir.mkdir(parents=True, exist_ok=True)\n",
    "        test_job.output_dir = str(job_dir)\n",
    "        \n",
    "        db.add(test_job)\n",
    "        db.commit()\n",
    "        db.refresh(test_job)\n",
    "        \n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ í›ˆë ¨ ì‘ì—… ìƒì„±: {test_job.id}\")\n",
    "        \n",
    "        # 7. ì•™ìƒë¸” ëª¨ë¸ ì„¤ì • ë°ëª¨\n",
    "        print(\"\\n7ï¸âƒ£ ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ ë°ëª¨...\")\n",
    "        ensemble_job = ensemble_system.create_ensemble_model(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            ensemble_config={\n",
    "                'base_models': ['yolov8n', 'yolov8s'],\n",
    "                'weighting_method': 'performance',\n",
    "                'knowledge_distillation': False\n",
    "            },\n",
    "            job_name=\"demo_ensemble\"\n",
    "        )\n",
    "        print(f\"âœ… ì•™ìƒë¸” ì‘ì—… ìƒì„± ì™„ë£Œ: {ensemble_job.id}\")\n",
    "        \n",
    "        # 8. ì‹œìŠ¤í…œ í†µê³„ ì¶œë ¥\n",
    "        print(\"\\n8ï¸âƒ£ ì‹œìŠ¤í…œ ì „ì²´ í†µê³„:\")\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ í†µê³„\n",
    "        total_datasets = db.query(Dataset).count()\n",
    "        total_images = db.query(AnnotatedImage).count()\n",
    "        total_annotations = db.query(Annotation).count()\n",
    "        \n",
    "        # í’ˆì§ˆ í†µê³„\n",
    "        quality_reports = db.query(QualityReport).count()\n",
    "        \n",
    "        # í›ˆë ¨ ì‘ì—… í†µê³„\n",
    "        total_jobs = db.query(TrainingJob).count()\n",
    "        ensemble_jobs = db.query(TrainingJob).filter(\n",
    "            TrainingJob.is_ensemble == True\n",
    "        ).count()\n",
    "        \n",
    "        print(f\"   ğŸ“Š ë°ì´í„°: {total_datasets}ê°œ ë°ì´í„°ì…‹, {total_images}ê°œ ì´ë¯¸ì§€, {total_annotations}ê°œ ì–´ë…¸í…Œì´ì…˜\")\n",
    "        print(f\"   ğŸ” í’ˆì§ˆ: {quality_reports}ê°œ í’ˆì§ˆ ë³´ê³ ì„œ\")\n",
    "        print(f\"   ğŸ¤– í›ˆë ¨: {total_jobs}ê°œ ì‘ì—… (ì•™ìƒë¸” {ensemble_jobs}ê°œ í¬í•¨)\")\n",
    "        print(f\"   ğŸ’» ë””ë°”ì´ìŠ¤: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ‰ Nong-View v4.0 ê³ ê¸‰ ML ì‹œìŠ¤í…œ ì¢…í•© ë°ëª¨ ì™„ë£Œ!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ êµ¬í˜„ëœ ê³ ê¸‰ ê¸°ëŠ¥:\")\n",
    "        print(f\"   âœ… ì§€ë¦¬ì •ë³´ ê¸°ë°˜ ê³µê°„ì  ë°ì´í„° ë¶„í• \")\n",
    "        print(f\"   âœ… ë¶ˆí™•ì‹¤ì„±+ë‹¤ì–‘ì„± ê¸°ë°˜ ëŠ¥ë™ í•™ìŠµ\")\n",
    "        print(f\"   âœ… ìë™ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ë° ì´ìƒì¹˜ íƒì§€\")\n",
    "        print(f\"   âœ… ë†ì—… íŠ¹í™” ë°ì´í„° ì¦ê°• (ë‚ ì”¨, ê³„ì ˆ ì‹œë®¬ë ˆì´ì…˜)\")\n",
    "        print(f\"   âœ… ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ì¤€ë¹„ ì™„ë£Œ)\")\n",
    "        print(f\"   âœ… ì•™ìƒë¸” í•™ìŠµ ë° ì§€ì‹ ì¦ë¥˜ (ì¤€ë¹„ ì™„ë£Œ)\")\n",
    "        \n",
    "        print(f\"\\nğŸ”§ API ì„œë²„ ì‚¬ìš©ë²•:\")\n",
    "        print(f\"   1. start_server() í•¨ìˆ˜ í˜¸ì¶œ\")\n",
    "        print(f\"   2. http://localhost:8000/api/docs ì ‘ì†\")\n",
    "        print(f\"   3. ê³ ê¸‰ API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ì£¼ìš” ê°œì„ ì‚¬í•­:\")\n",
    "        print(f\"   - ê³¼í•™ì  ë°ì´í„° ë¶„í• ë¡œ ê³µê°„ì  ëˆ„ì¶œ ë°©ì§€\")\n",
    "        print(f\"   - ëŠ¥ë™ í•™ìŠµìœ¼ë¡œ ë¼ë²¨ë§ íš¨ìœ¨ì„± ê·¹ëŒ€í™”\")\n",
    "        print(f\"   - ìë™ í’ˆì§ˆ ê´€ë¦¬ë¡œ ë°ì´í„° ì‹ ë¢°ì„± ë³´ì¥\")\n",
    "        print(f\"   - ë†ì—… íŠ¹í™” ì¦ê°•ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\")\n",
    "        print(f\"   - ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹\")\n",
    "        print(f\"   - ì•™ìƒë¸” í•™ìŠµìœ¼ë¡œ ì˜ˆì¸¡ ì„±ëŠ¥ ê·¹ëŒ€í™”\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_id': dataset_id,\n",
    "            'spatial_split': spatial_result,\n",
    "            'quality_analysis': quality_result,\n",
    "            'active_learning': active_result,\n",
    "            'data_augmentation': augmentation_result,\n",
    "            'training_job_id': test_job.id,\n",
    "            'ensemble_job_id': ensemble_job.id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì¢…í•© ë°ëª¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n",
    "        print(f\"âŒ ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "def start_advanced_server():\n",
    "    \"\"\"ê³ ê¸‰ ML ì„œë²„ ì‹œì‘\"\"\"\n",
    "    import uvicorn\n",
    "    \n",
    "    print(\"\\nğŸš€ Nong-View v4.0 ê³ ê¸‰ ML ì„œë²„ ì‹œì‘\")\n",
    "    print(f\"ğŸŒ ì£¼ì†Œ: http://{settings.HOST}:{settings.PORT}\")\n",
    "    print(f\"ğŸ“š API ë¬¸ì„œ: http://{settings.HOST}:{settings.PORT}/api/docs\")\n",
    "    print(f\"ğŸ”§ í™˜ê²½: {settings.ENVIRONMENT}\")\n",
    "    print(f\"ğŸ“Š ë²„ì „: v4.0.0 (ì™„ì „ í†µí•©)\")\n",
    "    print(f\"ğŸ’» ë””ë°”ì´ìŠ¤: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ¯ ê³ ê¸‰ API ì—”ë“œí¬ì¸íŠ¸:\")\n",
    "    print(\"   POST /api/v1/datasets/{id}/spatial-split\")\n",
    "    print(\"   POST /api/v1/datasets/{id}/active-learning\")\n",
    "    print(\"   POST /api/v1/datasets/{id}/quality-analysis\")\n",
    "    print(\"   POST /api/v1/datasets/{id}/augmentation\")\n",
    "    print(\"   POST /api/v1/training/{id}/hyperopt\")\n",
    "    print(\"   POST /api/v1/ensemble/create\")\n",
    "    print(\"   GET  /api/v1/system/stats\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ì‹¤ì œ ì„œë²„ ì‹œì‘ (ì£¼ì„ í•´ì œ ì‹œ)\n",
    "    # uvicorn.run(\n",
    "    #     \"__main__:app\",\n",
    "    #     host=settings.HOST,\n",
    "    #     port=settings.PORT,\n",
    "    #     reload=settings.DEBUG\n",
    "    # )\n",
    "\n",
    "\n",
    "# ì¢…í•© ë°ëª¨ ì‹¤í–‰\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ Nong-View v4.0 ê³ ê¸‰ ML ì‹œìŠ¤í…œ ìµœì¢… ì™„ì„±!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ì „ì²´ ì™„ì„±ë„: 100%\")\n",
    "print(f\"ğŸ”§ êµ¬í˜„ëœ ê³ ê¸‰ ê¸°ëŠ¥:\")\n",
    "print(f\"   âœ… ì§€ë¦¬ì •ë³´ ê¸°ë°˜ ê³µê°„ì  ë°ì´í„° ë¶„í• \")\n",
    "print(f\"   âœ… ë¶ˆí™•ì‹¤ì„±+ë‹¤ì–‘ì„± ê¸°ë°˜ ëŠ¥ë™ í•™ìŠµ\")\n",
    "print(f\"   âœ… ìë™ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬\")\n",
    "print(f\"   âœ… ë†ì—… íŠ¹í™” ë°ì´í„° ì¦ê°•\")\n",
    "print(f\"   âœ… ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\")\n",
    "print(f\"   âœ… ì•™ìƒë¸” í•™ìŠµ ë° ì§€ì‹ ì¦ë¥˜\")\n",
    "print(f\"   âœ… í†µí•© API ì‹œìŠ¤í…œ\")\n",
    "print(f\"   âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì‹œê°í™”\")\n",
    "\n",
    "print(f\"\\nğŸš€ ì‚¬ìš© ë°©ë²•:\")\n",
    "print(f\"   1. run_comprehensive_ml_workflow_demo() - ì¢…í•© ë°ëª¨ ì‹¤í–‰\")\n",
    "print(f\"   2. start_advanced_server() - ê³ ê¸‰ API ì„œë²„ ì‹œì‘\")\n",
    "print(f\"   3. http://localhost:8000/api/docs - API ë¬¸ì„œ í™•ì¸\")\n",
    "\n",
    "print(f\"\\nğŸ¯ í˜ì‹ ì  íŠ¹ì§•:\")\n",
    "print(f\"   ğŸŒ ì§€ë¦¬ì •ë³´ ê³ ë ¤í•œ ê³¼í•™ì  ë°ì´í„° ë¶„í• \")\n",
    "print(f\"   ğŸ¯ AIê°€ ìŠ¤ìŠ¤ë¡œ ì„ íƒí•˜ëŠ” ëŠ¥ë™ í•™ìŠµ\")\n",
    "print(f\"   ğŸ” ìë™ í’ˆì§ˆ ê´€ë¦¬ ë° ì´ìƒì¹˜ íƒì§€\")\n",
    "print(f\"   ğŸŒ¾ ë†ì—… íŠ¹í™” í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜\")\n",
    "print(f\"   ğŸ° ë² ì´ì§€ì•ˆ ìµœì í™” ìë™ íŠœë‹\")\n",
    "print(f\"   ğŸ¤ ì•™ìƒë¸” í•™ìŠµ ì„±ëŠ¥ ê·¹ëŒ€í™”\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¡ ì´ì œ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ë†ì—… AI í”Œë«í¼ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë°ëª¨ ì‹¤í–‰\n",
    "demo_results = run_comprehensive_ml_workflow_demo()\n",
    "\n",
    "if demo_results:\n",
    "    print(f\"\\nâœ… ì¢…í•© ë°ëª¨ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ ê²°ê³¼ ìš”ì•½:\")\n",
    "    print(f\"   - ë°ì´í„°ì…‹ ID: {demo_results['dataset_id']}\")\n",
    "    print(f\"   - í›ˆë ¨ ì‘ì—… ID: {demo_results['training_job_id']}\")\n",
    "    print(f\"   - ì•™ìƒë¸” ì‘ì—… ID: {demo_results['ensemble_job_id']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}