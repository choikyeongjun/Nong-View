{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Nong-View AI ë†ì—…ì˜ìƒë¶„ì„ í”Œë«í¼ - ê³ ê¸‰ ML ì‹œìŠ¤í…œ v4.0\n",
    "\n",
    "**í”„ë¡œì íŠ¸ í˜„í™©**: ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ í†µí•© ML í›ˆë ¨ ì‹œìŠ¤í…œ  \n",
    "**í•µì‹¬ í˜ì‹ **: ì§€ë¦¬ì •ë³´ ê¸°ë°˜ ë¶„í• , ëŠ¥ë™í•™ìŠµ, ë°ì´í„° í’ˆì§ˆê´€ë¦¬, ìë™ íŠœë‹  \n",
    "**ê°œë°œ ë‚ ì§œ**: 2025-10-27  \n",
    "**ë²„ì „**: v4.0\n",
    "\n",
    "## ğŸ“‹ ê³ ê¸‰ ê¸°ëŠ¥ ëª©ë¡\n",
    "- âœ… ì§€ë¦¬ì •ë³´ ê¸°ë°˜ ê³µê°„ì  ë°ì´í„° ë¶„í• \n",
    "- âœ… ëŠ¥ë™ í•™ìŠµ ì‹œìŠ¤í…œ (ë¶ˆí™•ì‹¤ì„± + ë‹¤ì–‘ì„±)\n",
    "- âœ… ìë™ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬\n",
    "- âœ… ë†ì—… íŠ¹í™” ë°ì´í„° ì¦ê°•\n",
    "- âœ… ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "- âœ… ì•™ìƒë¸” í•™ìŠµ ë° ì§€ì‹ ì¦ë¥˜\n",
    "- âœ… ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ 1. í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ê¸‰ ML ê¸°ëŠ¥ì„ ìœ„í•œ ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install scikit-learn albumentations opencv-python\n",
    "!pip install torch torchvision torchaudio ultralytics\n",
    "!pip install optuna bayesian-optimization hyperopt\n",
    "!pip install wandb tensorboard matplotlib seaborn plotly\n",
    "!pip install fastapi uvicorn sqlalchemy alembic\n",
    "!pip install rasterio geopandas shapely fiona rtree\n",
    "!pip install pydantic[email] python-multipart\n",
    "!pip install scikit-image scipy pandas numpy\n",
    "!pip install imbalanced-learn umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any, Union, Tuple, Callable\n",
    "from pathlib import Path\n",
    "from uuid import UUID, uuid4\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ìˆ˜í•™ ë° ê³¼í•™ ê³„ì‚°\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "\n",
    "# ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import cv2\n",
    "from skimage import exposure, filters, morphology\n",
    "from skimage.segmentation import slic\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedShuffleSplit, \n",
    "    cross_val_score, KFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    precision_recall_curve, roc_auc_score\n",
    ")\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# ë°ì´í„° ì¦ê°•\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# ì§€ë¦¬ì •ë³´ ì²˜ë¦¬\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "from shapely.ops import unary_union\n",
    "from rtree import index\n",
    "\n",
    "# ì›¹ í”„ë ˆì„ì›Œí¬\n",
    "from fastapi import FastAPI, HTTPException, Depends, UploadFile, File, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, FileResponse\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤\n",
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime, Float, Text, Boolean, ForeignKey, JSON, Enum\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session, relationship\n",
    "from sqlalchemy.dialects.postgresql import UUID as PG_UUID\n",
    "import enum\n",
    "\n",
    "# AI/ML\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… ê³ ê¸‰ ML ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. ê³ ê¸‰ ì„¤ì • ë° êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ê¸‰ ML ì‹œìŠ¤í…œ ì„¤ì •\n",
    "class AdvancedMLSettings:\n",
    "    \"\"\"ê³ ê¸‰ ML ì‹œìŠ¤í…œ ì„¤ì •\"\"\"\n",
    "    PROJECT_NAME: str = \"Nong-View Advanced ML System\"\n",
    "    VERSION: str = \"4.0.0\"\n",
    "    API_V1_STR: str = \"/api/v1\"\n",
    "    ENVIRONMENT: str = \"development\"\n",
    "    DEBUG: bool = True\n",
    "    \n",
    "    # ì„œë²„ ì„¤ì •\n",
    "    HOST: str = \"127.0.0.1\"\n",
    "    PORT: int = 8000\n",
    "    \n",
    "    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •\n",
    "    DATABASE_URL: str = \"sqlite:///./nongview_advanced_ml.db\"\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥ ì„¤ì •\n",
    "    DATA_ROOT: str = \"./data\"\n",
    "    DATASET_PATH: str = \"./data/datasets\"\n",
    "    ANNOTATIONS_PATH: str = \"./data/annotations\"\n",
    "    MODELS_PATH: str = \"./data/models\"\n",
    "    LOGS_PATH: str = \"./data/logs\"\n",
    "    CACHE_PATH: str = \"./data/cache\"\n",
    "    QUALITY_PATH: str = \"./data/quality\"\n",
    "    AUGMENTATION_PATH: str = \"./data/augmented\"\n",
    "    \n",
    "    # ê¸°ë³¸ ML ì„¤ì •\n",
    "    TRAIN_SPLIT: float = 0.7\n",
    "    VAL_SPLIT: float = 0.2\n",
    "    TEST_SPLIT: float = 0.1\n",
    "    RANDOM_SEED: int = 42\n",
    "    \n",
    "    # ì§€ë¦¬ì •ë³´ ê¸°ë°˜ ë¶„í•  ì„¤ì •\n",
    "    SPATIAL_BUFFER_DISTANCE: float = 1000.0  # ë¯¸í„°\n",
    "    MIN_SPATIAL_CLUSTER_SIZE: int = 5\n",
    "    SPATIAL_OVERLAP_THRESHOLD: float = 0.3\n",
    "    \n",
    "    # ëŠ¥ë™ í•™ìŠµ ì„¤ì •\n",
    "    ACTIVE_LEARNING_BATCH_SIZE: int = 50\n",
    "    UNCERTAINTY_THRESHOLD: float = 0.8\n",
    "    DIVERSITY_WEIGHT: float = 0.3\n",
    "    QUERY_STRATEGY: str = \"uncertainty_diversity\"  # uncertainty, diversity, uncertainty_diversity\n",
    "    \n",
    "    # ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì„¤ì •\n",
    "    QUALITY_SCORE_THRESHOLD: float = 0.7\n",
    "    OUTLIER_CONTAMINATION: float = 0.1\n",
    "    LABEL_NOISE_THRESHOLD: float = 0.2\n",
    "    IMAGE_BLUR_THRESHOLD: float = 100.0\n",
    "    \n",
    "    # ë°ì´í„° ì¦ê°• ì„¤ì •\n",
    "    AUGMENTATION_FACTOR: int = 3  # ì›ë³¸ ëŒ€ë¹„ ì¦ê°• ë°°ìˆ˜\n",
    "    WEATHER_SIMULATION: bool = True\n",
    "    SEASONAL_VARIATION: bool = True\n",
    "    SOIL_COLOR_VARIATION: bool = True\n",
    "    \n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì„¤ì •\n",
    "    OPTUNA_N_TRIALS: int = 100\n",
    "    OPTUNA_TIMEOUT: int = 7200  # 2ì‹œê°„\n",
    "    EARLY_STOPPING_ROUNDS: int = 20\n",
    "    \n",
    "    # YOLO ì„¤ì •\n",
    "    IMAGE_SIZE: int = 640\n",
    "    BATCH_SIZE: int = 16\n",
    "    EPOCHS: int = 100\n",
    "    LEARNING_RATE: float = 0.01\n",
    "    \n",
    "    # ì•™ìƒë¸” ì„¤ì •\n",
    "    ENSEMBLE_MODELS: List[str] = [\"yolov8n\", \"yolov8s\", \"yolov8m\"]\n",
    "    ENSEMBLE_WEIGHTS: List[float] = [0.3, 0.4, 0.3]\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ì •ì˜\n",
    "    CROP_CLASSES: List[str] = [\n",
    "        'IRG',        # ì´íƒˆë¦¬ì•ˆ ë¼ì´ê·¸ë¼ìŠ¤\n",
    "        'BARLEY',     # ë³´ë¦¬\n",
    "        'WHEAT',      # ë°€\n",
    "        'CORN_SILAGE', # ì˜¥ìˆ˜ìˆ˜ì‚¬ì¼ë¦¬ì§€\n",
    "        'HAY',        # ê±´ì´ˆ\n",
    "        'FALLOW',     # íœ´ê²½ì§€\n",
    "        'RICE',       # ë²¼\n",
    "        'SOYBEAN',    # ì½©\n",
    "        'POTATO'      # ê°ì\n",
    "    ]\n",
    "    \n",
    "    FACILITY_CLASSES: List[str] = [\n",
    "        'GREENHOUSE_SINGLE',  # ë‹¨ë™ ë¹„ë‹í•˜ìš°ìŠ¤\n",
    "        'GREENHOUSE_MULTI',   # ì—°ë™ ë¹„ë‹í•˜ìš°ìŠ¤\n",
    "        'STORAGE',            # ì €ì¥ì‹œì„¤\n",
    "        'LIVESTOCK',          # ì¶•ì‚¬\n",
    "        'SILO',              # ì‚¬ì¼ë¡œ\n",
    "        'MACHINERY',         # ë†ê¸°ê³„\n",
    "        'IRRIGATION',        # ê´€ê°œì‹œì„¤\n",
    "        'PROCESSING_PLANT'   # ê°€ê³µì‹œì„¤\n",
    "    ]\n",
    "    \n",
    "    # ê³„ì ˆë³„ íŠ¹ì„±\n",
    "    SEASONAL_CHARACTERISTICS: Dict[str, Dict] = {\n",
    "        'spring': {\n",
    "            'brightness_factor': (0.9, 1.1),\n",
    "            'green_enhancement': (1.1, 1.3),\n",
    "            'soil_visibility': (0.7, 0.9)\n",
    "        },\n",
    "        'summer': {\n",
    "            'brightness_factor': (1.0, 1.2),\n",
    "            'green_enhancement': (1.2, 1.5),\n",
    "            'soil_visibility': (0.3, 0.6)\n",
    "        },\n",
    "        'autumn': {\n",
    "            'brightness_factor': (0.8, 1.0),\n",
    "            'green_enhancement': (0.7, 1.0),\n",
    "            'soil_visibility': (0.8, 1.0)\n",
    "        },\n",
    "        'winter': {\n",
    "            'brightness_factor': (0.7, 0.9),\n",
    "            'green_enhancement': (0.5, 0.8),\n",
    "            'soil_visibility': (0.9, 1.0)\n",
    "        }\n",
    "    }\n",
    "\n",
    "settings = AdvancedMLSettings()\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "for path in [settings.DATA_ROOT, settings.DATASET_PATH, settings.ANNOTATIONS_PATH, \n",
    "             settings.MODELS_PATH, settings.LOGS_PATH, settings.CACHE_PATH,\n",
    "             settings.QUALITY_PATH, settings.AUGMENTATION_PATH]:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "def set_all_seeds(seed: int):\n",
    "    \"\"\"ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‹œë“œ ì„¤ì •\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(settings.RANDOM_SEED)\n",
    "\n",
    "print(\"âœ… ê³ ê¸‰ ML ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"ğŸ“ ë°ì´í„° ë£¨íŠ¸: {settings.DATA_ROOT}\")\n",
    "print(f\"ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤: {settings.DATABASE_URL}\")\n",
    "print(f\"ğŸ² ëœë¤ ì‹œë“œ: {settings.RANDOM_SEED}\")\n",
    "print(f\"ğŸ”¬ ì§€ì› ê¸°ëŠ¥: ì§€ë¦¬ì •ë³´ ë¶„í• , ëŠ¥ë™í•™ìŠµ, í’ˆì§ˆê´€ë¦¬, ìë™ ì¦ê°•, í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—„ï¸ 3. í™•ì¥ëœ ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy ì„¤ì •\n",
    "engine = create_engine(\n",
    "    settings.DATABASE_URL,\n",
    "    connect_args={\"check_same_thread\": False} if \"sqlite\" in settings.DATABASE_URL else {}\n",
    ")\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "\n",
    "# í™•ì¥ëœ ì—´ê±°í˜• ì •ì˜\n",
    "class DataSplitType(enum.Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VALIDATION = \"validation\"\n",
    "    TEST = \"test\"\n",
    "    ACTIVE_LEARNING = \"active_learning\"\n",
    "    HOLDOUT = \"holdout\"\n",
    "\n",
    "class AnnotationType(enum.Enum):\n",
    "    BOUNDING_BOX = \"bbox\"\n",
    "    SEGMENTATION = \"segmentation\"\n",
    "    CLASSIFICATION = \"classification\"\n",
    "    KEYPOINT = \"keypoint\"\n",
    "\n",
    "class ModelType(enum.Enum):\n",
    "    YOLO_DETECTION = \"yolo_detection\"\n",
    "    YOLO_SEGMENTATION = \"yolo_segmentation\"\n",
    "    CLASSIFICATION = \"classification\"\n",
    "    ENSEMBLE = \"ensemble\"\n",
    "\n",
    "class QualityStatus(enum.Enum):\n",
    "    EXCELLENT = \"excellent\"\n",
    "    GOOD = \"good\"\n",
    "    ACCEPTABLE = \"acceptable\"\n",
    "    POOR = \"poor\"\n",
    "    REJECTED = \"rejected\"\n",
    "\n",
    "class AugmentationType(enum.Enum):\n",
    "    GEOMETRIC = \"geometric\"\n",
    "    COLOR = \"color\"\n",
    "    WEATHER = \"weather\"\n",
    "    SEASONAL = \"seasonal\"\n",
    "    NOISE = \"noise\"\n",
    "    SYNTHETIC = \"synthetic\"\n",
    "\n",
    "# í™•ì¥ëœ ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸\n",
    "class Dataset(Base):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ëª¨ë¸ (í™•ì¥)\"\"\"\n",
    "    __tablename__ = \"datasets\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    name = Column(String(200), nullable=False, unique=True)\n",
    "    description = Column(Text)\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ë©”íƒ€ì •ë³´\n",
    "    task_type = Column(String(50), nullable=False)\n",
    "    annotation_type = Column(Enum(AnnotationType), nullable=False)\n",
    "    class_names = Column(JSON)\n",
    "    \n",
    "    # í†µê³„\n",
    "    total_images = Column(Integer, default=0)\n",
    "    total_annotations = Column(Integer, default=0)\n",
    "    \n",
    "    # ë¶„í•  ì •ë³´\n",
    "    train_count = Column(Integer, default=0)\n",
    "    val_count = Column(Integer, default=0)\n",
    "    test_count = Column(Integer, default=0)\n",
    "    active_learning_count = Column(Integer, default=0)\n",
    "    \n",
    "    # ì§€ë¦¬ì •ë³´\n",
    "    spatial_extent = Column(JSON)  # bounding box of all images\n",
    "    crs = Column(String(50), default=\"EPSG:4326\")\n",
    "    \n",
    "    # í’ˆì§ˆ ì •ë³´\n",
    "    average_quality_score = Column(Float)\n",
    "    quality_distribution = Column(JSON)  # quality status counts\n",
    "    \n",
    "    # ì¦ê°• ì •ë³´\n",
    "    augmentation_enabled = Column(Boolean, default=False)\n",
    "    augmentation_factor = Column(Integer, default=1)\n",
    "    augmentation_types = Column(JSON)  # list of augmentation types\n",
    "    \n",
    "    # ê²½ë¡œ\n",
    "    data_path = Column(String(500))\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    annotated_images = relationship(\"AnnotatedImage\", back_populates=\"dataset\", cascade=\"all, delete-orphan\")\n",
    "    data_splits = relationship(\"DataSplit\", back_populates=\"dataset\", cascade=\"all, delete-orphan\")\n",
    "    training_jobs = relationship(\"TrainingJob\", back_populates=\"dataset\")\n",
    "    quality_reports = relationship(\"QualityReport\", back_populates=\"dataset\", cascade=\"all, delete-orphan\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "\n",
    "class AnnotatedImage(Base):\n",
    "    \"\"\"ë¼ë²¨ë§ëœ ì´ë¯¸ì§€ ëª¨ë¸ (í™•ì¥)\"\"\"\n",
    "    __tablename__ = \"annotated_images\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    dataset_id = Column(String, ForeignKey(\"datasets.id\"), nullable=False)\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ì •ë³´\n",
    "    filename = Column(String(255), nullable=False)\n",
    "    file_path = Column(String(500), nullable=False)\n",
    "    file_size = Column(Integer)\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°\n",
    "    width = Column(Integer)\n",
    "    height = Column(Integer)\n",
    "    channels = Column(Integer)\n",
    "    \n",
    "    # ì§€ë¦¬ì •ë³´\n",
    "    latitude = Column(Float)\n",
    "    longitude = Column(Float)\n",
    "    crs = Column(String(50))\n",
    "    bounds = Column(JSON)\n",
    "    spatial_geometry = Column(JSON)  # polygon or point geometry\n",
    "    \n",
    "    # ì‹œê°„ ì •ë³´\n",
    "    capture_date = Column(DateTime)\n",
    "    season = Column(String(20))  # spring, summer, autumn, winter\n",
    "    \n",
    "    # í™˜ê²½ ì •ë³´\n",
    "    weather_condition = Column(String(50))\n",
    "    lighting_condition = Column(String(50))\n",
    "    soil_type = Column(String(50))\n",
    "    \n",
    "    # ë¼ë²¨ë§ ì •ë³´\n",
    "    annotation_count = Column(Integer, default=0)\n",
    "    is_validated = Column(Boolean, default=False)\n",
    "    validation_score = Column(Float)\n",
    "    \n",
    "    # í’ˆì§ˆ ì •ë³´\n",
    "    quality_score = Column(Float)\n",
    "    quality_status = Column(Enum(QualityStatus))\n",
    "    blur_score = Column(Float)\n",
    "    brightness_score = Column(Float)\n",
    "    contrast_score = Column(Float)\n",
    "    \n",
    "    # ëŠ¥ë™ í•™ìŠµ\n",
    "    uncertainty_score = Column(Float)\n",
    "    diversity_score = Column(Float)\n",
    "    selection_priority = Column(Float)\n",
    "    is_selected_for_labeling = Column(Boolean, default=False)\n",
    "    \n",
    "    # ì¦ê°• ì •ë³´\n",
    "    is_augmented = Column(Boolean, default=False)\n",
    "    parent_image_id = Column(String, ForeignKey(\"annotated_images.id\"))\n",
    "    augmentation_type = Column(Enum(AugmentationType))\n",
    "    augmentation_parameters = Column(JSON)\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„°\n",
    "    metadata = Column(JSON)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    dataset = relationship(\"Dataset\", back_populates=\"annotated_images\")\n",
    "    annotations = relationship(\"Annotation\", back_populates=\"image\", cascade=\"all, delete-orphan\")\n",
    "    data_split = relationship(\"DataSplit\", back_populates=\"image\", uselist=False)\n",
    "    quality_metrics = relationship(\"QualityMetric\", back_populates=\"image\", cascade=\"all, delete-orphan\")\n",
    "    augmented_images = relationship(\"AnnotatedImage\", remote_side=[id])\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "\n",
    "class QualityReport(Base):\n",
    "    \"\"\"ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"quality_reports\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    dataset_id = Column(String, ForeignKey(\"datasets.id\"), nullable=False)\n",
    "    \n",
    "    # í’ˆì§ˆ ë¶„ì„ ê²°ê³¼\n",
    "    total_images_analyzed = Column(Integer)\n",
    "    excellent_count = Column(Integer, default=0)\n",
    "    good_count = Column(Integer, default=0)\n",
    "    acceptable_count = Column(Integer, default=0)\n",
    "    poor_count = Column(Integer, default=0)\n",
    "    rejected_count = Column(Integer, default=0)\n",
    "    \n",
    "    # í†µê³„\n",
    "    average_quality_score = Column(Float)\n",
    "    quality_std_dev = Column(Float)\n",
    "    \n",
    "    # ì´ìƒì¹˜ ì •ë³´\n",
    "    outlier_count = Column(Integer, default=0)\n",
    "    outlier_image_ids = Column(JSON)  # list of outlier image IDs\n",
    "    \n",
    "    # ë¼ë²¨ í’ˆì§ˆ\n",
    "    label_consistency_score = Column(Float)\n",
    "    suspected_label_errors = Column(Integer, default=0)\n",
    "    \n",
    "    # ìƒì„¸ ë¶„ì„ ê²°ê³¼\n",
    "    analysis_details = Column(JSON)\n",
    "    recommendations = Column(JSON)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    dataset = relationship(\"Dataset\", back_populates=\"quality_reports\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "\n",
    "class QualityMetric(Base):\n",
    "    \"\"\"ì´ë¯¸ì§€ë³„ í’ˆì§ˆ ì§€í‘œ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"quality_metrics\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    image_id = Column(String, ForeignKey(\"annotated_images.id\"), nullable=False)\n",
    "    \n",
    "    # ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œ\n",
    "    blur_score = Column(Float)  # Laplacian variance\n",
    "    brightness_score = Column(Float)  # mean pixel intensity\n",
    "    contrast_score = Column(Float)  # standard deviation of pixels\n",
    "    saturation_score = Column(Float)  # color saturation\n",
    "    \n",
    "    # ê³ ê¸‰ í’ˆì§ˆ ì§€í‘œ\n",
    "    edge_density = Column(Float)  # edge detection based\n",
    "    noise_level = Column(Float)  # noise estimation\n",
    "    compression_artifacts = Column(Float)  # JPEG artifacts\n",
    "    \n",
    "    # ë†ì—… íŠ¹í™” ì§€í‘œ\n",
    "    vegetation_coverage = Column(Float)  # NDVI based\n",
    "    soil_visibility = Column(Float)  # soil area ratio\n",
    "    shadow_ratio = Column(Float)  # shadow detection\n",
    "    \n",
    "    # ì¢…í•© ì ìˆ˜\n",
    "    overall_quality_score = Column(Float)\n",
    "    quality_status = Column(Enum(QualityStatus))\n",
    "    \n",
    "    # ì´ìƒì¹˜ íƒì§€\n",
    "    is_outlier = Column(Boolean, default=False)\n",
    "    outlier_score = Column(Float)\n",
    "    outlier_reason = Column(String(200))\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    image = relationship(\"AnnotatedImage\", back_populates=\"quality_metrics\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "\n",
    "# ê¸°ì¡´ ëª¨ë¸ë“¤ë„ í¬í•¨ (Annotation, DataSplit, TrainingJob, ModelEvaluation)\n",
    "class Annotation(Base):\n",
    "    \"\"\"ì–´ë…¸í…Œì´ì…˜ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"annotations\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    image_id = Column(String, ForeignKey(\"annotated_images.id\"), nullable=False)\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ì •ë³´\n",
    "    class_id = Column(Integer, nullable=False)\n",
    "    class_name = Column(String(100), nullable=False)\n",
    "    \n",
    "    # ì–´ë…¸í…Œì´ì…˜ íƒ€ì…\n",
    "    annotation_type = Column(Enum(AnnotationType), nullable=False)\n",
    "    \n",
    "    # ë°”ìš´ë”© ë°•ìŠ¤ (normalized coordinates 0-1)\n",
    "    bbox_x = Column(Float)\n",
    "    bbox_y = Column(Float)\n",
    "    bbox_w = Column(Float)\n",
    "    bbox_h = Column(Float)\n",
    "    \n",
    "    # ì„¸ê·¸ë©˜í…Œì´ì…˜\n",
    "    segmentation_points = Column(JSON)\n",
    "    \n",
    "    # ì–´ë…¸í…Œì´ì…˜ ë©”íƒ€ì •ë³´\n",
    "    confidence = Column(Float, default=1.0)\n",
    "    is_difficult = Column(Boolean, default=False)\n",
    "    is_crowd = Column(Boolean, default=False)\n",
    "    \n",
    "    # í’ˆì§ˆ ì •ë³´\n",
    "    annotation_quality = Column(Float)\n",
    "    is_verified = Column(Boolean, default=False)\n",
    "    verification_score = Column(Float)\n",
    "    \n",
    "    # ë¼ë²¨ë§ ì •ë³´\n",
    "    annotator_id = Column(String(100))\n",
    "    annotation_tool = Column(String(100))\n",
    "    annotation_time = Column(Float)  # seconds\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    image = relationship(\"AnnotatedImage\", back_populates=\"annotations\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "\n",
    "class DataSplit(Base):\n",
    "    \"\"\"ë°ì´í„° ë¶„í•  ëª¨ë¸ (í™•ì¥)\"\"\"\n",
    "    __tablename__ = \"data_splits\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    dataset_id = Column(String, ForeignKey(\"datasets.id\"), nullable=False)\n",
    "    image_id = Column(String, ForeignKey(\"annotated_images.id\"), nullable=False, unique=True)\n",
    "    \n",
    "    # ë¶„í•  íƒ€ì…\n",
    "    split_type = Column(Enum(DataSplitType), nullable=False)\n",
    "    \n",
    "    # ë¶„í•  ì •ë³´\n",
    "    split_version = Column(String(50), default=\"v1.0\")\n",
    "    split_method = Column(String(100))  # random, stratified, spatial, temporal\n",
    "    stratify_key = Column(String(100))\n",
    "    \n",
    "    # ê³µê°„ì  ë¶„í•  ì •ë³´\n",
    "    spatial_cluster_id = Column(Integer)\n",
    "    spatial_distance_to_boundary = Column(Float)  # meters\n",
    "    \n",
    "    # ì‹œê°„ì  ë¶„í•  ì •ë³´\n",
    "    temporal_group = Column(String(50))  # year, season, month\n",
    "    \n",
    "    # ëŠ¥ë™ í•™ìŠµ ì •ë³´\n",
    "    selection_round = Column(Integer)\n",
    "    selection_score = Column(Float)\n",
    "    selection_reason = Column(String(200))\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    dataset = relationship(\"Dataset\", back_populates=\"data_splits\")\n",
    "    image = relationship(\"AnnotatedImage\", back_populates=\"data_split\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "\n",
    "class TrainingJob(Base):\n",
    "    \"\"\"ëª¨ë¸ í›ˆë ¨ ì‘ì—… ëª¨ë¸ (í™•ì¥)\"\"\"\n",
    "    __tablename__ = \"training_jobs\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    dataset_id = Column(String, ForeignKey(\"datasets.id\"), nullable=False)\n",
    "    \n",
    "    # ì‘ì—… ì •ë³´\n",
    "    job_name = Column(String(200), nullable=False)\n",
    "    description = Column(Text)\n",
    "    status = Column(String(20), default=\"pending\")\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´\n",
    "    model_type = Column(Enum(ModelType), nullable=False)\n",
    "    base_model = Column(String(100))\n",
    "    pretrained_weights = Column(String(500))\n",
    "    \n",
    "    # í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    hyperparameters = Column(JSON)\n",
    "    optimized_hyperparameters = Column(JSON)  # from hyperparameter optimization\n",
    "    \n",
    "    # í›ˆë ¨ ì§„í–‰ ìƒí™©\n",
    "    current_epoch = Column(Integer, default=0)\n",
    "    total_epochs = Column(Integer)\n",
    "    progress_percent = Column(Float, default=0.0)\n",
    "    \n",
    "    # ì„±ëŠ¥ ì§€í‘œ\n",
    "    best_map50 = Column(Float)\n",
    "    best_map50_95 = Column(Float)\n",
    "    final_loss = Column(Float)\n",
    "    \n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "    hyperopt_enabled = Column(Boolean, default=False)\n",
    "    hyperopt_trials = Column(Integer)\n",
    "    hyperopt_best_score = Column(Float)\n",
    "    hyperopt_study_id = Column(String(100))\n",
    "    \n",
    "    # ì•™ìƒë¸” ì •ë³´\n",
    "    is_ensemble = Column(Boolean, default=False)\n",
    "    ensemble_models = Column(JSON)  # list of model paths\n",
    "    ensemble_weights = Column(JSON)  # list of weights\n",
    "    \n",
    "    # íŒŒì¼ ê²½ë¡œ\n",
    "    output_dir = Column(String(500))\n",
    "    best_weights_path = Column(String(500))\n",
    "    last_weights_path = Column(String(500))\n",
    "    log_file_path = Column(String(500))\n",
    "    \n",
    "    # ì‹œê°„ ì •ë³´\n",
    "    started_at = Column(DateTime)\n",
    "    completed_at = Column(DateTime)\n",
    "    training_duration = Column(Float)\n",
    "    \n",
    "    # ì—ëŸ¬ ì •ë³´\n",
    "    error_message = Column(Text)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    dataset = relationship(\"Dataset\", back_populates=\"training_jobs\")\n",
    "    evaluations = relationship(\"ModelEvaluation\", back_populates=\"training_job\", cascade=\"all, delete-orphan\")\n",
    "    hyperopt_trials_rel = relationship(\"HyperoptTrial\", back_populates=\"training_job\", cascade=\"all, delete-orphan\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "\n",
    "class HyperoptTrial(Base):\n",
    "    \"\"\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œí–‰ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"hyperopt_trials\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    training_job_id = Column(String, ForeignKey(\"training_jobs.id\"), nullable=False)\n",
    "    \n",
    "    # ì‹œí–‰ ì •ë³´\n",
    "    trial_number = Column(Integer, nullable=False)\n",
    "    status = Column(String(20))  # running, completed, failed, pruned\n",
    "    \n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    hyperparameters = Column(JSON, nullable=False)\n",
    "    \n",
    "    # ì„±ëŠ¥ ì§€í‘œ\n",
    "    objective_value = Column(Float)  # ìµœì í™” ëª©í‘œ ê°’\n",
    "    map50 = Column(Float)\n",
    "    map50_95 = Column(Float)\n",
    "    precision = Column(Float)\n",
    "    recall = Column(Float)\n",
    "    f1_score = Column(Float)\n",
    "    training_time = Column(Float)\n",
    "    \n",
    "    # ë©”íƒ€ì •ë³´\n",
    "    is_pruned = Column(Boolean, default=False)\n",
    "    pruned_at_epoch = Column(Integer)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    training_job = relationship(\"TrainingJob\", back_populates=\"hyperopt_trials_rel\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    completed_at = Column(DateTime)\n",
    "\n",
    "\n",
    "class ModelEvaluation(Base):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ ê²°ê³¼ ëª¨ë¸ (í™•ì¥)\"\"\"\n",
    "    __tablename__ = \"model_evaluations\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    training_job_id = Column(String, ForeignKey(\"training_jobs.id\"), nullable=False)\n",
    "    \n",
    "    # í‰ê°€ ì •ë³´\n",
    "    evaluation_type = Column(String(50), nullable=False)\n",
    "    epoch = Column(Integer)\n",
    "    \n",
    "    # ì„±ëŠ¥ ì§€í‘œ\n",
    "    map50 = Column(Float)\n",
    "    map50_95 = Column(Float)\n",
    "    precision = Column(Float)\n",
    "    recall = Column(Float)\n",
    "    f1_score = Column(Float)\n",
    "    \n",
    "    # ì†ì‹¤ í•¨ìˆ˜\n",
    "    box_loss = Column(Float)\n",
    "    cls_loss = Column(Float)\n",
    "    dfl_loss = Column(Float)\n",
    "    total_loss = Column(Float)\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥\n",
    "    class_metrics = Column(JSON)\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬\n",
    "    confusion_matrix = Column(JSON)\n",
    "    \n",
    "    # ì§€ì—­ë³„ ì„±ëŠ¥ (ë†ì—… íŠ¹í™”)\n",
    "    regional_performance = Column(JSON)\n",
    "    seasonal_performance = Column(JSON)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    training_job = relationship(\"TrainingJob\", back_populates=\"evaluations\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "print(\"âœ… í™•ì¥ëœ ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"ğŸ“Š ìƒì„±ëœ í…Œì´ë¸”: {list(Base.metadata.tables.keys())}\")\n",
    "print(f\"ğŸ”¢ ì´ í…Œì´ë¸” ìˆ˜: {len(Base.metadata.tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ 4. ì§€ë¦¬ì •ë³´ ê¸°ë°˜ ê³µê°„ì  ë°ì´í„° ë¶„í•  ì—”ì§„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDataSplitEngine:\n",
    "    \"\"\"ì§€ë¦¬ì •ë³´ ê¸°ë°˜ ê³µê°„ì  ë°ì´í„° ë¶„í•  ì—”ì§„\"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_distance: float = 1000.0, min_cluster_size: int = 5):\n",
    "        self.buffer_distance = buffer_distance  # ë¯¸í„°\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        \n",
    "    def create_spatial_split(self, db: Session, dataset_id: str, \n",
    "                            train_ratio: float = 0.7, val_ratio: float = 0.2, test_ratio: float = 0.1,\n",
    "                            method: str = \"spatial_clustering\") -> dict:\n",
    "        \"\"\"ê³µê°„ì  ë¶„í•  ìƒì„±\"\"\"\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì´ë¯¸ì§€ ì¡°íšŒ (ì§€ë¦¬ì •ë³´ í¬í•¨)\n",
    "        images = db.query(AnnotatedImage).filter(\n",
    "            AnnotatedImage.dataset_id == dataset_id,\n",
    "            AnnotatedImage.latitude.isnot(None),\n",
    "            AnnotatedImage.longitude.isnot(None)\n",
    "        ).all()\n",
    "        \n",
    "        if len(images) < 10:\n",
    "            raise ValueError(\"ê³µê°„ì  ë¶„í• ì„ ìœ„í•´ ìµœì†Œ 10ê°œ ì´ìƒì˜ ì§€ë¦¬ì •ë³´ë¥¼ ê°€ì§„ ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        \n",
    "        # ì¢Œí‘œ ë°°ì—´ ìƒì„±\n",
    "        coordinates = np.array([[img.latitude, img.longitude] for img in images])\n",
    "        \n",
    "        if method == \"spatial_clustering\":\n",
    "            split_assignments = self._cluster_based_split(\n",
    "                coordinates, train_ratio, val_ratio, test_ratio\n",
    "            )\n",
    "        elif method == \"grid_based\":\n",
    "            split_assignments = self._grid_based_split(\n",
    "                coordinates, train_ratio, val_ratio, test_ratio\n",
    "            )\n",
    "        elif method == \"buffer_based\":\n",
    "            split_assignments = self._buffer_based_split(\n",
    "                coordinates, images, train_ratio, val_ratio, test_ratio\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ê³µê°„ ë¶„í•  ë°©ë²•: {method}\")\n",
    "        \n",
    "        # ê¸°ì¡´ ë¶„í•  ë°ì´í„° ì‚­ì œ\n",
    "        db.query(DataSplit).filter(DataSplit.dataset_id == dataset_id).delete()\n",
    "        \n",
    "        # ìƒˆë¡œìš´ ë¶„í•  ë°ì´í„° ì €ì¥\n",
    "        split_counts = {DataSplitType.TRAIN: 0, DataSplitType.VALIDATION: 0, DataSplitType.TEST: 0}\n",
    "        \n",
    "        for i, split_type in enumerate(split_assignments):\n",
    "            image = images[i]\n",
    "            \n",
    "            # ê²½ê³„ì™€ì˜ ê±°ë¦¬ ê³„ì‚°\n",
    "            distance_to_boundary = self._calculate_distance_to_boundary(\n",
    "                coordinates[i], coordinates, split_assignments, split_type\n",
    "            )\n",
    "            \n",
    "            data_split = DataSplit(\n",
    "                dataset_id=dataset_id,\n",
    "                image_id=image.id,\n",
    "                split_type=split_type,\n",
    "                split_method=f\"spatial_{method}\",\n",
    "                spatial_distance_to_boundary=distance_to_boundary,\n",
    "                split_version=\"v1.0\"\n",
    "            )\n",
    "            \n",
    "            db.add(data_split)\n",
    "            split_counts[split_type] += 1\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ í†µê³„ ì—…ë°ì´íŠ¸\n",
    "        dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()\n",
    "        if dataset:\n",
    "            dataset.train_count = split_counts[DataSplitType.TRAIN]\n",
    "            dataset.val_count = split_counts[DataSplitType.VALIDATION]\n",
    "            dataset.test_count = split_counts[DataSplitType.TEST]\n",
    "            \n",
    "            # ê³µê°„ì  ë²”ìœ„ ê³„ì‚° ë° ì €ì¥\n",
    "            min_lat, min_lon = coordinates.min(axis=0)\n",
    "            max_lat, max_lon = coordinates.max(axis=0)\n",
    "            dataset.spatial_extent = {\n",
    "                \"min_latitude\": float(min_lat),\n",
    "                \"min_longitude\": float(min_lon),\n",
    "                \"max_latitude\": float(max_lat),\n",
    "                \"max_longitude\": float(max_lon)\n",
    "            }\n",
    "        \n",
    "        db.commit()\n",
    "        \n",
    "        # ê³µê°„ì  ë¶„í•  í’ˆì§ˆ í‰ê°€\n",
    "        spatial_quality = self._evaluate_spatial_split_quality(\n",
    "            coordinates, split_assignments\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'total_images': len(images),\n",
    "            'train_count': split_counts[DataSplitType.TRAIN],\n",
    "            'val_count': split_counts[DataSplitType.VALIDATION],\n",
    "            'test_count': split_counts[DataSplitType.TEST],\n",
    "            'split_method': f\"spatial_{method}\",\n",
    "            'spatial_quality': spatial_quality,\n",
    "            'spatial_extent': dataset.spatial_extent if dataset else None\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"ê³µê°„ì  ë°ì´í„° ë¶„í•  ì™„ë£Œ: {result}\")\n",
    "        return result\n",
    "    \n",
    "    def _cluster_based_split(self, coordinates: np.ndarray, \n",
    "                           train_ratio: float, val_ratio: float, test_ratio: float) -> List[DataSplitType]:\n",
    "        \"\"\"í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ê³µê°„ ë¶„í• \"\"\"\n",
    "        \n",
    "        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ê³µê°„ì  í´ëŸ¬ìŠ¤í„° ìƒì„±\n",
    "        # ìœ„ë„/ê²½ë„ë¥¼ ë¯¸í„° ë‹¨ìœ„ë¡œ ê·¼ì‚¬ ë³€í™˜ (1ë„ â‰ˆ 111,000m)\n",
    "        coords_meter = coordinates * 111000  # ê·¼ì‚¬ ë³€í™˜\n",
    "        eps_meter = self.buffer_distance\n",
    "        \n",
    "        clustering = DBSCAN(eps=eps_meter, min_samples=self.min_cluster_size)\n",
    "        cluster_labels = clustering.fit_predict(coords_meter)\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë¶„í•  í• ë‹¹\n",
    "        unique_clusters = np.unique(cluster_labels[cluster_labels >= 0])  # ë…¸ì´ì¦ˆ ì œì™¸\n",
    "        n_clusters = len(unique_clusters)\n",
    "        \n",
    "        if n_clusters < 3:\n",
    "            logger.warning(\"í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ ëœë¤ ë¶„í• ë¡œ ì „í™˜\")\n",
    "            return self._random_spatial_split(len(coordinates), train_ratio, val_ratio, test_ratio)\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„°ë¥¼ train/val/testì— í• ë‹¹\n",
    "        cluster_assignments = self._assign_clusters_to_splits(\n",
    "            unique_clusters, train_ratio, val_ratio, test_ratio\n",
    "        )\n",
    "        \n",
    "        # ê°œë³„ ì´ë¯¸ì§€ì— ë¶„í•  í• ë‹¹\n",
    "        split_assignments = []\n",
    "        for i, cluster_id in enumerate(cluster_labels):\n",
    "            if cluster_id >= 0:  # ì •ìƒ í´ëŸ¬ìŠ¤í„°\n",
    "                split_assignments.append(cluster_assignments[cluster_id])\n",
    "            else:  # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ëŠ” ëœë¤ í• ë‹¹\n",
    "                split_assignments.append(np.random.choice([\n",
    "                    DataSplitType.TRAIN, DataSplitType.VALIDATION, DataSplitType.TEST\n",
    "                ], p=[train_ratio, val_ratio, test_ratio]))\n",
    "        \n",
    "        return split_assignments\n",
    "    \n",
    "    def _grid_based_split(self, coordinates: np.ndarray,\n",
    "                         train_ratio: float, val_ratio: float, test_ratio: float) -> List[DataSplitType]:\n",
    "        \"\"\"ê²©ì ê¸°ë°˜ ê³µê°„ ë¶„í• \"\"\"\n",
    "        \n",
    "        # ê³µê°„ ë²”ìœ„ ê³„ì‚°\n",
    "        min_lat, min_lon = coordinates.min(axis=0)\n",
    "        max_lat, max_lon = coordinates.max(axis=0)\n",
    "        \n",
    "        # ê²©ì í¬ê¸° ê²°ì • (ì ì ˆí•œ ì…€ ìˆ˜ê°€ ë˜ë„ë¡)\n",
    "        target_cells = max(10, int(np.sqrt(len(coordinates) / 5)))  # ì…€ë‹¹ í‰ê·  5ê°œ í¬ì¸íŠ¸\n",
    "        \n",
    "        lat_step = (max_lat - min_lat) / np.sqrt(target_cells)\n",
    "        lon_step = (max_lon - min_lon) / np.sqrt(target_cells)\n",
    "        \n",
    "        # ê° í¬ì¸íŠ¸ë¥¼ ê²©ì ì…€ì— í• ë‹¹\n",
    "        grid_assignments = []\n",
    "        for lat, lon in coordinates:\n",
    "            grid_row = int((lat - min_lat) / lat_step)\n",
    "            grid_col = int((lon - min_lon) / lon_step)\n",
    "            grid_assignments.append((grid_row, grid_col))\n",
    "        \n",
    "        # ìœ ë‹ˆí¬í•œ ì…€ë“¤ì„ train/val/testì— í• ë‹¹\n",
    "        unique_cells = list(set(grid_assignments))\n",
    "        cell_split_map = {}\n",
    "        \n",
    "        np.random.shuffle(unique_cells)\n",
    "        n_train = int(len(unique_cells) * train_ratio)\n",
    "        n_val = int(len(unique_cells) * val_ratio)\n",
    "        \n",
    "        for i, cell in enumerate(unique_cells):\n",
    "            if i < n_train:\n",
    "                cell_split_map[cell] = DataSplitType.TRAIN\n",
    "            elif i < n_train + n_val:\n",
    "                cell_split_map[cell] = DataSplitType.VALIDATION\n",
    "            else:\n",
    "                cell_split_map[cell] = DataSplitType.TEST\n",
    "        \n",
    "        # ê°œë³„ í¬ì¸íŠ¸ì— ë¶„í•  í• ë‹¹\n",
    "        split_assignments = [cell_split_map[cell] for cell in grid_assignments]\n",
    "        \n",
    "        return split_assignments\n",
    "    \n",
    "    def _buffer_based_split(self, coordinates: np.ndarray, images: List[AnnotatedImage],\n",
    "                           train_ratio: float, val_ratio: float, test_ratio: float) -> List[DataSplitType]:\n",
    "        \"\"\"ë²„í¼ ê¸°ë°˜ ê³µê°„ ë¶„í•  (ê³µê°„ì  ëˆ„ì¶œ ë°©ì§€)\"\"\"\n",
    "        \n",
    "        split_assignments = [None] * len(coordinates)\n",
    "        remaining_indices = list(range(len(coordinates)))\n",
    "        \n",
    "        # ë¶„í• ë³„ ëª©í‘œ ê°œìˆ˜\n",
    "        n_total = len(coordinates)\n",
    "        target_train = int(n_total * train_ratio)\n",
    "        target_val = int(n_total * val_ratio)\n",
    "        target_test = n_total - target_train - target_val\n",
    "        \n",
    "        counts = {DataSplitType.TRAIN: 0, DataSplitType.VALIDATION: 0, DataSplitType.TEST: 0}\n",
    "        \n",
    "        while remaining_indices:\n",
    "            # í˜„ì¬ ê°€ì¥ í•„ìš”í•œ ë¶„í•  ìœ í˜• ê²°ì •\n",
    "            train_need = max(0, target_train - counts[DataSplitType.TRAIN])\n",
    "            val_need = max(0, target_val - counts[DataSplitType.VALIDATION])\n",
    "            test_need = max(0, target_test - counts[DataSplitType.TEST])\n",
    "            \n",
    "            if train_need >= val_need and train_need >= test_need:\n",
    "                current_split = DataSplitType.TRAIN\n",
    "            elif val_need >= test_need:\n",
    "                current_split = DataSplitType.VALIDATION\n",
    "            else:\n",
    "                current_split = DataSplitType.TEST\n",
    "            \n",
    "            # ëœë¤í•˜ê²Œ ì‹œì‘ì  ì„ íƒ\n",
    "            seed_idx = np.random.choice(remaining_indices)\n",
    "            seed_coord = coordinates[seed_idx]\n",
    "            \n",
    "            # ë²„í¼ ë‚´ì˜ ëª¨ë“  í¬ì¸íŠ¸ë¥¼ ê°™ì€ ë¶„í• ì— í• ë‹¹\n",
    "            buffer_indices = []\n",
    "            for idx in remaining_indices:\n",
    "                if self._calculate_distance(seed_coord, coordinates[idx]) <= self.buffer_distance:\n",
    "                    buffer_indices.append(idx)\n",
    "            \n",
    "            # í• ë‹¹\n",
    "            for idx in buffer_indices:\n",
    "                split_assignments[idx] = current_split\n",
    "                counts[current_split] += 1\n",
    "                remaining_indices.remove(idx)\n",
    "        \n",
    "        return split_assignments\n",
    "    \n",
    "    def _assign_clusters_to_splits(self, clusters: np.ndarray,\n",
    "                                  train_ratio: float, val_ratio: float, test_ratio: float) -> dict:\n",
    "        \"\"\"í´ëŸ¬ìŠ¤í„°ë¥¼ train/val/test ë¶„í• ì— í• ë‹¹\"\"\"\n",
    "        \n",
    "        n_clusters = len(clusters)\n",
    "        assignment = {}\n",
    "        \n",
    "        # í´ëŸ¬ìŠ¤í„°ë¥¼ ì„ì–´ì„œ í• ë‹¹\n",
    "        shuffled_clusters = clusters.copy()\n",
    "        np.random.shuffle(shuffled_clusters)\n",
    "        \n",
    "        n_train = max(1, int(n_clusters * train_ratio))\n",
    "        n_val = max(1, int(n_clusters * val_ratio))\n",
    "        \n",
    "        for i, cluster_id in enumerate(shuffled_clusters):\n",
    "            if i < n_train:\n",
    "                assignment[cluster_id] = DataSplitType.TRAIN\n",
    "            elif i < n_train + n_val:\n",
    "                assignment[cluster_id] = DataSplitType.VALIDATION\n",
    "            else:\n",
    "                assignment[cluster_id] = DataSplitType.TEST\n",
    "        \n",
    "        return assignment\n",
    "    \n",
    "    def _calculate_distance(self, coord1: np.ndarray, coord2: np.ndarray) -> float:\n",
    "        \"\"\"ë‘ ì§€ë¦¬ ì¢Œí‘œ ê°„ ê±°ë¦¬ ê³„ì‚° (ë¯¸í„°)\"\"\"\n",
    "        \n",
    "        # í•˜ë²„ì‚¬ì¸ ê³µì‹ ì‚¬ìš©\n",
    "        lat1, lon1 = np.radians(coord1)\n",
    "        lat2, lon2 = np.radians(coord2)\n",
    "        \n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        \n",
    "        # ì§€êµ¬ ë°˜ì§€ë¦„ (ë¯¸í„°)\n",
    "        R = 6371000\n",
    "        distance = R * c\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    def _calculate_distance_to_boundary(self, point: np.ndarray, all_coordinates: np.ndarray,\n",
    "                                       split_assignments: List[DataSplitType], \n",
    "                                       point_split: DataSplitType) -> float:\n",
    "        \"\"\"ë‹¤ë¥¸ ë¶„í• ì˜ ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚°\"\"\"\n",
    "        \n",
    "        min_distance = float('inf')\n",
    "        \n",
    "        for i, (coord, split_type) in enumerate(zip(all_coordinates, split_assignments)):\n",
    "            if split_type != point_split:\n",
    "                distance = self._calculate_distance(point, coord)\n",
    "                min_distance = min(min_distance, distance)\n",
    "        \n",
    "        return min_distance if min_distance != float('inf') else 0.0\n",
    "    \n",
    "    def _evaluate_spatial_split_quality(self, coordinates: np.ndarray, \n",
    "                                       split_assignments: List[DataSplitType]) -> dict:\n",
    "        \"\"\"ê³µê°„ì  ë¶„í• ì˜ í’ˆì§ˆ í‰ê°€\"\"\"\n",
    "        \n",
    "        # ë¶„í• ë³„ë¡œ ì¢Œí‘œ ê·¸ë£¹í™”\n",
    "        split_coords = {\n",
    "            DataSplitType.TRAIN: [],\n",
    "            DataSplitType.VALIDATION: [],\n",
    "            DataSplitType.TEST: []\n",
    "        }\n",
    "        \n",
    "        for coord, split_type in zip(coordinates, split_assignments):\n",
    "            split_coords[split_type].append(coord)\n",
    "        \n",
    "        # ë¶„í•  ê°„ ìµœì†Œ ê±°ë¦¬ ê³„ì‚°\n",
    "        min_distances = {}\n",
    "        for split1 in split_coords:\n",
    "            for split2 in split_coords:\n",
    "                if split1 != split2 and split_coords[split1] and split_coords[split2]:\n",
    "                    coords1 = np.array(split_coords[split1])\n",
    "                    coords2 = np.array(split_coords[split2])\n",
    "                    \n",
    "                    distances = cdist(coords1, coords2, \n",
    "                                    metric=lambda u, v: self._calculate_distance(u, v))\n",
    "                    min_distances[f\"{split1.value}_{split2.value}\"] = float(distances.min())\n",
    "        \n",
    "        # ê³µê°„ì  ìê¸°ìƒê´€ ê³„ì‚° (Moran's I)\n",
    "        spatial_autocorr = self._calculate_spatial_autocorrelation(coordinates, split_assignments)\n",
    "        \n",
    "        return {\n",
    "            'min_distances': min_distances,\n",
    "            'spatial_autocorrelation': spatial_autocorr,\n",
    "            'quality_score': min(min_distances.values()) / 1000.0 if min_distances else 0.0  # km ë‹¨ìœ„\n",
    "        }\n",
    "    \n",
    "    def _calculate_spatial_autocorrelation(self, coordinates: np.ndarray, \n",
    "                                          split_assignments: List[DataSplitType]) -> float:\n",
    "        \"\"\"ê³µê°„ì  ìê¸°ìƒê´€ ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)\"\"\"\n",
    "        \n",
    "        # ë¶„í•  íƒ€ì…ì„ ìˆ«ìë¡œ ë³€í™˜\n",
    "        split_numeric = []\n",
    "        for split_type in split_assignments:\n",
    "            if split_type == DataSplitType.TRAIN:\n",
    "                split_numeric.append(0)\n",
    "            elif split_type == DataSplitType.VALIDATION:\n",
    "                split_numeric.append(1)\n",
    "            else:\n",
    "                split_numeric.append(2)\n",
    "        \n",
    "        split_numeric = np.array(split_numeric)\n",
    "        \n",
    "        # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°\n",
    "        n = len(coordinates)\n",
    "        weights = np.zeros((n, n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    distance = self._calculate_distance(coordinates[i], coordinates[j])\n",
    "                    if distance <= self.buffer_distance:\n",
    "                        weights[i, j] = 1.0 / (distance + 1)  # ì—­ê±°ë¦¬ ê°€ì¤‘ì¹˜\n",
    "        \n",
    "        # Moran's I ê³„ì‚°\n",
    "        W = weights.sum()\n",
    "        if W == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        mean_split = split_numeric.mean()\n",
    "        numerator = 0.0\n",
    "        denominator = 0.0\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                numerator += weights[i, j] * (split_numeric[i] - mean_split) * (split_numeric[j] - mean_split)\n",
    "            denominator += (split_numeric[i] - mean_split) ** 2\n",
    "        \n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        moran_i = (n / W) * (numerator / denominator)\n",
    "        return float(moran_i)\n",
    "    \n",
    "    def _random_spatial_split(self, n_samples: int, \n",
    "                             train_ratio: float, val_ratio: float, test_ratio: float) -> List[DataSplitType]:\n",
    "        \"\"\"ë°±ì—…ìš© ëœë¤ ë¶„í• \"\"\"\n",
    "        \n",
    "        splits = []\n",
    "        n_train = int(n_samples * train_ratio)\n",
    "        n_val = int(n_samples * val_ratio)\n",
    "        \n",
    "        splits.extend([DataSplitType.TRAIN] * n_train)\n",
    "        splits.extend([DataSplitType.VALIDATION] * n_val)\n",
    "        splits.extend([DataSplitType.TEST] * (n_samples - n_train - n_val))\n",
    "        \n",
    "        np.random.shuffle(splits)\n",
    "        return splits\n",
    "\n",
    "\n",
    "# ì „ì—­ ê³µê°„ì  ë¶„í•  ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤\n",
    "spatial_split_engine = SpatialDataSplitEngine(\n",
    "    buffer_distance=settings.SPATIAL_BUFFER_DISTANCE,\n",
    "    min_cluster_size=settings.MIN_SPATIAL_CLUSTER_SIZE\n",
    ")\n",
    "\n",
    "print(\"âœ… ì§€ë¦¬ì •ë³´ ê¸°ë°˜ ê³µê°„ì  ë°ì´í„° ë¶„í•  ì—”ì§„ êµ¬í˜„ ì™„ë£Œ\")\n",
    "print(f\"ğŸŒ ë²„í¼ ê±°ë¦¬: {settings.SPATIAL_BUFFER_DISTANCE}m\")\n",
    "print(f\"ğŸ“ ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {settings.MIN_SPATIAL_CLUSTER_SIZE}\")\n",
    "print(f\"ğŸ”§ ì§€ì› ë°©ë²•: spatial_clustering, grid_based, buffer_based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 5. ëŠ¥ë™ í•™ìŠµ ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearningEngine:\n",
    "    \"\"\"ëŠ¥ë™ í•™ìŠµ ì‹œìŠ¤í…œ - ë¶ˆí™•ì‹¤ì„±ê³¼ ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œ ì„ íƒ\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 50, uncertainty_weight: float = 0.7, \n",
    "                 diversity_weight: float = 0.3):\n",
    "        self.batch_size = batch_size\n",
    "        self.uncertainty_weight = uncertainty_weight\n",
    "        self.diversity_weight = diversity_weight\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "    def select_samples_for_labeling(self, db: Session, dataset_id: str, \n",
    "                                   model_path: str = None, \n",
    "                                   selection_strategy: str = \"uncertainty_diversity\") -> dict:\n",
    "        \"\"\"ë¼ë²¨ë§ì„ ìœ„í•œ ìƒ˜í”Œ ì„ íƒ\"\"\"\n",
    "        \n",
    "        # ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ì´ë¯¸ì§€ ì¡°íšŒ\n",
    "        unlabeled_images = db.query(AnnotatedImage).filter(\n",
    "            AnnotatedImage.dataset_id == dataset_id,\n",
    "            AnnotatedImage.annotation_count == 0,\n",
    "            AnnotatedImage.is_selected_for_labeling == False\n",
    "        ).all()\n",
    "        \n",
    "        if len(unlabeled_images) < self.batch_size:\n",
    "            logger.warning(f\"ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ì´ë¯¸ì§€ê°€ {len(unlabeled_images)}ê°œë¿ì…ë‹ˆë‹¤\")\n",
    "            selected_images = unlabeled_images\n",
    "        else:\n",
    "            if selection_strategy == \"uncertainty\":\n",
    "                selected_images = self._uncertainty_sampling(unlabeled_images, model_path)\n",
    "            elif selection_strategy == \"diversity\":\n",
    "                selected_images = self._diversity_sampling(unlabeled_images)\n",
    "            elif selection_strategy == \"uncertainty_diversity\":\n",
    "                selected_images = self._uncertainty_diversity_sampling(unlabeled_images, model_path)\n",
    "            elif selection_strategy == \"random\":\n",
    "                selected_images = self._random_sampling(unlabeled_images)\n",
    "            else:\n",
    "                raise ValueError(f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì„ íƒ ì „ëµ: {selection_strategy}\")\n",
    "        \n",
    "        # ì„ íƒëœ ì´ë¯¸ì§€ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— í‘œì‹œ\n",
    "        selection_round = self._get_next_selection_round(db, dataset_id)\n",
    "        \n",
    "        for i, image in enumerate(selected_images):\n",
    "            image.is_selected_for_labeling = True\n",
    "            image.selection_priority = float(i + 1)  # ì„ íƒ ìˆœì„œ\n",
    "            \n",
    "            # ëŠ¥ë™ í•™ìŠµ ë¶„í• ë¡œ ì´ë™\n",
    "            existing_split = db.query(DataSplit).filter(\n",
    "                DataSplit.image_id == image.id\n",
    "            ).first()\n",
    "            \n",
    "            if existing_split:\n",
    "                existing_split.split_type = DataSplitType.ACTIVE_LEARNING\n",
    "                existing_split.selection_round = selection_round\n",
    "                existing_split.selection_score = image.selection_priority\n",
    "                existing_split.selection_reason = selection_strategy\n",
    "            else:\n",
    "                new_split = DataSplit(\n",
    "                    dataset_id=dataset_id,\n",
    "                    image_id=image.id,\n",
    "                    split_type=DataSplitType.ACTIVE_LEARNING,\n",
    "                    selection_round=selection_round,\n",
    "                    selection_score=image.selection_priority,\n",
    "                    selection_reason=selection_strategy,\n",
    "                    split_method=\"active_learning\"\n",
    "                )\n",
    "                db.add(new_split)\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ í†µê³„ ì—…ë°ì´íŠ¸\n",
    "        dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()\n",
    "        if dataset:\n",
    "            dataset.active_learning_count += len(selected_images)\n",
    "        \n",
    "        db.commit()\n",
    "        \n",
    "        result = {\n",
    "            'selected_count': len(selected_images),\n",
    "            'selection_round': selection_round,\n",
    "            'selection_strategy': selection_strategy,\n",
    "            'selected_image_ids': [img.id for img in selected_images],\n",
    "            'total_unlabeled': len(unlabeled_images)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"ëŠ¥ë™ í•™ìŠµ ìƒ˜í”Œ ì„ íƒ ì™„ë£Œ: {result}\")\n",
    "        return result\n",
    "    \n",
    "    def _uncertainty_sampling(self, images: List[AnnotatedImage], model_path: str) -> List[AnnotatedImage]:\n",
    "        \"\"\"ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ìƒ˜í”Œë§\"\"\"\n",
    "        \n",
    "        if not model_path or not Path(model_path).exists():\n",
    "            logger.warning(\"ëª¨ë¸ì´ ì—†ì–´ ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ì „í™˜\")\n",
    "            return self._random_sampling(images)\n",
    "        \n",
    "        try:\n",
    "            # ëª¨ë¸ ë¡œë“œ\n",
    "            model = YOLO(model_path)\n",
    "            \n",
    "            uncertainty_scores = []\n",
    "            \n",
    "            for image in images:\n",
    "                try:\n",
    "                    # ì´ë¯¸ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "                    results = model(image.file_path, verbose=False)\n",
    "                    \n",
    "                    # ë¶ˆí™•ì‹¤ì„± ê³„ì‚° (ì—¬ëŸ¬ ë°©ë²• ì¡°í•©)\n",
    "                    uncertainty = self._calculate_uncertainty(results)\n",
    "                    uncertainty_scores.append((image, uncertainty))\n",
    "                    \n",
    "                    # ë°ì´í„°ë² ì´ìŠ¤ì— ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ ì €ì¥\n",
    "                    image.uncertainty_score = uncertainty\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"ì´ë¯¸ì§€ {image.filename} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}\")\n",
    "                    uncertainty_scores.append((image, 0.5))  # ê¸°ë³¸ê°’\n",
    "            \n",
    "            # ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "            uncertainty_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # ìƒìœ„ batch_sizeê°œ ì„ íƒ\n",
    "            selected = [img for img, _ in uncertainty_scores[:self.batch_size]]\n",
    "            \n",
    "            return selected\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ë¶ˆí™•ì‹¤ì„± ìƒ˜í”Œë§ ì‹¤íŒ¨: {e}\")\n",
    "            return self._random_sampling(images)\n",
    "    \n",
    "    def _diversity_sampling(self, images: List[AnnotatedImage]) -> List[AnnotatedImage]:\n",
    "        \"\"\"ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ\n",
    "            features = self._extract_image_features(images)\n",
    "            \n",
    "            if features is None or len(features) == 0:\n",
    "                return self._random_sampling(images)\n",
    "            \n",
    "            # K-means clusteringìœ¼ë¡œ ë‹¤ì–‘í•œ í´ëŸ¬ìŠ¤í„°ì—ì„œ ì„ íƒ\n",
    "            n_clusters = min(self.batch_size, len(images))\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            cluster_labels = kmeans.fit_predict(features)\n",
    "            \n",
    "            # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œ ì„ íƒ\n",
    "            selected_indices = []\n",
    "            for cluster_id in range(n_clusters):\n",
    "                cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "                if len(cluster_indices) > 0:\n",
    "                    # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ ì°¾ê¸°\n",
    "                    cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                    cluster_features = features[cluster_indices]\n",
    "                    distances = np.linalg.norm(cluster_features - cluster_center, axis=1)\n",
    "                    closest_idx = cluster_indices[np.argmin(distances)]\n",
    "                    selected_indices.append(closest_idx)\n",
    "            \n",
    "            # ì„ íƒëœ ì´ë¯¸ì§€ë“¤ì˜ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚° ë° ì €ì¥\n",
    "            selected_images = []\n",
    "            for idx in selected_indices[:self.batch_size]:\n",
    "                image = images[idx]\n",
    "                diversity_score = self._calculate_diversity_score(features[idx], features, selected_indices)\n",
    "                image.diversity_score = diversity_score\n",
    "                selected_images.append(image)\n",
    "            \n",
    "            return selected_images\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ë‹¤ì–‘ì„± ìƒ˜í”Œë§ ì‹¤íŒ¨: {e}\")\n",
    "            return self._random_sampling(images)\n",
    "    \n",
    "    def _uncertainty_diversity_sampling(self, images: List[AnnotatedImage], model_path: str) -> List[AnnotatedImage]:\n",
    "        \"\"\"ë¶ˆí™•ì‹¤ì„±ê³¼ ë‹¤ì–‘ì„±ì„ ê²°í•©í•œ ìƒ˜í”Œë§\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ ê³„ì‚°\n",
    "            uncertainty_images = self._uncertainty_sampling(images, model_path)\n",
    "            \n",
    "            # ë¶ˆí™•ì‹¤ì„± ìƒìœ„ í›„ë³´ë“¤ì—ì„œ ë‹¤ì–‘ì„± ê¸°ë°˜ ìµœì¢… ì„ íƒ\n",
    "            if len(uncertainty_images) > self.batch_size:\n",
    "                # ë¶ˆí™•ì‹¤ì„± ìƒìœ„ 2ë°°ìˆ˜ì—ì„œ ë‹¤ì–‘ì„± ê¸°ë°˜ ì„ íƒ\n",
    "                top_uncertain = uncertainty_images[:self.batch_size * 2]\n",
    "                final_selection = self._diversity_sampling(top_uncertain)\n",
    "            else:\n",
    "                final_selection = uncertainty_images\n",
    "            \n",
    "            # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ë¶ˆí™•ì‹¤ì„± + ë‹¤ì–‘ì„±)\n",
    "            for image in final_selection:\n",
    "                uncertainty = getattr(image, 'uncertainty_score', 0.5)\n",
    "                diversity = getattr(image, 'diversity_score', 0.5)\n",
    "                \n",
    "                combined_score = (\n",
    "                    self.uncertainty_weight * uncertainty + \n",
    "                    self.diversity_weight * diversity\n",
    "                )\n",
    "                image.selection_priority = combined_score\n",
    "            \n",
    "            # ìµœì¢… ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "            final_selection.sort(key=lambda x: x.selection_priority, reverse=True)\n",
    "            \n",
    "            return final_selection[:self.batch_size]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ë¶ˆí™•ì‹¤ì„±-ë‹¤ì–‘ì„± ìƒ˜í”Œë§ ì‹¤íŒ¨: {e}\")\n",
    "            return self._random_sampling(images)\n",
    "    \n",
    "    def _random_sampling(self, images: List[AnnotatedImage]) -> List[AnnotatedImage]:\n",
    "        \"\"\"ëœë¤ ìƒ˜í”Œë§ (ë°±ì—… ë°©ë²•)\"\"\"\n",
    "        \n",
    "        selected = np.random.choice(\n",
    "            images, \n",
    "            size=min(self.batch_size, len(images)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # ëœë¤ ì ìˆ˜ í• ë‹¹\n",
    "        for i, image in enumerate(selected):\n",
    "            image.uncertainty_score = 0.5\n",
    "            image.diversity_score = 0.5\n",
    "            image.selection_priority = float(i + 1)\n",
    "        \n",
    "        return list(selected)\n",
    "    \n",
    "    def _calculate_uncertainty(self, results) -> float:\n",
    "        \"\"\"YOLO ê²°ê³¼ì—ì„œ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°\"\"\"\n",
    "        \n",
    "        if not results or len(results) == 0:\n",
    "            return 1.0  # ì˜ˆì¸¡ ì—†ìŒ = ë†’ì€ ë¶ˆí™•ì‹¤ì„±\n",
    "        \n",
    "        result = results[0]\n",
    "        \n",
    "        if result.boxes is None or len(result.boxes) == 0:\n",
    "            return 0.8  # ê°ì²´ ì—†ìŒ = ë†’ì€ ë¶ˆí™•ì‹¤ì„±\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±\n",
    "        confidences = result.boxes.conf.cpu().numpy()\n",
    "        \n",
    "        # ì—¬ëŸ¬ ë¶ˆí™•ì‹¤ì„± ì§€í‘œ ê³„ì‚°\n",
    "        # 1. í‰ê·  ì‹ ë¢°ë„ì˜ ì—­ìˆ˜\n",
    "        mean_conf = np.mean(confidences)\n",
    "        conf_uncertainty = 1.0 - mean_conf\n",
    "        \n",
    "        # 2. ì‹ ë¢°ë„ ë¶„ì‚° (ì¼ê´€ì„± ë¶€ì¡±)\n",
    "        conf_variance = np.var(confidences) if len(confidences) > 1 else 0.0\n",
    "        \n",
    "        # 3. ì˜ˆì¸¡ ê°œìˆ˜ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±\n",
    "        count_uncertainty = 1.0 / (1.0 + len(confidences))  # ë„ˆë¬´ ë§ê±°ë‚˜ ì ì€ ì˜ˆì¸¡\n",
    "        \n",
    "        # ì¢…í•© ë¶ˆí™•ì‹¤ì„± ê³„ì‚°\n",
    "        total_uncertainty = (\n",
    "            0.6 * conf_uncertainty + \n",
    "            0.3 * conf_variance + \n",
    "            0.1 * count_uncertainty\n",
    "        )\n",
    "        \n",
    "        return float(np.clip(total_uncertainty, 0.0, 1.0))\n",
    "    \n",
    "    def _extract_image_features(self, images: List[AnnotatedImage]) -> np.ndarray:\n",
    "        \"\"\"ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ (ë‹¤ì–‘ì„± ê³„ì‚°ìš©)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            features = []\n",
    "            \n",
    "            for image in images:\n",
    "                try:\n",
    "                    # ì´ë¯¸ì§€ ë¡œë“œ ë° ë¦¬ì‚¬ì´ì¦ˆ\n",
    "                    img = cv2.imread(image.file_path)\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    \n",
    "                    img = cv2.resize(img, (224, 224))\n",
    "                    \n",
    "                    # ì—¬ëŸ¬ íŠ¹ì§• ì¶”ì¶œ\n",
    "                    # 1. ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨\n",
    "                    color_hist = self._calculate_color_histogram(img)\n",
    "                    \n",
    "                    # 2. í…ìŠ¤ì²˜ íŠ¹ì§• (LBP)\n",
    "                    texture_features = self._calculate_texture_features(img)\n",
    "                    \n",
    "                    # 3. ê¸°ë³¸ í†µê³„ íŠ¹ì§•\n",
    "                    stat_features = self._calculate_statistical_features(img)\n",
    "                    \n",
    "                    # íŠ¹ì§• ê²°í•©\n",
    "                    combined_features = np.concatenate([\n",
    "                        color_hist, texture_features, stat_features\n",
    "                    ])\n",
    "                    \n",
    "                    features.append(combined_features)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"ì´ë¯¸ì§€ {image.filename} íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "                    # ê¸°ë³¸ íŠ¹ì§• ë²¡í„° ì‚¬ìš©\n",
    "                    features.append(np.zeros(128))\n",
    "            \n",
    "            if len(features) == 0:\n",
    "                return None\n",
    "            \n",
    "            features_array = np.array(features)\n",
    "            \n",
    "            # íŠ¹ì§• ì •ê·œí™”\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            normalized_features = scaler.fit_transform(features_array)\n",
    "            \n",
    "            return normalized_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _calculate_color_histogram(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°\"\"\"\n",
    "        \n",
    "        # RGB ê° ì±„ë„ì˜ íˆìŠ¤í† ê·¸ë¨\n",
    "        histograms = []\n",
    "        for i in range(3):\n",
    "            hist = cv2.calcHist([img], [i], None, [16], [0, 256])\n",
    "            hist = hist.flatten() / hist.sum()  # ì •ê·œí™”\n",
    "            histograms.append(hist)\n",
    "        \n",
    "        return np.concatenate(histograms)\n",
    "    \n",
    "    def _calculate_texture_features(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"í…ìŠ¤ì²˜ íŠ¹ì§• ê³„ì‚° (ë‹¨ìˆœí™”ëœ LBP)\"\"\"\n",
    "        \n",
    "        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í…ìŠ¤ì²˜ íŠ¹ì§•ë“¤\n",
    "        # 1. ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°\n",
    "        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "        \n",
    "        # 2. ë¼í”Œë¼ì‹œì•ˆ\n",
    "        laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "        \n",
    "        # í†µê³„ íŠ¹ì§•\n",
    "        features = [\n",
    "            np.mean(grad_magnitude),\n",
    "            np.std(grad_magnitude),\n",
    "            np.mean(np.abs(laplacian)),\n",
    "            np.std(laplacian)\n",
    "        ]\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def _calculate_statistical_features(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"í†µê³„ì  íŠ¹ì§• ê³„ì‚°\"\"\"\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for channel in range(3):\n",
    "            ch_data = img[:, :, channel].flatten()\n",
    "            \n",
    "            features.extend([\n",
    "                np.mean(ch_data),\n",
    "                np.std(ch_data),\n",
    "                np.median(ch_data),\n",
    "                stats.skew(ch_data),\n",
    "                stats.kurtosis(ch_data)\n",
    "            ])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def _calculate_diversity_score(self, feature: np.ndarray, all_features: np.ndarray, \n",
    "                                  selected_indices: List[int]) -> float:\n",
    "        \"\"\"ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "        \n",
    "        if len(selected_indices) <= 1:\n",
    "            return 1.0\n",
    "        \n",
    "        # ì„ íƒëœ ë‹¤ë¥¸ ìƒ˜í”Œë“¤ê³¼ì˜ í‰ê·  ê±°ë¦¬\n",
    "        distances = []\n",
    "        for idx in selected_indices:\n",
    "            if not np.array_equal(feature, all_features[idx]):\n",
    "                dist = np.linalg.norm(feature - all_features[idx])\n",
    "                distances.append(dist)\n",
    "        \n",
    "        return float(np.mean(distances)) if distances else 0.5\n",
    "    \n",
    "    def _get_next_selection_round(self, db: Session, dataset_id: str) -> int:\n",
    "        \"\"\"ë‹¤ìŒ ì„ íƒ ë¼ìš´ë“œ ë²ˆí˜¸ ë°˜í™˜\"\"\"\n",
    "        \n",
    "        max_round = db.query(DataSplit).filter(\n",
    "            DataSplit.dataset_id == dataset_id,\n",
    "            DataSplit.split_type == DataSplitType.ACTIVE_LEARNING\n",
    "        ).with_entities(DataSplit.selection_round).order_by(\n",
    "            DataSplit.selection_round.desc()\n",
    "        ).first()\n",
    "        \n",
    "        return (max_round[0] + 1) if max_round and max_round[0] else 1\n",
    "    \n",
    "    def evaluate_selection_quality(self, db: Session, dataset_id: str, \n",
    "                                  selection_round: int) -> dict:\n",
    "        \"\"\"ëŠ¥ë™ í•™ìŠµ ì„ íƒ í’ˆì§ˆ í‰ê°€\"\"\"\n",
    "        \n",
    "        # í•´ë‹¹ ë¼ìš´ë“œì—ì„œ ì„ íƒëœ ì´ë¯¸ì§€ë“¤\n",
    "        selected_splits = db.query(DataSplit).filter(\n",
    "            DataSplit.dataset_id == dataset_id,\n",
    "            DataSplit.split_type == DataSplitType.ACTIVE_LEARNING,\n",
    "            DataSplit.selection_round == selection_round\n",
    "        ).all()\n",
    "        \n",
    "        if not selected_splits:\n",
    "            return {\"error\": \"ì„ íƒëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤\"}\n",
    "        \n",
    "        # ì„ íƒëœ ì´ë¯¸ì§€ë“¤ì˜ í†µê³„\n",
    "        selected_images = []\n",
    "        for split in selected_splits:\n",
    "            image = db.query(AnnotatedImage).filter(\n",
    "                AnnotatedImage.id == split.image_id\n",
    "            ).first()\n",
    "            if image:\n",
    "                selected_images.append(image)\n",
    "        \n",
    "        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°\n",
    "        uncertainty_scores = [img.uncertainty_score or 0.5 for img in selected_images]\n",
    "        diversity_scores = [img.diversity_score or 0.5 for img in selected_images]\n",
    "        \n",
    "        quality_metrics = {\n",
    "            'selection_round': selection_round,\n",
    "            'selected_count': len(selected_images),\n",
    "            'average_uncertainty': float(np.mean(uncertainty_scores)),\n",
    "            'uncertainty_std': float(np.std(uncertainty_scores)),\n",
    "            'average_diversity': float(np.mean(diversity_scores)),\n",
    "            'diversity_std': float(np.std(diversity_scores)),\n",
    "            'selection_efficiency': self._calculate_selection_efficiency(selected_images)\n",
    "        }\n",
    "        \n",
    "        return quality_metrics\n",
    "    \n",
    "    def _calculate_selection_efficiency(self, selected_images: List[AnnotatedImage]) -> float:\n",
    "        \"\"\"ì„ íƒ íš¨ìœ¨ì„± ê³„ì‚°\"\"\"\n",
    "        \n",
    "        # ê°„ë‹¨í•œ íš¨ìœ¨ì„± ì§€í‘œ: ë¶ˆí™•ì‹¤ì„±ê³¼ ë‹¤ì–‘ì„±ì˜ ê· í˜•\n",
    "        uncertainty_scores = [img.uncertainty_score or 0.5 for img in selected_images]\n",
    "        diversity_scores = [img.diversity_score or 0.5 for img in selected_images]\n",
    "        \n",
    "        # ë†’ì€ ë¶ˆí™•ì‹¤ì„±ê³¼ ë†’ì€ ë‹¤ì–‘ì„±ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ë¹„ìœ¨\n",
    "        high_uncertainty = np.array(uncertainty_scores) > 0.6\n",
    "        high_diversity = np.array(diversity_scores) > 0.6\n",
    "        \n",
    "        efficient_selections = np.sum(high_uncertainty & high_diversity)\n",
    "        efficiency = efficient_selections / len(selected_images) if selected_images else 0.0\n",
    "        \n",
    "        return float(efficiency)\n",
    "\n",
    "\n",
    "# ì „ì—­ ëŠ¥ë™ í•™ìŠµ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤\n",
    "active_learning_engine = ActiveLearningEngine(\n",
    "    batch_size=settings.ACTIVE_LEARNING_BATCH_SIZE,\n",
    "    uncertainty_weight=1.0 - settings.DIVERSITY_WEIGHT,\n",
    "    diversity_weight=settings.DIVERSITY_WEIGHT\n",
    ")\n",
    "\n",
    "print(\"âœ… ëŠ¥ë™ í•™ìŠµ ì‹œìŠ¤í…œ êµ¬í˜„ ì™„ë£Œ\")\n",
    "print(f\"ğŸ¯ ë°°ì¹˜ í¬ê¸°: {settings.ACTIVE_LEARNING_BATCH_SIZE}\")\n",
    "print(f\"âš–ï¸ ê°€ì¤‘ì¹˜ - ë¶ˆí™•ì‹¤ì„±: {1.0 - settings.DIVERSITY_WEIGHT:.1f}, ë‹¤ì–‘ì„±: {settings.DIVERSITY_WEIGHT:.1f}\")\n",
    "print(f\"ğŸ”§ ì§€ì› ì „ëµ: uncertainty, diversity, uncertainty_diversity, random\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}