{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Nong-View AI ë†ì—…ì˜ìƒë¶„ì„ í”Œë«í¼ - ML í›ˆë ¨ í†µí•© ë…¸íŠ¸ë¶ v3.0\n",
    "\n",
    "**í”„ë¡œì íŠ¸ í˜„í™©**: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê¸°ë°˜ ML í›ˆë ¨ ì‹œìŠ¤í…œ êµ¬í˜„  \n",
    "**í•µì‹¬ ê°œì„ **: ì˜¬ë°”ë¥¸ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  êµ¬í˜„  \n",
    "**ê°œë°œ ë‚ ì§œ**: 2025-10-27  \n",
    "**ë²„ì „**: v3.0\n",
    "\n",
    "## ğŸ“‹ ì£¼ìš” ê°œì„ ì‚¬í•­\n",
    "- âœ… ML í›ˆë ¨ìš© ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ì¶”ê°€\n",
    "- âœ… ì˜¬ë°”ë¥¸ Train/Val/Test ë¶„í•  ë¡œì§\n",
    "- âœ… ë¼ë²¨ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ\n",
    "- âœ… ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸\n",
    "- âœ… ë°ì´í„° ì¦ê°• ì§€ì›"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ 1. í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML í›ˆë ¨ì„ ìœ„í•œ ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install scikit-learn albumentations opencv-python\n",
    "!pip install torch torchvision torchaudio ultralytics\n",
    "!pip install wandb tensorboard matplotlib seaborn\n",
    "!pip install fastapi uvicorn sqlalchemy alembic\n",
    "!pip install rasterio geopandas shapely fiona\n",
    "!pip install pydantic[email] python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any, Union, Tuple\n",
    "from pathlib import Path\n",
    "from uuid import UUID, uuid4\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# ë°ì´í„° ì²˜ë¦¬ ë° ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ì§€ë¦¬ì •ë³´ ì²˜ë¦¬\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, mapping\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "# ì›¹ í”„ë ˆì„ì›Œí¬\n",
    "from fastapi import FastAPI, HTTPException, Depends, UploadFile, File, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, FileResponse\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤\n",
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime, Float, Text, Boolean, ForeignKey, JSON, Enum\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session, relationship\n",
    "from sqlalchemy.dialects.postgresql import UUID as PG_UUID\n",
    "import enum\n",
    "\n",
    "# AI/ML\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. ì„¤ì • ë° êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML í›ˆë ¨ìš© ì„¤ì • í´ë˜ìŠ¤ ì •ì˜\n",
    "class MLTrainingSettings:\n",
    "    \"\"\"ML í›ˆë ¨ ê´€ë ¨ ì„¤ì •\"\"\"\n",
    "    PROJECT_NAME: str = \"Nong-View ML Training\"\n",
    "    VERSION: str = \"3.0.0\"\n",
    "    API_V1_STR: str = \"/api/v1\"\n",
    "    ENVIRONMENT: str = \"development\"\n",
    "    DEBUG: bool = True\n",
    "    \n",
    "    # ì„œë²„ ì„¤ì •\n",
    "    HOST: str = \"127.0.0.1\"\n",
    "    PORT: int = 8000\n",
    "    \n",
    "    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •\n",
    "    DATABASE_URL: str = \"sqlite:///./nongview_ml_training.db\"\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥ ì„¤ì •\n",
    "    DATA_ROOT: str = \"./data\"\n",
    "    DATASET_PATH: str = \"./data/datasets\"\n",
    "    ANNOTATIONS_PATH: str = \"./data/annotations\"\n",
    "    MODELS_PATH: str = \"./data/models\"\n",
    "    LOGS_PATH: str = \"./data/logs\"\n",
    "    CACHE_PATH: str = \"./data/cache\"\n",
    "    \n",
    "    # ML í›ˆë ¨ ì„¤ì •\n",
    "    TRAIN_SPLIT: float = 0.7\n",
    "    VAL_SPLIT: float = 0.2\n",
    "    TEST_SPLIT: float = 0.1\n",
    "    RANDOM_SEED: int = 42\n",
    "    \n",
    "    # YOLO ì„¤ì •\n",
    "    IMAGE_SIZE: int = 640\n",
    "    BATCH_SIZE: int = 16\n",
    "    EPOCHS: int = 100\n",
    "    LEARNING_RATE: float = 0.01\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ì •ì˜\n",
    "    CROP_CLASSES: List[str] = [\n",
    "        'IRG',        # ì´íƒˆë¦¬ì•ˆ ë¼ì´ê·¸ë¼ìŠ¤\n",
    "        'BARLEY',     # ë³´ë¦¬\n",
    "        'WHEAT',      # ë°€\n",
    "        'CORN_SILAGE', # ì˜¥ìˆ˜ìˆ˜ì‚¬ì¼ë¦¬ì§€\n",
    "        'HAY',        # ê±´ì´ˆ\n",
    "        'FALLOW'      # íœ´ê²½ì§€\n",
    "    ]\n",
    "    \n",
    "    FACILITY_CLASSES: List[str] = [\n",
    "        'GREENHOUSE_SINGLE',  # ë‹¨ë™ ë¹„ë‹í•˜ìš°ìŠ¤\n",
    "        'GREENHOUSE_MULTI',   # ì—°ë™ ë¹„ë‹í•˜ìš°ìŠ¤\n",
    "        'STORAGE',            # ì €ì¥ì‹œì„¤\n",
    "        'LIVESTOCK',          # ì¶•ì‚¬\n",
    "        'SILO',              # ì‚¬ì¼ë¡œ\n",
    "        'MACHINERY'          # ë†ê¸°ê³„\n",
    "    ]\n",
    "\n",
    "settings = MLTrainingSettings()\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "for path in [settings.DATA_ROOT, settings.DATASET_PATH, settings.ANNOTATIONS_PATH, \n",
    "             settings.MODELS_PATH, settings.LOGS_PATH, settings.CACHE_PATH]:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "random.seed(settings.RANDOM_SEED)\n",
    "np.random.seed(settings.RANDOM_SEED)\n",
    "torch.manual_seed(settings.RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(settings.RANDOM_SEED)\n",
    "\n",
    "print(\"âœ… ML í›ˆë ¨ ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"ğŸ“ ë°ì´í„° ë£¨íŠ¸: {settings.DATA_ROOT}\")\n",
    "print(f\"ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤: {settings.DATABASE_URL}\")\n",
    "print(f\"ğŸ² ëœë¤ ì‹œë“œ: {settings.RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—„ï¸ 3. ML í›ˆë ¨ìš© ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy ì„¤ì •\n",
    "engine = create_engine(\n",
    "    settings.DATABASE_URL,\n",
    "    connect_args={\"check_same_thread\": False} if \"sqlite\" in settings.DATABASE_URL else {}\n",
    ")\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "\n",
    "# ì—´ê±°í˜• ì •ì˜\n",
    "class DataSplitType(enum.Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VALIDATION = \"validation\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "class AnnotationType(enum.Enum):\n",
    "    BOUNDING_BOX = \"bbox\"\n",
    "    SEGMENTATION = \"segmentation\"\n",
    "    CLASSIFICATION = \"classification\"\n",
    "\n",
    "class ModelType(enum.Enum):\n",
    "    YOLO_DETECTION = \"yolo_detection\"\n",
    "    YOLO_SEGMENTATION = \"yolo_segmentation\"\n",
    "    CLASSIFICATION = \"classification\"\n",
    "\n",
    "# ML í›ˆë ¨ìš© ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ì •ì˜\n",
    "class Dataset(Base):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"datasets\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    name = Column(String(200), nullable=False, unique=True)\n",
    "    description = Column(Text)\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ë©”íƒ€ì •ë³´\n",
    "    task_type = Column(String(50), nullable=False)  # crop, facility, landuse\n",
    "    annotation_type = Column(Enum(AnnotationType), nullable=False)\n",
    "    class_names = Column(JSON)  # List[str]\n",
    "    \n",
    "    # í†µê³„\n",
    "    total_images = Column(Integer, default=0)\n",
    "    total_annotations = Column(Integer, default=0)\n",
    "    \n",
    "    # ë¶„í•  ì •ë³´\n",
    "    train_count = Column(Integer, default=0)\n",
    "    val_count = Column(Integer, default=0)\n",
    "    test_count = Column(Integer, default=0)\n",
    "    \n",
    "    # ë°ì´í„° ê²½ë¡œ\n",
    "    data_path = Column(String(500))\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    annotated_images = relationship(\"AnnotatedImage\", back_populates=\"dataset\", cascade=\"all, delete-orphan\")\n",
    "    data_splits = relationship(\"DataSplit\", back_populates=\"dataset\", cascade=\"all, delete-orphan\")\n",
    "    training_jobs = relationship(\"TrainingJob\", back_populates=\"dataset\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "\n",
    "class AnnotatedImage(Base):\n",
    "    \"\"\"ë¼ë²¨ë§ëœ ì´ë¯¸ì§€ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"annotated_images\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    dataset_id = Column(String, ForeignKey(\"datasets.id\"), nullable=False)\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ì •ë³´\n",
    "    filename = Column(String(255), nullable=False)\n",
    "    file_path = Column(String(500), nullable=False)\n",
    "    file_size = Column(Integer)\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°\n",
    "    width = Column(Integer)\n",
    "    height = Column(Integer)\n",
    "    channels = Column(Integer)\n",
    "    \n",
    "    # ì§€ë¦¬ì •ë³´ (ì„ íƒì )\n",
    "    crs = Column(String(50))\n",
    "    bounds = Column(JSON)\n",
    "    \n",
    "    # ë¼ë²¨ë§ ì •ë³´\n",
    "    annotation_count = Column(Integer, default=0)\n",
    "    is_validated = Column(Boolean, default=False)\n",
    "    validation_score = Column(Float)  # ë¼ë²¨ë§ í’ˆì§ˆ ì ìˆ˜\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„°\n",
    "    metadata = Column(JSON)  # ì¶”ê°€ ì •ë³´ (ì´¬ì˜ì¼ì‹œ, ë“œë¡  ì •ë³´ ë“±)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    dataset = relationship(\"Dataset\", back_populates=\"annotated_images\")\n",
    "    annotations = relationship(\"Annotation\", back_populates=\"image\", cascade=\"all, delete-orphan\")\n",
    "    data_split = relationship(\"DataSplit\", back_populates=\"image\", uselist=False)\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "\n",
    "class Annotation(Base):\n",
    "    \"\"\"ì–´ë…¸í…Œì´ì…˜ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"annotations\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    image_id = Column(String, ForeignKey(\"annotated_images.id\"), nullable=False)\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ì •ë³´\n",
    "    class_id = Column(Integer, nullable=False)\n",
    "    class_name = Column(String(100), nullable=False)\n",
    "    \n",
    "    # ì–´ë…¸í…Œì´ì…˜ íƒ€ì…\n",
    "    annotation_type = Column(Enum(AnnotationType), nullable=False)\n",
    "    \n",
    "    # ë°”ìš´ë”© ë°•ìŠ¤ (normalized coordinates 0-1)\n",
    "    bbox_x = Column(Float)  # center x\n",
    "    bbox_y = Column(Float)  # center y\n",
    "    bbox_w = Column(Float)  # width\n",
    "    bbox_h = Column(Float)  # height\n",
    "    \n",
    "    # ì„¸ê·¸ë©˜í…Œì´ì…˜ (ì„ íƒì )\n",
    "    segmentation_points = Column(JSON)  # List of normalized points\n",
    "    \n",
    "    # ì–´ë…¸í…Œì´ì…˜ ë©”íƒ€ì •ë³´\n",
    "    confidence = Column(Float, default=1.0)  # ë¼ë²¨ë§ ì‹ ë¢°ë„\n",
    "    is_difficult = Column(Boolean, default=False)  # ì–´ë ¤ìš´ ê°ì²´ í‘œì‹œ\n",
    "    is_crowd = Column(Boolean, default=False)  # êµ°ì§‘ ê°ì²´ í‘œì‹œ\n",
    "    \n",
    "    # ë¼ë²¨ë§ ì •ë³´\n",
    "    annotator_id = Column(String(100))  # ë¼ë²¨ë§ ì‘ì—…ì ID\n",
    "    annotation_tool = Column(String(100))  # ì‚¬ìš©ëœ ë¼ë²¨ë§ ë„êµ¬\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    image = relationship(\"AnnotatedImage\", back_populates=\"annotations\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "\n",
    "class DataSplit(Base):\n",
    "    \"\"\"ë°ì´í„° ë¶„í•  ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"data_splits\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    dataset_id = Column(String, ForeignKey(\"datasets.id\"), nullable=False)\n",
    "    image_id = Column(String, ForeignKey(\"annotated_images.id\"), nullable=False, unique=True)\n",
    "    \n",
    "    # ë¶„í•  íƒ€ì…\n",
    "    split_type = Column(Enum(DataSplitType), nullable=False)\n",
    "    \n",
    "    # ë¶„í•  ì •ë³´\n",
    "    split_version = Column(String(50), default=\"v1.0\")  # ë¶„í•  ë²„ì „ ê´€ë¦¬\n",
    "    stratify_key = Column(String(100))  # ê³„ì¸µí™” ë¶„í• ì— ì‚¬ìš©ëœ í‚¤\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    dataset = relationship(\"Dataset\", back_populates=\"data_splits\")\n",
    "    image = relationship(\"AnnotatedImage\", back_populates=\"data_split\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "\n",
    "class TrainingJob(Base):\n",
    "    \"\"\"ëª¨ë¸ í›ˆë ¨ ì‘ì—… ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"training_jobs\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    dataset_id = Column(String, ForeignKey(\"datasets.id\"), nullable=False)\n",
    "    \n",
    "    # ì‘ì—… ì •ë³´\n",
    "    job_name = Column(String(200), nullable=False)\n",
    "    description = Column(Text)\n",
    "    status = Column(String(20), default=\"pending\")  # pending, running, completed, failed, stopped\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´\n",
    "    model_type = Column(Enum(ModelType), nullable=False)\n",
    "    base_model = Column(String(100))  # yolov8n, yolov8s, etc.\n",
    "    pretrained_weights = Column(String(500))  # ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ê²½ë¡œ\n",
    "    \n",
    "    # í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    hyperparameters = Column(JSON)  # epochs, batch_size, lr, etc.\n",
    "    \n",
    "    # í›ˆë ¨ ì§„í–‰ ìƒí™©\n",
    "    current_epoch = Column(Integer, default=0)\n",
    "    total_epochs = Column(Integer)\n",
    "    progress_percent = Column(Float, default=0.0)\n",
    "    \n",
    "    # ì„±ëŠ¥ ì§€í‘œ\n",
    "    best_map50 = Column(Float)  # Best mAP@0.5\n",
    "    best_map50_95 = Column(Float)  # Best mAP@0.5:0.95\n",
    "    final_loss = Column(Float)\n",
    "    \n",
    "    # íŒŒì¼ ê²½ë¡œ\n",
    "    output_dir = Column(String(500))\n",
    "    best_weights_path = Column(String(500))\n",
    "    last_weights_path = Column(String(500))\n",
    "    log_file_path = Column(String(500))\n",
    "    \n",
    "    # ì‹œê°„ ì •ë³´\n",
    "    started_at = Column(DateTime)\n",
    "    completed_at = Column(DateTime)\n",
    "    training_duration = Column(Float)  # ì´ í›ˆë ¨ ì‹œê°„ (ì´ˆ)\n",
    "    \n",
    "    # ì—ëŸ¬ ì •ë³´\n",
    "    error_message = Column(Text)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    dataset = relationship(\"Dataset\", back_populates=\"training_jobs\")\n",
    "    evaluations = relationship(\"ModelEvaluation\", back_populates=\"training_job\", cascade=\"all, delete-orphan\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "\n",
    "class ModelEvaluation(Base):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ ê²°ê³¼ ëª¨ë¸\"\"\"\n",
    "    __tablename__ = \"model_evaluations\"\n",
    "    \n",
    "    id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n",
    "    training_job_id = Column(String, ForeignKey(\"training_jobs.id\"), nullable=False)\n",
    "    \n",
    "    # í‰ê°€ ì •ë³´\n",
    "    evaluation_type = Column(String(50), nullable=False)  # validation, test\n",
    "    epoch = Column(Integer)  # í•´ë‹¹ ì—í¬í¬ (validationì˜ ê²½ìš°)\n",
    "    \n",
    "    # ì„±ëŠ¥ ì§€í‘œ\n",
    "    map50 = Column(Float)  # mAP@0.5\n",
    "    map50_95 = Column(Float)  # mAP@0.5:0.95\n",
    "    precision = Column(Float)\n",
    "    recall = Column(Float)\n",
    "    f1_score = Column(Float)\n",
    "    \n",
    "    # ì†ì‹¤ í•¨ìˆ˜\n",
    "    box_loss = Column(Float)\n",
    "    cls_loss = Column(Float)\n",
    "    dfl_loss = Column(Float)\n",
    "    total_loss = Column(Float)\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥\n",
    "    class_metrics = Column(JSON)  # í´ë˜ìŠ¤ë³„ precision, recall, AP\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬\n",
    "    confusion_matrix = Column(JSON)\n",
    "    \n",
    "    # ê´€ê³„\n",
    "    training_job = relationship(\"TrainingJob\", back_populates=\"evaluations\")\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "print(\"âœ… ML í›ˆë ¨ìš© ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"ğŸ“Š ìƒì„±ëœ í…Œì´ë¸”: {list(Base.metadata.tables.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— 4. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì˜ì¡´ì„±\n",
    "def get_db():\n",
    "    \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„±\"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤\n",
    "class DatabaseService:\n",
    "    \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ì„œë¹„ìŠ¤\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_session() -> Session:\n",
    "        \"\"\"ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ë°˜í™˜\"\"\"\n",
    "        return SessionLocal()\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_dataset(db: Session, name: str, task_type: str, \n",
    "                      annotation_type: AnnotationType, class_names: List[str],\n",
    "                      description: str = None) -> Dataset:\n",
    "        \"\"\"ìƒˆ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "        dataset = Dataset(\n",
    "            name=name,\n",
    "            description=description,\n",
    "            task_type=task_type,\n",
    "            annotation_type=annotation_type,\n",
    "            class_names=class_names,\n",
    "            data_path=str(Path(settings.DATASET_PATH) / name)\n",
    "        )\n",
    "        db.add(dataset)\n",
    "        db.commit()\n",
    "        db.refresh(dataset)\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        Path(dataset.data_path).mkdir(parents=True, exist_ok=True)\n",
    "        Path(dataset.data_path, \"images\").mkdir(exist_ok=True)\n",
    "        Path(dataset.data_path, \"labels\").mkdir(exist_ok=True)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_annotated_image(db: Session, dataset_id: str, \n",
    "                           image_path: str, annotations: List[dict]) -> AnnotatedImage:\n",
    "        \"\"\"ë¼ë²¨ë§ëœ ì´ë¯¸ì§€ ì¶”ê°€\"\"\"\n",
    "        # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "        channels = len(img.getbands())\n",
    "        file_size = Path(image_path).stat().st_size\n",
    "        \n",
    "        # ë°ì´í„°ë² ì´ìŠ¤ì— ì´ë¯¸ì§€ ì •ë³´ ì €ì¥\n",
    "        annotated_image = AnnotatedImage(\n",
    "            dataset_id=dataset_id,\n",
    "            filename=Path(image_path).name,\n",
    "            file_path=str(image_path),\n",
    "            file_size=file_size,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            channels=channels,\n",
    "            annotation_count=len(annotations)\n",
    "        )\n",
    "        \n",
    "        db.add(annotated_image)\n",
    "        db.flush()  # ID ìƒì„±ì„ ìœ„í•´\n",
    "        \n",
    "        # ì–´ë…¸í…Œì´ì…˜ ì¶”ê°€\n",
    "        for ann in annotations:\n",
    "            annotation = Annotation(\n",
    "                image_id=annotated_image.id,\n",
    "                class_id=ann['class_id'],\n",
    "                class_name=ann['class_name'],\n",
    "                annotation_type=AnnotationType(ann['type']),\n",
    "                bbox_x=ann.get('bbox_x'),\n",
    "                bbox_y=ann.get('bbox_y'),\n",
    "                bbox_w=ann.get('bbox_w'),\n",
    "                bbox_h=ann.get('bbox_h'),\n",
    "                segmentation_points=ann.get('segmentation_points'),\n",
    "                confidence=ann.get('confidence', 1.0)\n",
    "            )\n",
    "            db.add(annotation)\n",
    "        \n",
    "        db.commit()\n",
    "        db.refresh(annotated_image)\n",
    "        return annotated_image\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 5. ë°ì´í„° ë¶„í•  ì—”ì§„ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplitEngine:\n",
    "    \"\"\"ë°ì´í„° ë¶„í•  ì—”ì§„ - ì˜¬ë°”ë¥¸ Train/Val/Test ë¶„í•  êµ¬í˜„\"\"\"\n",
    "    \n",
    "    def __init__(self, train_ratio: float = 0.7, val_ratio: float = 0.2, test_ratio: float = 0.1, \n",
    "                 random_seed: int = 42):\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        # ë¹„ìœ¨ ê²€ì¦\n",
    "        total_ratio = train_ratio + val_ratio + test_ratio\n",
    "        if abs(total_ratio - 1.0) > 1e-6:\n",
    "            raise ValueError(f\"ë¶„í•  ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤: {total_ratio}\")\n",
    "    \n",
    "    def create_stratified_split(self, db: Session, dataset_id: str, \n",
    "                               stratify_by: str = \"class_distribution\",\n",
    "                               version: str = \"v1.0\") -> dict:\n",
    "        \"\"\"ê³„ì¸µí™” ë¶„í•  ìƒì„± - í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ê³ ë ¤í•œ ë¶„í• \"\"\"\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì´ë¯¸ì§€ ì¡°íšŒ\n",
    "        images = db.query(AnnotatedImage).filter(\n",
    "            AnnotatedImage.dataset_id == dataset_id\n",
    "        ).all()\n",
    "        \n",
    "        if len(images) < 3:\n",
    "            raise ValueError(\"ìµœì†Œ 3ê°œ ì´ìƒì˜ ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        \n",
    "        # ê° ì´ë¯¸ì§€ì˜ í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°\n",
    "        image_class_info = []\n",
    "        for img in images:\n",
    "            annotations = db.query(Annotation).filter(\n",
    "                Annotation.image_id == img.id\n",
    "            ).all()\n",
    "            \n",
    "            # ì´ë¯¸ì§€ì˜ ì£¼ìš” í´ë˜ìŠ¤ ê²°ì • (ê°€ì¥ ë§ì€ ì–´ë…¸í…Œì´ì…˜ì„ ê°€ì§„ í´ë˜ìŠ¤)\n",
    "            class_counts = {}\n",
    "            for ann in annotations:\n",
    "                class_counts[ann.class_name] = class_counts.get(ann.class_name, 0) + 1\n",
    "            \n",
    "            if class_counts:\n",
    "                dominant_class = max(class_counts, key=class_counts.get)\n",
    "                total_annotations = sum(class_counts.values())\n",
    "            else:\n",
    "                dominant_class = \"no_annotation\"\n",
    "                total_annotations = 0\n",
    "            \n",
    "            image_class_info.append({\n",
    "                'image': img,\n",
    "                'dominant_class': dominant_class,\n",
    "                'class_counts': class_counts,\n",
    "                'total_annotations': total_annotations\n",
    "            })\n",
    "        \n",
    "        # ê³„ì¸µí™”ë¥¼ ìœ„í•œ ë ˆì´ë¸” ìƒì„±\n",
    "        if stratify_by == \"class_distribution\":\n",
    "            # ì£¼ìš” í´ë˜ìŠ¤ë¡œ ê³„ì¸µí™”\n",
    "            stratify_labels = [info['dominant_class'] for info in image_class_info]\n",
    "        elif stratify_by == \"annotation_count\":\n",
    "            # ì–´ë…¸í…Œì´ì…˜ ìˆ˜ êµ¬ê°„ìœ¼ë¡œ ê³„ì¸µí™”\n",
    "            stratify_labels = []\n",
    "            for info in image_class_info:\n",
    "                count = info['total_annotations']\n",
    "                if count == 0:\n",
    "                    label = \"none\"\n",
    "                elif count <= 5:\n",
    "                    label = \"low\"\n",
    "                elif count <= 20:\n",
    "                    label = \"medium\"\n",
    "                else:\n",
    "                    label = \"high\"\n",
    "                stratify_labels.append(label)\n",
    "        else:\n",
    "            # ê¸°ë³¸: ëœë¤ ë¶„í• \n",
    "            stratify_labels = None\n",
    "        \n",
    "        # ë°ì´í„° ë¶„í•  ìˆ˜í–‰\n",
    "        image_indices = list(range(len(images)))\n",
    "        \n",
    "        if stratify_labels:\n",
    "            # ê³„ì¸µí™” ë¶„í• \n",
    "            try:\n",
    "                # ë¨¼ì € trainê³¼ temp(val+test)ë¡œ ë¶„í• \n",
    "                train_indices, temp_indices = train_test_split(\n",
    "                    image_indices,\n",
    "                    test_size=(self.val_ratio + self.test_ratio),\n",
    "                    stratify=stratify_labels,\n",
    "                    random_state=self.random_seed\n",
    "                )\n",
    "                \n",
    "                # tempë¥¼ valê³¼ testë¡œ ë¶„í• \n",
    "                temp_labels = [stratify_labels[i] for i in temp_indices]\n",
    "                val_size = self.val_ratio / (self.val_ratio + self.test_ratio)\n",
    "                \n",
    "                val_indices, test_indices = train_test_split(\n",
    "                    temp_indices,\n",
    "                    test_size=(1 - val_size),\n",
    "                    stratify=temp_labels,\n",
    "                    random_state=self.random_seed\n",
    "                )\n",
    "                \n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"ê³„ì¸µí™” ë¶„í•  ì‹¤íŒ¨, ëœë¤ ë¶„í• ë¡œ ì „í™˜: {e}\")\n",
    "                # ê³„ì¸µí™”ê°€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ëœë¤ ë¶„í• \n",
    "                train_indices, temp_indices = train_test_split(\n",
    "                    image_indices,\n",
    "                    test_size=(self.val_ratio + self.test_ratio),\n",
    "                    random_state=self.random_seed\n",
    "                )\n",
    "                \n",
    "                val_size = self.val_ratio / (self.val_ratio + self.test_ratio)\n",
    "                val_indices, test_indices = train_test_split(\n",
    "                    temp_indices,\n",
    "                    test_size=(1 - val_size),\n",
    "                    random_state=self.random_seed\n",
    "                )\n",
    "        else:\n",
    "            # ëœë¤ ë¶„í• \n",
    "            train_indices, temp_indices = train_test_split(\n",
    "                image_indices,\n",
    "                test_size=(self.val_ratio + self.test_ratio),\n",
    "                random_state=self.random_seed\n",
    "            )\n",
    "            \n",
    "            val_size = self.val_ratio / (self.val_ratio + self.test_ratio)\n",
    "            val_indices, test_indices = train_test_split(\n",
    "                temp_indices,\n",
    "                test_size=(1 - val_size),\n",
    "                random_state=self.random_seed\n",
    "            )\n",
    "        \n",
    "        # ê¸°ì¡´ ë¶„í•  ë°ì´í„° ì‚­ì œ\n",
    "        db.query(DataSplit).filter(DataSplit.dataset_id == dataset_id).delete()\n",
    "        \n",
    "        # ìƒˆë¡œìš´ ë¶„í•  ë°ì´í„° ì €ì¥\n",
    "        split_mapping = {\n",
    "            DataSplitType.TRAIN: train_indices,\n",
    "            DataSplitType.VALIDATION: val_indices,\n",
    "            DataSplitType.TEST: test_indices\n",
    "        }\n",
    "        \n",
    "        for split_type, indices in split_mapping.items():\n",
    "            for idx in indices:\n",
    "                image = images[idx]\n",
    "                data_split = DataSplit(\n",
    "                    dataset_id=dataset_id,\n",
    "                    image_id=image.id,\n",
    "                    split_type=split_type,\n",
    "                    split_version=version,\n",
    "                    stratify_key=stratify_by\n",
    "                )\n",
    "                db.add(data_split)\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ í†µê³„ ì—…ë°ì´íŠ¸\n",
    "        dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()\n",
    "        if dataset:\n",
    "            dataset.total_images = len(images)\n",
    "            dataset.train_count = len(train_indices)\n",
    "            dataset.val_count = len(val_indices)\n",
    "            dataset.test_count = len(test_indices)\n",
    "        \n",
    "        db.commit()\n",
    "        \n",
    "        # ë¶„í•  ê²°ê³¼ í†µê³„\n",
    "        split_stats = {\n",
    "            'total_images': len(images),\n",
    "            'train_count': len(train_indices),\n",
    "            'val_count': len(val_indices),\n",
    "            'test_count': len(test_indices),\n",
    "            'train_ratio': len(train_indices) / len(images),\n",
    "            'val_ratio': len(val_indices) / len(images),\n",
    "            'test_ratio': len(test_indices) / len(images),\n",
    "            'stratify_method': stratify_by,\n",
    "            'version': version\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"ë°ì´í„° ë¶„í•  ì™„ë£Œ: {split_stats}\")\n",
    "        return split_stats\n",
    "    \n",
    "    def get_split_data(self, db: Session, dataset_id: str, \n",
    "                      split_type: DataSplitType) -> List[AnnotatedImage]:\n",
    "        \"\"\"íŠ¹ì • ë¶„í• ì˜ ì´ë¯¸ì§€ ë°ì´í„° ì¡°íšŒ\"\"\"\n",
    "        \n",
    "        # í•´ë‹¹ ë¶„í• ì˜ ì´ë¯¸ì§€ ID ì¡°íšŒ\n",
    "        split_data = db.query(DataSplit).filter(\n",
    "            DataSplit.dataset_id == dataset_id,\n",
    "            DataSplit.split_type == split_type\n",
    "        ).all()\n",
    "        \n",
    "        image_ids = [split.image_id for split in split_data]\n",
    "        \n",
    "        # ì´ë¯¸ì§€ ë°ì´í„° ì¡°íšŒ\n",
    "        images = db.query(AnnotatedImage).filter(\n",
    "            AnnotatedImage.id.in_(image_ids)\n",
    "        ).all()\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def export_yolo_format(self, db: Session, dataset_id: str, output_dir: str) -> str:\n",
    "        \"\"\"YOLO í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸°\"\"\"\n",
    "        \n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ì •ë³´ ì¡°íšŒ\n",
    "        dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()\n",
    "        if not dataset:\n",
    "            raise ValueError(f\"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_id}\")\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ë§¤í•‘ ìƒì„±\n",
    "        class_names = dataset.class_names\n",
    "        class_to_id = {name: idx for idx, name in enumerate(class_names)}\n",
    "        \n",
    "        # ê° ë¶„í• ë³„ë¡œ ì²˜ë¦¬\n",
    "        for split_type in [DataSplitType.TRAIN, DataSplitType.VALIDATION, DataSplitType.TEST]:\n",
    "            split_name = split_type.value\n",
    "            \n",
    "            # ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "            images_dir = output_path / split_name / \"images\"\n",
    "            labels_dir = output_path / split_name / \"labels\"\n",
    "            images_dir.mkdir(parents=True, exist_ok=True)\n",
    "            labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # í•´ë‹¹ ë¶„í• ì˜ ì´ë¯¸ì§€ë“¤ ì²˜ë¦¬\n",
    "            images = self.get_split_data(db, dataset_id, split_type)\n",
    "            \n",
    "            for img in images:\n",
    "                # ì´ë¯¸ì§€ íŒŒì¼ ë³µì‚¬\n",
    "                src_image_path = Path(img.file_path)\n",
    "                dst_image_path = images_dir / src_image_path.name\n",
    "                \n",
    "                if src_image_path.exists():\n",
    "                    shutil.copy2(src_image_path, dst_image_path)\n",
    "                \n",
    "                # ë¼ë²¨ íŒŒì¼ ìƒì„±\n",
    "                label_filename = src_image_path.stem + \".txt\"\n",
    "                label_path = labels_dir / label_filename\n",
    "                \n",
    "                # ì–´ë…¸í…Œì´ì…˜ ì¡°íšŒ ë° YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "                annotations = db.query(Annotation).filter(\n",
    "                    Annotation.image_id == img.id\n",
    "                ).all()\n",
    "                \n",
    "                with open(label_path, 'w') as f:\n",
    "                    for ann in annotations:\n",
    "                        if ann.annotation_type == AnnotationType.BOUNDING_BOX:\n",
    "                            class_id = class_to_id.get(ann.class_name, 0)\n",
    "                            # YOLO format: class_id center_x center_y width height\n",
    "                            line = f\"{class_id} {ann.bbox_x} {ann.bbox_y} {ann.bbox_w} {ann.bbox_h}\\n\"\n",
    "                            f.write(line)\n",
    "        \n",
    "        # dataset.yaml íŒŒì¼ ìƒì„±\n",
    "        yaml_content = {\n",
    "            'path': str(output_path.absolute()),\n",
    "            'train': 'train/images',\n",
    "            'val': 'validation/images',\n",
    "            'test': 'test/images',\n",
    "            'nc': len(class_names),\n",
    "            'names': class_names\n",
    "        }\n",
    "        \n",
    "        yaml_path = output_path / \"dataset.yaml\"\n",
    "        with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "            import yaml\n",
    "            yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True)\n",
    "        \n",
    "        logger.info(f\"YOLO í˜•ì‹ ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}\")\n",
    "        return str(yaml_path)\n",
    "\n",
    "\n",
    "# ì „ì—­ ë°ì´í„° ë¶„í•  ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤\n",
    "data_split_engine = DataSplitEngine(\n",
    "    train_ratio=settings.TRAIN_SPLIT,\n",
    "    val_ratio=settings.VAL_SPLIT,\n",
    "    test_ratio=settings.TEST_SPLIT,\n",
    "    random_seed=settings.RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë¶„í•  ì—”ì§„ êµ¬í˜„ ì™„ë£Œ\")\n",
    "print(f\"ğŸ“Š ë¶„í•  ë¹„ìœ¨: Train {settings.TRAIN_SPLIT}, Val {settings.VAL_SPLIT}, Test {settings.TEST_SPLIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  6. ML í›ˆë ¨ ì—”ì§„ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLTrainingEngine:\n",
    "    \"\"\"ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì—”ì§„\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        logger.info(f\"ML í›ˆë ¨ ì—”ì§„ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}\")\n",
    "    \n",
    "    def create_training_job(self, db: Session, dataset_id: str, job_name: str,\n",
    "                           model_type: ModelType, base_model: str = \"yolov8n\",\n",
    "                           hyperparameters: dict = None, description: str = None) -> TrainingJob:\n",
    "        \"\"\"ìƒˆë¡œìš´ í›ˆë ¨ ì‘ì—… ìƒì„±\"\"\"\n",
    "        \n",
    "        # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "        default_hyperparams = {\n",
    "            'epochs': settings.EPOCHS,\n",
    "            'batch_size': settings.BATCH_SIZE,\n",
    "            'learning_rate': settings.LEARNING_RATE,\n",
    "            'image_size': settings.IMAGE_SIZE,\n",
    "            'patience': 50,\n",
    "            'save_period': 10,\n",
    "            'optimizer': 'AdamW',\n",
    "            'cos_lr': True,\n",
    "            'mosaic': 1.0,\n",
    "            'mixup': 0.1,\n",
    "            'copy_paste': 0.1\n",
    "        }\n",
    "        \n",
    "        if hyperparameters:\n",
    "            default_hyperparams.update(hyperparameters)\n",
    "        \n",
    "        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        job_dir = Path(settings.MODELS_PATH) / f\"job_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{job_name}\"\n",
    "        job_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # í›ˆë ¨ ì‘ì—… ìƒì„±\n",
    "        training_job = TrainingJob(\n",
    "            dataset_id=dataset_id,\n",
    "            job_name=job_name,\n",
    "            description=description,\n",
    "            model_type=model_type,\n",
    "            base_model=base_model,\n",
    "            hyperparameters=default_hyperparams,\n",
    "            total_epochs=default_hyperparams['epochs'],\n",
    "            output_dir=str(job_dir),\n",
    "            log_file_path=str(job_dir / \"training.log\")\n",
    "        )\n",
    "        \n",
    "        db.add(training_job)\n",
    "        db.commit()\n",
    "        db.refresh(training_job)\n",
    "        \n",
    "        return training_job\n",
    "    \n",
    "    def start_training(self, db: Session, training_job_id: str) -> bool:\n",
    "        \"\"\"ëª¨ë¸ í›ˆë ¨ ì‹œì‘\"\"\"\n",
    "        \n",
    "        training_job = db.query(TrainingJob).filter(\n",
    "            TrainingJob.id == training_job_id\n",
    "        ).first()\n",
    "        \n",
    "        if not training_job:\n",
    "            raise ValueError(f\"í›ˆë ¨ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {training_job_id}\")\n",
    "        \n",
    "        try:\n",
    "            # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "            training_job.status = \"running\"\n",
    "            training_job.started_at = datetime.utcnow()\n",
    "            db.commit()\n",
    "            \n",
    "            logger.info(f\"ëª¨ë¸ í›ˆë ¨ ì‹œì‘: {training_job.job_name}\")\n",
    "            \n",
    "            # ë°ì´í„°ì…‹ì„ YOLO í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°\n",
    "            yolo_dataset_path = data_split_engine.export_yolo_format(\n",
    "                db, training_job.dataset_id, \n",
    "                Path(training_job.output_dir) / \"dataset\"\n",
    "            )\n",
    "            \n",
    "            # YOLO ëª¨ë¸ ì´ˆê¸°í™”\n",
    "            if training_job.model_type == ModelType.YOLO_DETECTION:\n",
    "                model = YOLO(f\"{training_job.base_model}.pt\")\n",
    "            else:\n",
    "                raise ValueError(f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {training_job.model_type}\")\n",
    "            \n",
    "            # í›ˆë ¨ ì‹¤í–‰\n",
    "            train_start_time = time.time()\n",
    "            \n",
    "            results = model.train(\n",
    "                data=yolo_dataset_path,\n",
    "                epochs=training_job.hyperparameters['epochs'],\n",
    "                batch=training_job.hyperparameters['batch_size'],\n",
    "                imgsz=training_job.hyperparameters['image_size'],\n",
    "                lr0=training_job.hyperparameters['learning_rate'],\n",
    "                patience=training_job.hyperparameters['patience'],\n",
    "                save_period=training_job.hyperparameters['save_period'],\n",
    "                project=training_job.output_dir,\n",
    "                name=\"train\",\n",
    "                exist_ok=True,\n",
    "                device=self.device,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            train_duration = time.time() - train_start_time\n",
    "            \n",
    "            # í›ˆë ¨ ê²°ê³¼ ì²˜ë¦¬\n",
    "            best_weights_path = Path(training_job.output_dir) / \"train\" / \"weights\" / \"best.pt\"\n",
    "            last_weights_path = Path(training_job.output_dir) / \"train\" / \"weights\" / \"last.pt\"\n",
    "            \n",
    "            # ìµœì¢… ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ\n",
    "            metrics = results.results_dict if hasattr(results, 'results_dict') else {}\n",
    "            \n",
    "            # í›ˆë ¨ ì‘ì—… ì™„ë£Œ ì²˜ë¦¬\n",
    "            training_job.status = \"completed\"\n",
    "            training_job.completed_at = datetime.utcnow()\n",
    "            training_job.training_duration = train_duration\n",
    "            training_job.current_epoch = training_job.total_epochs\n",
    "            training_job.progress_percent = 100.0\n",
    "            \n",
    "            if best_weights_path.exists():\n",
    "                training_job.best_weights_path = str(best_weights_path)\n",
    "            if last_weights_path.exists():\n",
    "                training_job.last_weights_path = str(last_weights_path)\n",
    "            \n",
    "            # ì„±ëŠ¥ ì§€í‘œ ì €ì¥\n",
    "            training_job.best_map50 = metrics.get('metrics/mAP50(B)', 0.0)\n",
    "            training_job.best_map50_95 = metrics.get('metrics/mAP50-95(B)', 0.0)\n",
    "            \n",
    "            db.commit()\n",
    "            \n",
    "            logger.info(f\"ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {training_job.job_name} ({train_duration:.2f}ì´ˆ)\")\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ í‰ê°€ ì‹¤í–‰\n",
    "            self.evaluate_model(db, training_job_id, \"test\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "            # ì—ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "            training_job.status = \"failed\"\n",
    "            training_job.error_message = str(e)\n",
    "            training_job.completed_at = datetime.utcnow()\n",
    "            db.commit()\n",
    "            \n",
    "            return False\n",
    "    \n",
    "    def evaluate_model(self, db: Session, training_job_id: str, \n",
    "                      evaluation_type: str = \"test\") -> dict:\n",
    "        \"\"\"ëª¨ë¸ í‰ê°€ ìˆ˜í–‰\"\"\"\n",
    "        \n",
    "        training_job = db.query(TrainingJob).filter(\n",
    "            TrainingJob.id == training_job_id\n",
    "        ).first()\n",
    "        \n",
    "        if not training_job or not training_job.best_weights_path:\n",
    "            raise ValueError(\"í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "        try:\n",
    "            # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ\n",
    "            model = YOLO(training_job.best_weights_path)\n",
    "            \n",
    "            # ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "            dataset_yaml = Path(training_job.output_dir) / \"dataset\" / \"dataset.yaml\"\n",
    "            \n",
    "            # í‰ê°€ ì‹¤í–‰\n",
    "            if evaluation_type == \"test\":\n",
    "                results = model.val(data=str(dataset_yaml), split=\"test\")\n",
    "            else:\n",
    "                results = model.val(data=str(dataset_yaml), split=\"val\")\n",
    "            \n",
    "            # ê²°ê³¼ ì¶”ì¶œ\n",
    "            metrics = results.results_dict if hasattr(results, 'results_dict') else {}\n",
    "            \n",
    "            # í‰ê°€ ê²°ê³¼ ì €ì¥\n",
    "            evaluation = ModelEvaluation(\n",
    "                training_job_id=training_job_id,\n",
    "                evaluation_type=evaluation_type,\n",
    "                map50=metrics.get('metrics/mAP50(B)', 0.0),\n",
    "                map50_95=metrics.get('metrics/mAP50-95(B)', 0.0),\n",
    "                precision=metrics.get('metrics/precision(B)', 0.0),\n",
    "                recall=metrics.get('metrics/recall(B)', 0.0),\n",
    "                # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ì€ ë³„ë„ ì²˜ë¦¬ í•„ìš”\n",
    "                class_metrics={}  # TODO: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì¶”ì¶œ\n",
    "            )\n",
    "            \n",
    "            db.add(evaluation)\n",
    "            db.commit()\n",
    "            db.refresh(evaluation)\n",
    "            \n",
    "            logger.info(f\"ëª¨ë¸ í‰ê°€ ì™„ë£Œ: {evaluation_type} - mAP50: {evaluation.map50:.4f}\")\n",
    "            \n",
    "            return {\n",
    "                'evaluation_id': evaluation.id,\n",
    "                'map50': evaluation.map50,\n",
    "                'map50_95': evaluation.map50_95,\n",
    "                'precision': evaluation.precision,\n",
    "                'recall': evaluation.recall\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_training_status(self, db: Session, training_job_id: str) -> dict:\n",
    "        \"\"\"í›ˆë ¨ ìƒíƒœ ì¡°íšŒ\"\"\"\n",
    "        \n",
    "        training_job = db.query(TrainingJob).filter(\n",
    "            TrainingJob.id == training_job_id\n",
    "        ).first()\n",
    "        \n",
    "        if not training_job:\n",
    "            raise ValueError(f\"í›ˆë ¨ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {training_job_id}\")\n",
    "        \n",
    "        # í‰ê°€ ê²°ê³¼ ì¡°íšŒ\n",
    "        evaluations = db.query(ModelEvaluation).filter(\n",
    "            ModelEvaluation.training_job_id == training_job_id\n",
    "        ).all()\n",
    "        \n",
    "        evaluation_results = {}\n",
    "        for eval_result in evaluations:\n",
    "            evaluation_results[eval_result.evaluation_type] = {\n",
    "                'map50': eval_result.map50,\n",
    "                'map50_95': eval_result.map50_95,\n",
    "                'precision': eval_result.precision,\n",
    "                'recall': eval_result.recall\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'job_id': training_job.id,\n",
    "            'job_name': training_job.job_name,\n",
    "            'status': training_job.status,\n",
    "            'progress_percent': training_job.progress_percent,\n",
    "            'current_epoch': training_job.current_epoch,\n",
    "            'total_epochs': training_job.total_epochs,\n",
    "            'best_map50': training_job.best_map50,\n",
    "            'best_map50_95': training_job.best_map50_95,\n",
    "            'training_duration': training_job.training_duration,\n",
    "            'started_at': training_job.started_at,\n",
    "            'completed_at': training_job.completed_at,\n",
    "            'evaluations': evaluation_results,\n",
    "            'error_message': training_job.error_message\n",
    "        }\n",
    "\n",
    "\n",
    "# ì „ì—­ ML í›ˆë ¨ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤\n",
    "ml_training_engine = MLTrainingEngine()\n",
    "\n",
    "print(\"âœ… ML í›ˆë ¨ ì—”ì§„ êµ¬í˜„ ì™„ë£Œ\")\n",
    "print(f\"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {ml_training_engine.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ 7. API ìŠ¤í‚¤ë§ˆ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API ì‘ë‹µ ìŠ¤í‚¤ë§ˆ\n",
    "class APIResponse(BaseModel):\n",
    "    \"\"\"ê¸°ë³¸ API ì‘ë‹µ\"\"\"\n",
    "    success: bool = True\n",
    "    message: str = \"\"\n",
    "    data: Optional[Any] = None\n",
    "    error: Optional[str] = None\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê´€ë ¨ ìŠ¤í‚¤ë§ˆ\n",
    "class DatasetCreateRequest(BaseModel):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ìƒì„± ìš”ì²­\"\"\"\n",
    "    name: str\n",
    "    description: Optional[str] = None\n",
    "    task_type: str  # crop, facility, landuse\n",
    "    annotation_type: str  # bbox, segmentation, classification\n",
    "    class_names: List[str]\n",
    "\n",
    "class DatasetResponse(BaseModel):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ì‘ë‹µ\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    description: Optional[str]\n",
    "    task_type: str\n",
    "    annotation_type: str\n",
    "    class_names: List[str]\n",
    "    total_images: int\n",
    "    total_annotations: int\n",
    "    train_count: int\n",
    "    val_count: int\n",
    "    test_count: int\n",
    "    created_at: datetime\n",
    "    updated_at: datetime\n",
    "\n",
    "# ë°ì´í„° ë¶„í•  ê´€ë ¨ ìŠ¤í‚¤ë§ˆ\n",
    "class DataSplitRequest(BaseModel):\n",
    "    \"\"\"ë°ì´í„° ë¶„í•  ìš”ì²­\"\"\"\n",
    "    dataset_id: str\n",
    "    stratify_by: str = \"class_distribution\"  # class_distribution, annotation_count, random\n",
    "    train_ratio: float = Field(0.7, ge=0.1, le=0.8)\n",
    "    val_ratio: float = Field(0.2, ge=0.1, le=0.4)\n",
    "    test_ratio: float = Field(0.1, ge=0.1, le=0.4)\n",
    "    version: str = \"v1.0\"\n",
    "\n",
    "class DataSplitResponse(BaseModel):\n",
    "    \"\"\"ë°ì´í„° ë¶„í•  ì‘ë‹µ\"\"\"\n",
    "    total_images: int\n",
    "    train_count: int\n",
    "    val_count: int\n",
    "    test_count: int\n",
    "    train_ratio: float\n",
    "    val_ratio: float\n",
    "    test_ratio: float\n",
    "    stratify_method: str\n",
    "    version: str\n",
    "\n",
    "# ëª¨ë¸ í›ˆë ¨ ê´€ë ¨ ìŠ¤í‚¤ë§ˆ\n",
    "class TrainingJobCreateRequest(BaseModel):\n",
    "    \"\"\"í›ˆë ¨ ì‘ì—… ìƒì„± ìš”ì²­\"\"\"\n",
    "    dataset_id: str\n",
    "    job_name: str\n",
    "    description: Optional[str] = None\n",
    "    model_type: str = \"yolo_detection\"\n",
    "    base_model: str = \"yolov8n\"\n",
    "    epochs: int = Field(100, ge=1, le=1000)\n",
    "    batch_size: int = Field(16, ge=1, le=64)\n",
    "    learning_rate: float = Field(0.01, ge=0.0001, le=0.1)\n",
    "    image_size: int = Field(640, ge=320, le=1280)\n",
    "\n",
    "class TrainingJobResponse(BaseModel):\n",
    "    \"\"\"í›ˆë ¨ ì‘ì—… ì‘ë‹µ\"\"\"\n",
    "    id: str\n",
    "    job_name: str\n",
    "    description: Optional[str]\n",
    "    status: str\n",
    "    model_type: str\n",
    "    base_model: str\n",
    "    progress_percent: float\n",
    "    current_epoch: int\n",
    "    total_epochs: int\n",
    "    best_map50: Optional[float]\n",
    "    best_map50_95: Optional[float]\n",
    "    training_duration: Optional[float]\n",
    "    started_at: Optional[datetime]\n",
    "    completed_at: Optional[datetime]\n",
    "    error_message: Optional[str]\n",
    "    created_at: datetime\n",
    "\n",
    "class ModelEvaluationResponse(BaseModel):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ ì‘ë‹µ\"\"\"\n",
    "    id: str\n",
    "    evaluation_type: str\n",
    "    epoch: Optional[int]\n",
    "    map50: float\n",
    "    map50_95: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: Optional[float]\n",
    "    created_at: datetime\n",
    "\n",
    "print(\"âœ… API ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 8. FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ë° ML í›ˆë ¨ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±\n",
    "app = FastAPI(\n",
    "    title=settings.PROJECT_NAME,\n",
    "    description=\"AI ë†ì—… ì˜ìƒ ë¶„ì„ í”Œë«í¼ - ML í›ˆë ¨ ì‹œìŠ¤í…œ\",\n",
    "    version=settings.VERSION,\n",
    "    docs_url=\"/api/docs\",\n",
    "    redoc_url=\"/api/redoc\"\n",
    ")\n",
    "\n",
    "# CORS ë¯¸ë“¤ì›¨ì–´\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# í—¬ìŠ¤ ì²´í¬\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸\"\"\"\n",
    "    return APIResponse(\n",
    "        success=True,\n",
    "        message=\"ML í›ˆë ¨ ì„œë¹„ìŠ¤ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤\",\n",
    "        data={\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"version\": settings.VERSION,\n",
    "            \"environment\": settings.ENVIRONMENT,\n",
    "            \"device\": ml_training_engine.device\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê´€ë¦¬ API\n",
    "@app.post(\"/api/v1/datasets\", response_model=APIResponse)\n",
    "async def create_dataset(\n",
    "    request: DatasetCreateRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    try:\n",
    "        # ì–´ë…¸í…Œì´ì…˜ íƒ€ì… ê²€ì¦\n",
    "        try:\n",
    "            annotation_type = AnnotationType(request.annotation_type)\n",
    "        except ValueError:\n",
    "            raise HTTPException(\n",
    "                status_code=400, \n",
    "                detail=f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì–´ë…¸í…Œì´ì…˜ íƒ€ì…: {request.annotation_type}\"\n",
    "            )\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ìƒì„±\n",
    "        dataset = DatabaseService.create_dataset(\n",
    "            db=db,\n",
    "            name=request.name,\n",
    "            task_type=request.task_type,\n",
    "            annotation_type=annotation_type,\n",
    "            class_names=request.class_names,\n",
    "            description=request.description\n",
    "        )\n",
    "        \n",
    "        dataset_data = DatasetResponse(\n",
    "            id=dataset.id,\n",
    "            name=dataset.name,\n",
    "            description=dataset.description,\n",
    "            task_type=dataset.task_type,\n",
    "            annotation_type=dataset.annotation_type.value,\n",
    "            class_names=dataset.class_names,\n",
    "            total_images=dataset.total_images,\n",
    "            total_annotations=dataset.total_annotations,\n",
    "            train_count=dataset.train_count,\n",
    "            val_count=dataset.val_count,\n",
    "            test_count=dataset.test_count,\n",
    "            created_at=dataset.created_at,\n",
    "            updated_at=dataset.updated_at\n",
    "        )\n",
    "        \n",
    "        return APIResponse(\n",
    "            success=True,\n",
    "            message=\"ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ\",\n",
    "            data=dataset_data.dict()\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "\n",
    "@app.get(\"/api/v1/datasets\", response_model=APIResponse)\n",
    "async def list_datasets(db: Session = Depends(get_db)):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ\"\"\"\n",
    "    try:\n",
    "        datasets = db.query(Dataset).all()\n",
    "        \n",
    "        dataset_list = []\n",
    "        for dataset in datasets:\n",
    "            dataset_data = DatasetResponse(\n",
    "                id=dataset.id,\n",
    "                name=dataset.name,\n",
    "                description=dataset.description,\n",
    "                task_type=dataset.task_type,\n",
    "                annotation_type=dataset.annotation_type.value,\n",
    "                class_names=dataset.class_names,\n",
    "                total_images=dataset.total_images,\n",
    "                total_annotations=dataset.total_annotations,\n",
    "                train_count=dataset.train_count,\n",
    "                val_count=dataset.val_count,\n",
    "                test_count=dataset.test_count,\n",
    "                created_at=dataset.created_at,\n",
    "                updated_at=dataset.updated_at\n",
    "            )\n",
    "            dataset_list.append(dataset_data.dict())\n",
    "        \n",
    "        return APIResponse(\n",
    "            success=True,\n",
    "            message=f\"{len(dataset_list)}ê°œì˜ ë°ì´í„°ì…‹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤\",\n",
    "            data={\"datasets\": dataset_list, \"total\": len(dataset_list)}\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "\n",
    "# ë°ì´í„° ë¶„í•  API\n",
    "@app.post(\"/api/v1/datasets/{dataset_id}/split\", response_model=APIResponse)\n",
    "async def create_data_split(\n",
    "    dataset_id: str,\n",
    "    request: DataSplitRequest,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"ë°ì´í„° ë¶„í•  ìƒì„±\"\"\"\n",
    "    try:\n",
    "        # ë¶„í•  ë¹„ìœ¨ ê²€ì¦\n",
    "        total_ratio = request.train_ratio + request.val_ratio + request.test_ratio\n",
    "        if abs(total_ratio - 1.0) > 1e-6:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"ë¶„í•  ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤: {total_ratio}\"\n",
    "            )\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸\n",
    "        dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()\n",
    "        if not dataset:\n",
    "            raise HTTPException(status_code=404, detail=\"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "        # ë¶„í•  ì—”ì§„ ì„¤ì • ì—…ë°ì´íŠ¸\n",
    "        data_split_engine.train_ratio = request.train_ratio\n",
    "        data_split_engine.val_ratio = request.val_ratio\n",
    "        data_split_engine.test_ratio = request.test_ratio\n",
    "        \n",
    "        # ë¶„í•  ì‹¤í–‰\n",
    "        split_stats = data_split_engine.create_stratified_split(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            stratify_by=request.stratify_by,\n",
    "            version=request.version\n",
    "        )\n",
    "        \n",
    "        split_response = DataSplitResponse(**split_stats)\n",
    "        \n",
    "        return APIResponse(\n",
    "            success=True,\n",
    "            message=\"ë°ì´í„° ë¶„í•  ì™„ë£Œ\",\n",
    "            data=split_response.dict()\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ë°ì´í„° ë¶„í•  ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ë°ì´í„° ë¶„í•  ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "\n",
    "# ëª¨ë¸ í›ˆë ¨ API\n",
    "@app.post(\"/api/v1/training/jobs\", response_model=APIResponse)\n",
    "async def create_training_job(\n",
    "    request: TrainingJobCreateRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"í›ˆë ¨ ì‘ì—… ìƒì„±\"\"\"\n",
    "    try:\n",
    "        # ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸\n",
    "        dataset = db.query(Dataset).filter(Dataset.id == request.dataset_id).first()\n",
    "        if not dataset:\n",
    "            raise HTTPException(status_code=404, detail=\"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "        # ë°ì´í„° ë¶„í•  í™•ì¸\n",
    "        train_count = db.query(DataSplit).filter(\n",
    "            DataSplit.dataset_id == request.dataset_id,\n",
    "            DataSplit.split_type == DataSplitType.TRAIN\n",
    "        ).count()\n",
    "        \n",
    "        if train_count == 0:\n",
    "            raise HTTPException(\n",
    "                status_code=400, \n",
    "                detail=\"ë°ì´í„° ë¶„í• ì´ ë¨¼ì € í•„ìš”í•©ë‹ˆë‹¤. /datasets/{dataset_id}/split ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "        \n",
    "        # ëª¨ë¸ íƒ€ì… ê²€ì¦\n",
    "        try:\n",
    "            model_type = ModelType(request.model_type)\n",
    "        except ValueError:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {request.model_type}\"\n",
    "            )\n",
    "        \n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° êµ¬ì„±\n",
    "        hyperparameters = {\n",
    "            'epochs': request.epochs,\n",
    "            'batch_size': request.batch_size,\n",
    "            'learning_rate': request.learning_rate,\n",
    "            'image_size': request.image_size\n",
    "        }\n",
    "        \n",
    "        # í›ˆë ¨ ì‘ì—… ìƒì„±\n",
    "        training_job = ml_training_engine.create_training_job(\n",
    "            db=db,\n",
    "            dataset_id=request.dataset_id,\n",
    "            job_name=request.job_name,\n",
    "            model_type=model_type,\n",
    "            base_model=request.base_model,\n",
    "            hyperparameters=hyperparameters,\n",
    "            description=request.description\n",
    "        )\n",
    "        \n",
    "        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í›ˆë ¨ ì‹œì‘\n",
    "        background_tasks.add_task(\n",
    "            ml_training_engine.start_training,\n",
    "            db=SessionLocal(),\n",
    "            training_job_id=training_job.id\n",
    "        )\n",
    "        \n",
    "        training_job_data = TrainingJobResponse(\n",
    "            id=training_job.id,\n",
    "            job_name=training_job.job_name,\n",
    "            description=training_job.description,\n",
    "            status=training_job.status,\n",
    "            model_type=training_job.model_type.value,\n",
    "            base_model=training_job.base_model,\n",
    "            progress_percent=training_job.progress_percent,\n",
    "            current_epoch=training_job.current_epoch,\n",
    "            total_epochs=training_job.total_epochs,\n",
    "            best_map50=training_job.best_map50,\n",
    "            best_map50_95=training_job.best_map50_95,\n",
    "            training_duration=training_job.training_duration,\n",
    "            started_at=training_job.started_at,\n",
    "            completed_at=training_job.completed_at,\n",
    "            error_message=training_job.error_message,\n",
    "            created_at=training_job.created_at\n",
    "        )\n",
    "        \n",
    "        return APIResponse(\n",
    "            success=True,\n",
    "            message=\"í›ˆë ¨ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤\",\n",
    "            data=training_job_data.dict()\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"í›ˆë ¨ ì‘ì—… ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"í›ˆë ¨ ì‘ì—… ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "\n",
    "@app.get(\"/api/v1/training/jobs\", response_model=APIResponse)\n",
    "async def list_training_jobs(db: Session = Depends(get_db)):\n",
    "    \"\"\"í›ˆë ¨ ì‘ì—… ëª©ë¡ ì¡°íšŒ\"\"\"\n",
    "    try:\n",
    "        training_jobs = db.query(TrainingJob).order_by(TrainingJob.created_at.desc()).all()\n",
    "        \n",
    "        job_list = []\n",
    "        for job in training_jobs:\n",
    "            job_data = TrainingJobResponse(\n",
    "                id=job.id,\n",
    "                job_name=job.job_name,\n",
    "                description=job.description,\n",
    "                status=job.status,\n",
    "                model_type=job.model_type.value,\n",
    "                base_model=job.base_model,\n",
    "                progress_percent=job.progress_percent,\n",
    "                current_epoch=job.current_epoch,\n",
    "                total_epochs=job.total_epochs,\n",
    "                best_map50=job.best_map50,\n",
    "                best_map50_95=job.best_map50_95,\n",
    "                training_duration=job.training_duration,\n",
    "                started_at=job.started_at,\n",
    "                completed_at=job.completed_at,\n",
    "                error_message=job.error_message,\n",
    "                created_at=job.created_at\n",
    "            )\n",
    "            job_list.append(job_data.dict())\n",
    "        \n",
    "        return APIResponse(\n",
    "            success=True,\n",
    "            message=f\"{len(job_list)}ê°œì˜ í›ˆë ¨ ì‘ì—…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤\",\n",
    "            data={\"training_jobs\": job_list, \"total\": len(job_list)}\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"í›ˆë ¨ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"í›ˆë ¨ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "\n",
    "@app.get(\"/api/v1/training/jobs/{job_id}\", response_model=APIResponse)\n",
    "async def get_training_job(job_id: str, db: Session = Depends(get_db)):\n",
    "    \"\"\"í›ˆë ¨ ì‘ì—… ìƒì„¸ ì¡°íšŒ\"\"\"\n",
    "    try:\n",
    "        job_status = ml_training_engine.get_training_status(db, job_id)\n",
    "        \n",
    "        return APIResponse(\n",
    "            success=True,\n",
    "            message=\"í›ˆë ¨ ì‘ì—… ì¡°íšŒ ì™„ë£Œ\",\n",
    "            data=job_status\n",
    "        )\n",
    "        \n",
    "    except ValueError as e:\n",
    "        raise HTTPException(status_code=404, detail=str(e))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"í›ˆë ¨ ì‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"í›ˆë ¨ ì‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "print(\"âœ… FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ë° ML í›ˆë ¨ API êµ¬í˜„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 9. ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_ml_dataset(db: Session) -> str:\n",
    "    \"\"\"ML í›ˆë ¨ìš© ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    \n",
    "    logger.info(\"ML í›ˆë ¨ìš© ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...\")\n",
    "    \n",
    "    # 1. ë°ì´í„°ì…‹ ìƒì„±\n",
    "    dataset = DatabaseService.create_dataset(\n",
    "        db=db,\n",
    "        name=\"sample_crop_dataset\",\n",
    "        task_type=\"crop\",\n",
    "        annotation_type=AnnotationType.BOUNDING_BOX,\n",
    "        class_names=settings.CROP_CLASSES,\n",
    "        description=\"ìƒ˜í”Œ ì‘ë¬¼ ë¶„ë¥˜ ë°ì´í„°ì…‹\"\n",
    "    )\n",
    "    \n",
    "    # 2. ìƒ˜í”Œ ì´ë¯¸ì§€ ë° ì–´ë…¸í…Œì´ì…˜ ìƒì„±\n",
    "    dataset_path = Path(dataset.data_path)\n",
    "    images_dir = dataset_path / \"images\"\n",
    "    images_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ë¡œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    total_images = 50  # í…ŒìŠ¤íŠ¸ìš© ì†ŒëŸ‰ ë°ì´í„°\n",
    "    images_per_class = total_images // len(settings.CROP_CLASSES)\n",
    "    \n",
    "    for class_idx, class_name in enumerate(settings.CROP_CLASSES):\n",
    "        for img_idx in range(images_per_class):\n",
    "            # ê°€ìƒ ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì‹¤ì œ ì´ë¯¸ì§€ ì‚¬ìš©)\n",
    "            img_array = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
    "            \n",
    "            # í´ë˜ìŠ¤ì— ë”°ë¥¸ ìƒ‰ìƒ íŒ¨í„´ ì¶”ê°€\n",
    "            if class_name == 'IRG':\n",
    "                img_array[:, :, 1] = np.maximum(img_array[:, :, 1], 150)  # ë…¹ìƒ‰ ê°•ì¡°\n",
    "            elif class_name == 'BARLEY':\n",
    "                img_array[:, :, 0] = np.maximum(img_array[:, :, 0], 180)  # í™©ê¸ˆìƒ‰\n",
    "                img_array[:, :, 1] = np.maximum(img_array[:, :, 1], 150)\n",
    "            # ... ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ë„ ìœ ì‚¬í•˜ê²Œ\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ì €ì¥\n",
    "            img_filename = f\"{class_name.lower()}_{img_idx:03d}.jpg\"\n",
    "            img_path = images_dir / img_filename\n",
    "            \n",
    "            pil_image = Image.fromarray(img_array)\n",
    "            pil_image.save(img_path, 'JPEG')\n",
    "            \n",
    "            # ìƒ˜í”Œ ì–´ë…¸í…Œì´ì…˜ ìƒì„± (ì¤‘ì•™ì— í•˜ë‚˜ì˜ ê°ì²´)\n",
    "            annotations = [{\n",
    "                'class_id': class_idx,\n",
    "                'class_name': class_name,\n",
    "                'type': 'bbox',\n",
    "                'bbox_x': 0.5,  # center x (normalized)\n",
    "                'bbox_y': 0.5,  # center y (normalized)\n",
    "                'bbox_w': 0.3 + np.random.uniform(-0.1, 0.1),  # width\n",
    "                'bbox_h': 0.3 + np.random.uniform(-0.1, 0.1),  # height\n",
    "                'confidence': 1.0\n",
    "            }]\n",
    "            \n",
    "            # ê°€ë” ì¶”ê°€ ê°ì²´ ìƒì„±\n",
    "            if np.random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ì¶”ê°€ ê°ì²´\n",
    "                annotations.append({\n",
    "                    'class_id': class_idx,\n",
    "                    'class_name': class_name,\n",
    "                    'type': 'bbox',\n",
    "                    'bbox_x': np.random.uniform(0.2, 0.8),\n",
    "                    'bbox_y': np.random.uniform(0.2, 0.8),\n",
    "                    'bbox_w': np.random.uniform(0.1, 0.2),\n",
    "                    'bbox_h': np.random.uniform(0.1, 0.2),\n",
    "                    'confidence': 1.0\n",
    "                })\n",
    "            \n",
    "            # ë°ì´í„°ë² ì´ìŠ¤ì— ì´ë¯¸ì§€ ë° ì–´ë…¸í…Œì´ì…˜ ì¶”ê°€\n",
    "            DatabaseService.add_annotated_image(\n",
    "                db=db,\n",
    "                dataset_id=dataset.id,\n",
    "                image_path=str(img_path),\n",
    "                annotations=annotations\n",
    "            )\n",
    "    \n",
    "    # 3. ë°ì´í„°ì…‹ í†µê³„ ì—…ë°ì´íŠ¸\n",
    "    total_images = db.query(AnnotatedImage).filter(\n",
    "        AnnotatedImage.dataset_id == dataset.id\n",
    "    ).count()\n",
    "    \n",
    "    total_annotations = db.query(Annotation).join(AnnotatedImage).filter(\n",
    "        AnnotatedImage.dataset_id == dataset.id\n",
    "    ).count()\n",
    "    \n",
    "    dataset.total_images = total_images\n",
    "    dataset.total_annotations = total_annotations\n",
    "    db.commit()\n",
    "    \n",
    "    logger.info(f\"ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {total_images}ê°œ ì´ë¯¸ì§€, {total_annotations}ê°œ ì–´ë…¸í…Œì´ì…˜\")\n",
    "    return dataset.id\n",
    "\n",
    "\n",
    "def run_sample_ml_workflow():\n",
    "    \"\"\"ì™„ì „í•œ ML ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤í–‰\"\"\"\n",
    "    \n",
    "    db = DatabaseService.get_session()\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸš€ ML í›ˆë ¨ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "        print(\"\\n1ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "        dataset_id = create_sample_ml_dataset(db)\n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {dataset_id}\")\n",
    "        \n",
    "        # 2. ë°ì´í„° ë¶„í• \n",
    "        print(\"\\n2ï¸âƒ£ ë°ì´í„° ë¶„í•  ìˆ˜í–‰ ì¤‘...\")\n",
    "        split_stats = data_split_engine.create_stratified_split(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            stratify_by=\"class_distribution\",\n",
    "            version=\"v1.0\"\n",
    "        )\n",
    "        print(f\"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:\")\n",
    "        print(f\"   - ì´ ì´ë¯¸ì§€: {split_stats['total_images']}ê°œ\")\n",
    "        print(f\"   - í›ˆë ¨: {split_stats['train_count']}ê°œ ({split_stats['train_ratio']:.1%})\")\n",
    "        print(f\"   - ê²€ì¦: {split_stats['val_count']}ê°œ ({split_stats['val_ratio']:.1%})\")\n",
    "        print(f\"   - í…ŒìŠ¤íŠ¸: {split_stats['test_count']}ê°œ ({split_stats['test_ratio']:.1%})\")\n",
    "        \n",
    "        # 3. í›ˆë ¨ ì‘ì—… ìƒì„±\n",
    "        print(\"\\n3ï¸âƒ£ í›ˆë ¨ ì‘ì—… ìƒì„± ì¤‘...\")\n",
    "        hyperparams = {\n",
    "            'epochs': 5,  # í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ê°’\n",
    "            'batch_size': 4,\n",
    "            'learning_rate': 0.01,\n",
    "            'image_size': 640\n",
    "        }\n",
    "        \n",
    "        training_job = ml_training_engine.create_training_job(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            job_name=\"sample_crop_training\",\n",
    "            model_type=ModelType.YOLO_DETECTION,\n",
    "            base_model=\"yolov8n\",\n",
    "            hyperparameters=hyperparams,\n",
    "            description=\"ìƒ˜í”Œ ì‘ë¬¼ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨\"\n",
    "        )\n",
    "        print(f\"âœ… í›ˆë ¨ ì‘ì—… ìƒì„± ì™„ë£Œ: {training_job.id}\")\n",
    "        \n",
    "        # 4. YOLO í˜•ì‹ ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸\n",
    "        print(\"\\n4ï¸âƒ£ YOLO í˜•ì‹ ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸...\")\n",
    "        export_path = Path(training_job.output_dir) / \"dataset\"\n",
    "        yaml_path = data_split_engine.export_yolo_format(\n",
    "            db=db,\n",
    "            dataset_id=dataset_id,\n",
    "            output_dir=str(export_path)\n",
    "        )\n",
    "        print(f\"âœ… YOLO ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {yaml_path}\")\n",
    "        \n",
    "        # 5. í›ˆë ¨ ì •ë³´ ì¶œë ¥\n",
    "        print(\"\\n5ï¸âƒ£ í›ˆë ¨ ì •ë³´:\")\n",
    "        print(f\"   - ì‘ì—… ID: {training_job.id}\")\n",
    "        print(f\"   - ì‘ì—… ì´ë¦„: {training_job.job_name}\")\n",
    "        print(f\"   - ëª¨ë¸ íƒ€ì…: {training_job.model_type.value}\")\n",
    "        print(f\"   - ë² ì´ìŠ¤ ëª¨ë¸: {training_job.base_model}\")\n",
    "        print(f\"   - ì¶œë ¥ ë””ë ‰í† ë¦¬: {training_job.output_dir}\")\n",
    "        print(f\"   - í•˜ì´í¼íŒŒë¼ë¯¸í„°: {training_job.hyperparameters}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ‰ ML í›ˆë ¨ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "        print(\"\\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "        print(\"   1. API ì„œë²„ ì‹œì‘: start_server()\")\n",
    "        print(\"   2. í›ˆë ¨ ì‹œì‘: POST /api/v1/training/jobs\")\n",
    "        print(\"   3. ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§: GET /api/v1/training/jobs/{job_id}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_id': dataset_id,\n",
    "            'training_job_id': training_job.id,\n",
    "            'split_stats': split_stats\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ML ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "# ìƒ˜í”Œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\n",
    "print(\"ğŸ“Š ML í›ˆë ¨ ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "sample_results = run_sample_ml_workflow()\n",
    "\n",
    "if sample_results:\n",
    "    print(f\"\\nâœ… ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ ê²°ê³¼ ìš”ì•½:\")\n",
    "    print(f\"   - ë°ì´í„°ì…‹ ID: {sample_results['dataset_id']}\")\n",
    "    print(f\"   - í›ˆë ¨ ì‘ì—… ID: {sample_results['training_job_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ–¥ï¸ 10. ì„œë²„ ì‹œì‘ ë° ì‚¬ìš©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„œë²„ ì‹œì‘ í•¨ìˆ˜\n",
    "def start_server():\n",
    "    \"\"\"ML í›ˆë ¨ ì„œë²„ ì‹œì‘\"\"\"\n",
    "    import uvicorn\n",
    "    \n",
    "    print(\"\\nğŸš€ Nong-View ML í›ˆë ¨ ì„œë²„ ì‹œì‘ (v3.0)\")\n",
    "    print(f\"ğŸŒ ì£¼ì†Œ: http://{settings.HOST}:{settings.PORT}\")\n",
    "    print(f\"ğŸ“š API ë¬¸ì„œ: http://{settings.HOST}:{settings.PORT}/api/docs\")\n",
    "    print(f\"ğŸ”§ í™˜ê²½: {settings.ENVIRONMENT}\")\n",
    "    print(f\"ğŸ“Š ë²„ì „: {settings.VERSION}\")\n",
    "    print(f\"ğŸ’» ë””ë°”ì´ìŠ¤: {ml_training_engine.device}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ì„œë²„ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "    print(\"start_server()\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ì‹¤ì œ ì„œë²„ ì‹œì‘ (ì£¼ì„ í•´ì œ ì‹œ ìë™ ì‹œì‘)\n",
    "    # uvicorn.run(\n",
    "    #     \"__main__:app\",\n",
    "    #     host=settings.HOST,\n",
    "    #     port=settings.PORT,\n",
    "    #     reload=settings.DEBUG\n",
    "    # )\n",
    "\n",
    "\n",
    "def show_usage_examples():\n",
    "    \"\"\"ì‚¬ìš© ì˜ˆì‹œ ì¶œë ¥\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ“– API ì‚¬ìš© ì˜ˆì‹œ:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    examples = [\n",
    "        {\n",
    "            \"title\": \"1. ë°ì´í„°ì…‹ ìƒì„±\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/api/v1/datasets\",\n",
    "            \"body\": {\n",
    "                \"name\": \"my_crop_dataset\",\n",
    "                \"description\": \"ì‘ë¬¼ ë¶„ë¥˜ìš© ë°ì´í„°ì…‹\",\n",
    "                \"task_type\": \"crop\",\n",
    "                \"annotation_type\": \"bbox\",\n",
    "                \"class_names\": [\"IRG\", \"BARLEY\", \"WHEAT\", \"CORN_SILAGE\", \"HAY\", \"FALLOW\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"2. ë°ì´í„° ë¶„í• \",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/api/v1/datasets/{dataset_id}/split\",\n",
    "            \"body\": {\n",
    "                \"dataset_id\": \"your_dataset_id\",\n",
    "                \"stratify_by\": \"class_distribution\",\n",
    "                \"train_ratio\": 0.7,\n",
    "                \"val_ratio\": 0.2,\n",
    "                \"test_ratio\": 0.1\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"3. í›ˆë ¨ ì‘ì—… ì‹œì‘\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/api/v1/training/jobs\",\n",
    "            \"body\": {\n",
    "                \"dataset_id\": \"your_dataset_id\",\n",
    "                \"job_name\": \"crop_detection_v1\",\n",
    "                \"description\": \"ì²« ë²ˆì§¸ ì‘ë¬¼ ê²€ì¶œ ëª¨ë¸\",\n",
    "                \"model_type\": \"yolo_detection\",\n",
    "                \"base_model\": \"yolov8n\",\n",
    "                \"epochs\": 100,\n",
    "                \"batch_size\": 16,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"image_size\": 640\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"4. í›ˆë ¨ ì§„í–‰ ìƒí™© í™•ì¸\",\n",
    "            \"method\": \"GET\",\n",
    "            \"url\": \"/api/v1/training/jobs/{job_id}\",\n",
    "            \"body\": None\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for example in examples:\n",
    "        print(f\"\\n{example['title']}\")\n",
    "        print(f\"   {example['method']} {example['url']}\")\n",
    "        if example['body']:\n",
    "            print(f\"   Body: {json.dumps(example['body'], indent=8, ensure_ascii=False)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "\n",
    "# ì„œë²„ ì •ë³´ ë° ì‚¬ìš©ë²• ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ Nong-View v3.0 ML í›ˆë ¨ ë…¸íŠ¸ë¶ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“ˆ ì£¼ìš” ê°œì„ ì‚¬í•­:\")\n",
    "print(f\"   - âœ… ì˜¬ë°”ë¥¸ Train/Val/Test ë°ì´í„° ë¶„í•  êµ¬í˜„\")\n",
    "print(f\"   - âœ… ê³„ì¸µí™” ë¶„í• ë¡œ í´ë˜ìŠ¤ ê· í˜• ìœ ì§€\")\n",
    "print(f\"   - âœ… SQLAlchemy ê¸°ë°˜ ML í›ˆë ¨ ë°ì´í„°ë² ì´ìŠ¤\")\n",
    "print(f\"   - âœ… YOLO ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸\")\n",
    "print(f\"   - âœ… ëª¨ë¸ í‰ê°€ ë° ì„±ëŠ¥ ì§€í‘œ ì¶”ì \")\n",
    "print(f\"   - âœ… RESTful APIë¡œ ì „ì²´ ML ì›Œí¬í”Œë¡œìš° ì œì–´\")\n",
    "\n",
    "show_usage_examples()\n",
    "\n",
    "print(\"\\nğŸ”§ ì‹œì‘ ë°©ë²•:\")\n",
    "print(\"   1. ëª¨ë“  ì…€ ì‹¤í–‰ ì™„ë£Œ\")\n",
    "print(\"   2. start_server() í•¨ìˆ˜ í˜¸ì¶œ\")\n",
    "print(\"   3. http://localhost:8000/api/docs ì ‘ì†\")\n",
    "print(\"   4. APIë¥¼ í†µí•´ ë°ì´í„°ì…‹ ìƒì„± â†’ ë¶„í•  â†’ í›ˆë ¨ â†’ í‰ê°€\")\n",
    "\n",
    "print(\"\\nğŸ“š ë¬¸ì„œ:\")\n",
    "print(\"   - API ë¬¸ì„œ: /api/docs\")\n",
    "print(\"   - Redoc: /api/redoc\")\n",
    "print(\"   - í—¬ìŠ¤ ì²´í¬: /health\")\n",
    "\n",
    "print(\"\\nğŸ¯ ì™„ë£Œëœ ê¸°ëŠ¥:\")\n",
    "print(\"   - ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ë°ì´í„°ì…‹ ê´€ë¦¬\")\n",
    "print(\"   - ê³¼í•™ì  ë°ì´í„° ë¶„í•  (stratified split)\")\n",
    "print(\"   - YOLO ëª¨ë¸ í›ˆë ¨ ìë™í™”\")\n",
    "print(\"   - ì‹¤ì‹œê°„ í›ˆë ¨ ì§„í–‰ ìƒí™© ì¶”ì \")\n",
    "print(\"   - ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì €ì¥\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}