#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Model3 Greenhouse ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ê³¼í•™ì  ë°ì´í„° ë¶„í•  ë° ì¬êµ¬ì„±

ê¸°ëŠ¥:
- ì„ì˜ë¡œ ë¶„í• ëœ train/val/test ë°ì´í„°ë¥¼ í†µí•©
- ê³„ì¸µí™” ë¶„í•  (Stratified Split)ì„ í†µí•œ ê³¼í•™ì  ì¬ë¶„í• 
- í´ë˜ìŠ¤ ê· í˜• ìœ ì§€
- YOLO í˜•ì‹ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
"""

import os
import shutil
import random
import yaml
from pathlib import Path
from typing import Dict, List, Tuple
from collections import Counter, defaultdict
from dataclasses import dataclass
from tqdm import tqdm
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ImageInfo:
    """ì´ë¯¸ì§€ ì •ë³´ í´ë˜ìŠ¤"""
    image_path: Path
    label_path: Path
    filename: str
    classes: List[int]
    dominant_class: int  # ê°€ì¥ ë§ì€ ê°ì²´ì˜ í´ë˜ìŠ¤


class Model3GreenhousePreprocessor:
    """Model3 Greenhouse ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, source_dir: str, output_dir: str,
                 train_ratio: float = 0.8, val_ratio: float = 0.1, test_ratio: float = 0.1,
                 random_seed: int = 42):
        """
        Args:
            source_dir: í˜„ì¬ model3_greenhouse ë””ë ‰í† ë¦¬
            output_dir: ì¬êµ¬ì„±ëœ ë°ì´í„°ì…‹ ì¶œë ¥ ë””ë ‰í† ë¦¬
            train_ratio: í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.8)
            val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)
            test_ratio: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)
            random_seed: ëœë¤ ì‹œë“œ (ì¬í˜„ì„±)
        """
        self.source_dir = Path(source_dir)
        self.output_dir = Path(output_dir)
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        self.random_seed = random_seed

        # ëœë¤ ì‹œë“œ ì„¤ì •
        random.seed(random_seed)

        # í†µê³„
        self.stats = {
            'total_images': 0,
            'total_objects': 0,
            'class_distribution': Counter(),
            'split_distribution': {}
        }

        logger.info(f"ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - ì†ŒìŠ¤: {self.source_dir}")
        logger.info(f"  - ì¶œë ¥: {self.output_dir}")
        logger.info(f"  - ë¶„í•  ë¹„ìœ¨: Train={train_ratio}, Val={val_ratio}, Test={test_ratio}")

    def run(self):
        """ì „ì²´ ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Model3 Greenhouse ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        logger.info("=" * 60)

        # 1. ë°ì´í„° ìˆ˜ì§‘
        logger.info("\n[1/5] ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        all_images = self._collect_all_images()
        logger.info(f"  âœ“ ì´ {len(all_images)}ê°œ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì™„ë£Œ")

        # 2. í†µê³„ ë¶„ì„
        logger.info("\n[2/5] ë°ì´í„° ë¶„ì„ ì¤‘...")
        self._analyze_data(all_images)
        self._print_statistics()

        # 3. ê³„ì¸µí™” ë¶„í• 
        logger.info("\n[3/5] ê³„ì¸µí™” ë¶„í•  ì¤‘...")
        train_images, val_images, test_images = self._stratified_split(all_images)
        logger.info(f"  âœ“ Train: {len(train_images)}ê°œ")
        logger.info(f"  âœ“ Val: {len(val_images)}ê°œ")
        logger.info(f"  âœ“ Test: {len(test_images)}ê°œ")

        # 4. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        logger.info("\n[4/5] ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘...")
        self._setup_output_structure()

        # 5. ë°ì´í„° ë³µì‚¬
        logger.info("\n[5/5] ë°ì´í„° ë³µì‚¬ ì¤‘...")
        self._copy_split_data('train', train_images)
        self._copy_split_data('val', val_images)
        self._copy_split_data('test', test_images)

        # 6. YAML íŒŒì¼ ìƒì„±
        logger.info("\nYAML ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        self._create_yaml_file(len(train_images), len(val_images), len(test_images))

        logger.info("\n" + "=" * 60)
        logger.info("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info("=" * 60)
        self._print_final_summary()

    def _collect_all_images(self) -> List[ImageInfo]:
        """ëª¨ë“  ì´ë¯¸ì§€ ìˆ˜ì§‘ (train/val/test í†µí•©)"""
        all_images = []

        # train, val, test í´ë”ì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ ìˆ˜ì§‘
        for split in ['train', 'val', 'test']:
            images_dir = self.source_dir / 'images' / split
            labels_dir = self.source_dir / 'labels' / split

            if not images_dir.exists():
                logger.warning(f"  âš  ë””ë ‰í† ë¦¬ ì—†ìŒ: {images_dir}")
                continue

            # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
            image_files = list(images_dir.glob('*.png')) + list(images_dir.glob('*.jpg'))

            for img_path in image_files:
                # ëŒ€ì‘í•˜ëŠ” ë¼ë²¨ íŒŒì¼ ì°¾ê¸°
                label_path = labels_dir / f"{img_path.stem}.txt"

                if label_path.exists():
                    # ë¼ë²¨ íŒŒì‹±
                    classes = self._parse_label_file(label_path)

                    if classes:
                        # ê°€ì¥ ë§ì€ ê°ì²´ë¥¼ ê°€ì§„ í´ë˜ìŠ¤ ì°¾ê¸°
                        class_counts = Counter(classes)
                        dominant_class = class_counts.most_common(1)[0][0]

                        image_info = ImageInfo(
                            image_path=img_path,
                            label_path=label_path,
                            filename=img_path.name,
                            classes=classes,
                            dominant_class=dominant_class
                        )
                        all_images.append(image_info)
                else:
                    logger.warning(f"  âš  ë¼ë²¨ ì—†ìŒ: {img_path.name}")

        return all_images

    def _parse_label_file(self, label_path: Path) -> List[int]:
        """YOLO í˜•ì‹ ë¼ë²¨ íŒŒì¼ íŒŒì‹±"""
        classes = []
        try:
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:  # class_id x_center y_center width height
                        class_id = int(parts[0])
                        classes.append(class_id)
        except Exception as e:
            logger.error(f"  âœ— ë¼ë²¨ íŒŒì‹± ì‹¤íŒ¨ {label_path}: {e}")

        return classes

    def _analyze_data(self, images: List[ImageInfo]):
        """ë°ì´í„° í†µê³„ ë¶„ì„"""
        self.stats['total_images'] = len(images)

        for img_info in images:
            # ê°ì²´ ê°œìˆ˜
            self.stats['total_objects'] += len(img_info.classes)

            # í´ë˜ìŠ¤ ë¶„í¬
            for class_id in img_info.classes:
                self.stats['class_distribution'][class_id] += 1

    def _print_statistics(self):
        """í†µê³„ ì¶œë ¥"""
        logger.info(f"  ğŸ“Š í†µê³„:")
        logger.info(f"    - ì´ ì´ë¯¸ì§€: {self.stats['total_images']}ê°œ")
        logger.info(f"    - ì´ ê°ì²´: {self.stats['total_objects']}ê°œ")
        logger.info(f"    - í´ë˜ìŠ¤ ë¶„í¬:")

        class_names = {0: 'Greenhouse_single', 1: 'Greenhouse_multi'}
        for class_id, count in sorted(self.stats['class_distribution'].items()):
            class_name = class_names.get(class_id, f'Class_{class_id}')
            percentage = (count / self.stats['total_objects']) * 100
            logger.info(f"      â€¢ {class_name}: {count}ê°œ ({percentage:.1f}%)")

    def _stratified_split(self, images: List[ImageInfo]) -> Tuple[List[ImageInfo], List[ImageInfo], List[ImageInfo]]:
        """ê³„ì¸µí™” ë¶„í•  (Stratified Split)"""
        # í´ë˜ìŠ¤ë³„ë¡œ ì´ë¯¸ì§€ ê·¸ë£¹í™”
        class_groups = defaultdict(list)
        for img_info in images:
            class_groups[img_info.dominant_class].append(img_info)

        train_images = []
        val_images = []
        test_images = []

        # ê° í´ë˜ìŠ¤ë³„ë¡œ ë¶„í• 
        for class_id, class_images in class_groups.items():
            # ì…”í”Œ
            random.shuffle(class_images)

            n_total = len(class_images)
            n_train = int(n_total * self.train_ratio)
            n_val = int(n_total * self.val_ratio)

            # ë¶„í• 
            train_images.extend(class_images[:n_train])
            val_images.extend(class_images[n_train:n_train + n_val])
            test_images.extend(class_images[n_train + n_val:])

            class_name = {0: 'Greenhouse_single', 1: 'Greenhouse_multi'}.get(class_id, f'Class_{class_id}')
            logger.info(f"  â€¢ {class_name}: Train={n_train}, Val={n_val}, Test={n_total - n_train - n_val}")

        # ìµœì¢… ì…”í”Œ
        random.shuffle(train_images)
        random.shuffle(val_images)
        random.shuffle(test_images)

        return train_images, val_images, test_images

    def _setup_output_structure(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        # ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ (ìˆëŠ” ê²½ìš°)
        if self.output_dir.exists():
            logger.info(f"  âš  ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì‚­ì œ: {self.output_dir}")
            shutil.rmtree(self.output_dir)

        # YOLO í˜•ì‹ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        for split in ['train', 'val', 'test']:
            (self.output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)
            (self.output_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)

        logger.info(f"  âœ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {self.output_dir}")

    def _copy_split_data(self, split_name: str, images: List[ImageInfo]):
        """ë¶„í• ëœ ë°ì´í„° ë³µì‚¬"""
        images_dir = self.output_dir / 'images' / split_name
        labels_dir = self.output_dir / 'labels' / split_name

        # í´ë˜ìŠ¤ ë¶„í¬ ì¶”ì 
        class_dist = Counter()

        for img_info in tqdm(images, desc=f"  {split_name} ë³µì‚¬"):
            # ì´ë¯¸ì§€ ë³µì‚¬
            shutil.copy2(img_info.image_path, images_dir / img_info.filename)

            # ë¼ë²¨ ë³µì‚¬
            shutil.copy2(img_info.label_path, labels_dir / f"{Path(img_info.filename).stem}.txt")

            # í´ë˜ìŠ¤ ë¶„í¬ ì—…ë°ì´íŠ¸
            for class_id in img_info.classes:
                class_dist[class_id] += 1

        # ë¶„í• ë³„ í†µê³„ ì €ì¥
        self.stats['split_distribution'][split_name] = {
            'images': len(images),
            'objects': sum(class_dist.values()),
            'class_distribution': dict(class_dist)
        }

        logger.info(f"  âœ“ {split_name}: {len(images)}ê°œ ì´ë¯¸ì§€, {sum(class_dist.values())}ê°œ ê°ì²´")

    def _create_yaml_file(self, n_train: int, n_val: int, n_test: int):
        """ë°ì´í„°ì…‹ YAML íŒŒì¼ ìƒì„±"""
        yaml_content = {
            'path': str(self.output_dir.absolute()),
            'train': 'images/train',
            'val': 'images/val',
            'test': 'images/test',
            'nc': 2,
            'names': ['Greenhouse_single', 'Greenhouse_multi'],

            # ë©”íƒ€ë°ì´í„°
            'dataset_info': {
                'total': self.stats['total_images'],
                'train': n_train,
                'val': n_val,
                'test': n_test,
                'augmented': 0
            },

            'preprocessing': {
                'method': 'stratified_split',
                'train_ratio': self.train_ratio,
                'val_ratio': self.val_ratio,
                'test_ratio': self.test_ratio,
                'random_seed': self.random_seed
            }
        }

        yaml_path = self.output_dir / 'data.yaml'
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True, sort_keys=False)

        logger.info(f"  âœ“ YAML íŒŒì¼ ìƒì„±: {yaml_path}")

    def _print_final_summary(self):
        """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
        logger.info("\nğŸ“Š ìµœì¢… ë°ì´í„° ë¶„í¬:")

        class_names = {0: 'Greenhouse_single', 1: 'Greenhouse_multi'}

        for split_name in ['train', 'val', 'test']:
            if split_name in self.stats['split_distribution']:
                split_info = self.stats['split_distribution'][split_name]
                logger.info(f"\n  ğŸ“ {split_name.upper()}:")
                logger.info(f"    - ì´ë¯¸ì§€: {split_info['images']}ê°œ")
                logger.info(f"    - ê°ì²´: {split_info['objects']}ê°œ")
                logger.info(f"    - í´ë˜ìŠ¤ ë¶„í¬:")

                for class_id, count in sorted(split_info['class_distribution'].items()):
                    class_name = class_names.get(class_id, f'Class_{class_id}')
                    percentage = (count / split_info['objects']) * 100
                    logger.info(f"      â€¢ {class_name}: {count}ê°œ ({percentage:.1f}%)")

        logger.info(f"\nâœ¨ ì¬êµ¬ì„±ëœ ë°ì´í„°ì…‹ ìœ„ì¹˜: {self.output_dir}")
        logger.info(f"âœ¨ ì„¤ì • íŒŒì¼: {self.output_dir / 'data.yaml'}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ê²½ë¡œ ì„¤ì •
    source_dir = r"C:\Users\LX\Nong-View\model3_greenhouse"
    output_dir = r"C:\Users\LX\Nong-View\model3_greenhouse_processed"

    # ì „ì²˜ë¦¬ê¸° ìƒì„± ë° ì‹¤í–‰
    preprocessor = Model3GreenhousePreprocessor(
        source_dir=source_dir,
        output_dir=output_dir,
        train_ratio=0.8,
        val_ratio=0.1,
        test_ratio=0.1,
        random_seed=42
    )

    preprocessor.run()


if __name__ == "__main__":
    main()
