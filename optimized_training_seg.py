"""
Optimized Segmentation Model Training System for Model3 Greenhouse
Based on: optimized_training.py by Claude Opus
Modified for: YOLOv11-seg Segmentation Task
Date: 2025-11-04
Version: 2.0.0 (Segmentation)

ë°ì´í„°: model3_greenhouse_seg_processed
í´ë˜ìŠ¤: Greenhouse_single (ë‹¨ë™), Greenhouse_multi (ì—°ë™)
ëª¨ë¸: YOLOv11-seg
íƒœìŠ¤í¬: Segmentation

Advanced training optimization system implementing:
- Intelligent hyperparameter optimization
- Dynamic learning rate scheduling
- Advanced loss function design (with mask loss)
- Multi-stage training strategies
- Hardware-aware optimization
"""

import os
import sys
import torch
import torch.nn as nn
from torch.cuda.amp import GradScaler, autocast
from torch.optim import AdamW, SGD
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import numpy as np
from datetime import datetime
import json
import yaml
import logging
from dataclasses import dataclass, asdict, field
from enum import Enum
import warnings
from ultralytics import YOLO
from collections import defaultdict
import psutil
try:
    import GPUtil
except ImportError:
    GPUtil = None
from tqdm import tqdm
import gc
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ================== Enums ==================

class TrainingStrategy(Enum):
    """í•™ìŠµ ì „ëµ"""
    STANDARD = "standard"
    PROGRESSIVE = "progressive"      # Progressive resizing
    CURRICULUM = "curriculum"        # Easy to hard samples
    ENSEMBLE = "ensemble"            # Multiple model training
    DISTILLATION = "distillation"    # Knowledge distillation


class SegModelSize(Enum):
    """YOLOv11-seg ëª¨ë¸ í¬ê¸°"""
    NANO = "yolo11n-seg.pt"
    SMALL = "yolo11s-seg.pt"
    MEDIUM = "yolo11m-seg.pt"
    LARGE = "yolo11l-seg.pt"
    XLARGE = "yolo11x-seg.pt"


# ================== Configuration ==================

@dataclass
class SegmentationTrainingConfig:
    """Segmentation í•™ìŠµ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    model_size: SegModelSize = SegModelSize.NANO
    data_yaml: str = r"C:\Users\LX\Nong-View\model3_greenhouse_seg_processed\data.yaml"
    task: str = "segment"  # Segmentation íƒœìŠ¤í¬

    epochs: int = 100
    batch_size: int = 16
    imgsz: int = 640

    # ìµœì í™” ì„¤ì •
    optimizer: str = "AdamW"
    base_lr: float = 0.001
    final_lr: float = 0.00001
    warmup_epochs: int = 3
    weight_decay: float = 0.0005
    momentum: float = 0.937

    # ê³ ê¸‰ ì„¤ì •
    strategy: TrainingStrategy = TrainingStrategy.PROGRESSIVE
    use_amp: bool = True
    gradient_clip_val: float = 10.0
    ema_decay: float = 0.9999
    label_smoothing: float = 0.0

    # ë°ì´í„° ì¦ê°• ì„¤ì •
    mosaic: float = 1.0
    mixup: float = 0.15
    copy_paste: float = 0.3
    degrees: float = 10.0
    translate: float = 0.2
    scale: float = 0.9
    shear: float = 2.0
    perspective: float = 0.0
    hsv_h: float = 0.015
    hsv_s: float = 0.7
    hsv_v: float = 0.4
    flipud: float = 0.5
    fliplr: float = 0.5

    # ì†ì‹¤ ê°€ì¤‘ì¹˜ (Segmentation)
    box_loss_weight: float = 7.5   # Box loss
    cls_loss_weight: float = 0.5   # Classification loss
    dfl_loss_weight: float = 1.5   # DFL loss
    # Segmentation ì „ìš©
    mask_loss_weight: float = 2.5  # Mask loss (ì¤‘ìš”!)

    # Early stopping
    patience: int = 30
    min_delta: float = 0.001

    # ì²´í¬í¬ì¸íŠ¸
    save_period: int = 5
    keep_checkpoints: int = 3

    # í•˜ë“œì›¨ì–´ ì„¤ì •
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    workers: int = 8
    pin_memory: bool = True

    # í”„ë¡œì íŠ¸ ì„¤ì •
    project: str = "runs/segment"
    name: str = "model3_greenhouse"
    exist_ok: bool = False

    # ì¶”ê°€ ì„¤ì •
    verbose: bool = True
    plots: bool = True
    save: bool = True
    val: bool = True
    cache: bool = False
    resume: bool = False
    overlap_mask: bool = True  # Segmentation ì „ìš©
    mask_ratio: int = 4        # Segmentation mask downsampling ratio


# ================== Learning Rate Scheduler ==================

class AdvancedLearningRateScheduler:
    """ê³ ê¸‰ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬"""

    def __init__(self, optimizer: torch.optim.Optimizer, config: SegmentationTrainingConfig):
        self.optimizer = optimizer
        self.config = config
        self.current_epoch = 0
        self.current_lr = config.base_lr

        # ì „ëµì— ë”°ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        if config.strategy == TrainingStrategy.PROGRESSIVE:
            self.scheduler = CosineAnnealingWarmRestarts(
                optimizer,
                T_0=config.epochs // 4,
                T_mult=2,
                eta_min=config.final_lr
            )
        else:
            self.scheduler = OneCycleLR(
                optimizer,
                max_lr=config.base_lr,
                epochs=config.epochs,
                steps_per_epoch=1,
                pct_start=config.warmup_epochs / config.epochs,
                final_div_factor=config.base_lr / config.final_lr
            )

    def step(self, metrics: Optional[Dict] = None):
        """ë©”íŠ¸ë¦­ ê¸°ë°˜ í•™ìŠµë¥  ì—…ë°ì´íŠ¸"""
        if metrics and 'loss' in metrics:
            if self._is_plateau(metrics['loss']):
                self._reduce_lr_on_plateau()

        self.scheduler.step()
        self.current_epoch += 1
        self.current_lr = self.optimizer.param_groups[0]['lr']

        return self.current_lr

    def _is_plateau(self, loss: float, window: int = 5) -> bool:
        """ì†ì‹¤ ì •ì²´ ê°ì§€"""
        if not hasattr(self, 'loss_history'):
            self.loss_history = []

        self.loss_history.append(loss)

        if len(self.loss_history) < window:
            return False

        recent_losses = self.loss_history[-window:]
        mean_loss = np.mean(recent_losses)
        std_loss = np.std(recent_losses)

        return std_loss < self.config.min_delta * mean_loss

    def _reduce_lr_on_plateau(self, factor: float = 0.5):
        """ì •ì²´ ì‹œ í•™ìŠµë¥  ê°ì†Œ"""
        for param_group in self.optimizer.param_groups:
            param_group['lr'] *= factor
        logger.info(f"í•™ìŠµë¥  ê°ì†Œ: {self.current_lr * factor:.6f}")


# ================== Loss Function Optimizer ==================

class SegmentationLossFunctionOptimizer:
    """Segmentation ì†ì‹¤ í•¨ìˆ˜ ìµœì í™”"""

    def __init__(self, config: SegmentationTrainingConfig):
        self.config = config
        self.focal_loss_alpha = 0.25
        self.focal_loss_gamma = 1.5

        # Segmentation ì „ìš© ì ì‘í˜• ê°€ì¤‘ì¹˜
        self.adaptive_weights = self._calculate_adaptive_weights()

    def _calculate_adaptive_weights(self) -> Dict[str, float]:
        """ë°ì´í„°ì…‹ íŠ¹ì„± ê¸°ë°˜ ì ì‘í˜• ì†ì‹¤ ê°€ì¤‘ì¹˜"""
        weights = {
            'box': self.config.box_loss_weight,
            'cls': self.config.cls_loss_weight,
            'dfl': self.config.dfl_loss_weight,
            'mask': self.config.mask_loss_weight  # Segmentation ì¶”ê°€
        }

        # ë¹„ë‹í•˜ìš°ìŠ¤ ë°ì´í„° íŠ¹ì„± ë°˜ì˜
        # ì •ë°€í•œ ê²½ê³„ ê²€ì¶œì´ ì¤‘ìš”í•˜ë¯€ë¡œ mask loss ê°€ì¤‘ì¹˜ ì¦ê°€
        weights['mask'] *= 1.2

        logger.info(f"ì ì‘í˜• ì†ì‹¤ ê°€ì¤‘ì¹˜: {weights}")
        return weights

    def compute_focal_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """Focal Loss (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)"""
        ce_loss = nn.functional.cross_entropy(pred, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.focal_loss_alpha * (1 - pt) ** self.focal_loss_gamma * ce_loss

        if self.config.label_smoothing > 0:
            n_classes = pred.shape[1]
            smooth_target = target * (1 - self.config.label_smoothing) + \
                           self.config.label_smoothing / n_classes
            focal_loss = focal_loss * smooth_target

        return focal_loss.mean()


# ================== Main Trainer ==================

class OptimizedSegmentationTrainer:
    """Segmentation ìµœì í™” í•™ìŠµ í´ë˜ìŠ¤"""

    def __init__(self, config: SegmentationTrainingConfig):
        self.config = config
        self.device = torch.device(config.device)
        self.model = None
        self.optimizer = None
        self.lr_scheduler = None
        self.loss_optimizer = SegmentationLossFunctionOptimizer(config)
        self.scaler = GradScaler() if config.use_amp else None

        # ì„±ëŠ¥ ì¶”ì  (Segmentation ë©”íŠ¸ë¦­)
        self.best_metrics = {
            'mAP50': 0,
            'mAP50-95': 0,
            'mask_mAP50': 0,      # Mask mAP50 ì¶”ê°€
            'mask_mAP50-95': 0,   # Mask mAP50-95 ì¶”ê°€
            'loss': float('inf')
        }
        self.training_history = defaultdict(list)

        # í•˜ë“œì›¨ì–´ ìµœì í™”
        self._setup_hardware_optimization()

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = Path(config.project) / config.name
        self.output_dir.mkdir(parents=True, exist_ok=config.exist_ok)

        logger.info("=" * 60)
        logger.info("YOLOv11-seg Segmentation í•™ìŠµ ì‹œìŠ¤í…œ")
        logger.info("=" * 60)
        logger.info(f"ëª¨ë¸: {config.model_size.value}")
        logger.info(f"ë°ì´í„°: {config.data_yaml}")
        logger.info(f"íƒœìŠ¤í¬: {config.task}")
        logger.info(f"ë””ë°”ì´ìŠ¤: {config.device}")
        logger.info(f"ì¶œë ¥: {self.output_dir}")

    def _setup_hardware_optimization(self):
        """í•˜ë“œì›¨ì–´ ìµœì í™” ì„¤ì •"""
        if self.device.type == 'cuda':
            # GPU ìµœì í™”
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False

            # TF32 í™œì„±í™” (Ampere ì´ìƒ)
            if torch.cuda.get_device_capability()[0] >= 8:
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True

            # ë©”ëª¨ë¦¬ ìµœì í™”
            torch.cuda.empty_cache()
            gc.collect()

            logger.info(f"GPU: {torch.cuda.get_device_name()}")
            logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
        else:
            logger.warning("CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ - í•™ìŠµì´ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")

    def train(self) -> Dict[str, Any]:
        """í•™ìŠµ ì‹¤í–‰"""
        logger.info("\ní•™ìŠµ ì‹œì‘...")

        # ëª¨ë¸ ë¡œë“œ
        self.model = YOLO(self.config.model_size.value)
        logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.config.model_size.value}")

        # í•™ìŠµ ì¸ì ì„¤ì •
        train_args = {
            # ê¸°ë³¸ ì„¤ì •
            'data': self.config.data_yaml,
            'task': self.config.task,
            'epochs': self.config.epochs,
            'batch': self.config.batch_size,
            'imgsz': self.config.imgsz,

            # ìµœì í™” ì„¤ì •
            'optimizer': self.config.optimizer,
            'lr0': self.config.base_lr,
            'lrf': self.config.final_lr / self.config.base_lr,
            'momentum': self.config.momentum,
            'weight_decay': self.config.weight_decay,
            'warmup_epochs': self.config.warmup_epochs,
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,

            # ì†ì‹¤ ê°€ì¤‘ì¹˜
            'box': self.loss_optimizer.adaptive_weights['box'],
            'cls': self.loss_optimizer.adaptive_weights['cls'],
            'dfl': self.loss_optimizer.adaptive_weights['dfl'],

            # ë°ì´í„° ì¦ê°•
            'mosaic': self.config.mosaic,
            'mixup': self.config.mixup,
            'copy_paste': self.config.copy_paste,
            'degrees': self.config.degrees,
            'translate': self.config.translate,
            'scale': self.config.scale,
            'shear': self.config.shear,
            'perspective': self.config.perspective,
            'hsv_h': self.config.hsv_h,
            'hsv_s': self.config.hsv_s,
            'hsv_v': self.config.hsv_v,
            'flipud': self.config.flipud,
            'fliplr': self.config.fliplr,

            # Segmentation ì „ìš© ì„¤ì •
            'overlap_mask': self.config.overlap_mask,
            'mask_ratio': self.config.mask_ratio,

            # Early stopping
            'patience': self.config.patience,

            # í•˜ë“œì›¨ì–´ ì„¤ì •
            'device': self.config.device,
            'workers': self.config.workers,

            # ì²´í¬í¬ì¸íŠ¸
            'save': self.config.save,
            'save_period': self.config.save_period,

            # ê¸°íƒ€
            'project': self.config.project,
            'name': self.config.name,
            'exist_ok': self.config.exist_ok,
            'verbose': self.config.verbose,
            'plots': self.config.plots,
            'val': self.config.val,
            'cache': self.config.cache,
            'resume': self.config.resume,
            'amp': self.config.use_amp
        }

        # Progressive resizing ì „ëµ
        if self.config.strategy == TrainingStrategy.PROGRESSIVE:
            logger.info("Progressive resizing ì „ëµ ì‚¬ìš©")
        elif self.config.strategy == TrainingStrategy.CURRICULUM:
            logger.info("Curriculum learning ì „ëµ ì‚¬ìš©")

        # í•™ìŠµ ì‹œì‘
        try:
            start_time = time.time()

            results = self.model.train(**train_args)

            training_time = time.time() - start_time

            # ìµœì¢… ëª¨ë¸ ì €ì¥
            final_model_path = self.output_dir / 'best.pt'
            self.model.save(final_model_path)
            logger.info(f"ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")

            # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
            self._save_training_history()

            # í•™ìŠµ ë¦¬í¬íŠ¸ ìƒì„±
            report = self._generate_training_report(results, training_time)

            logger.info(f"\ní•™ìŠµ ì™„ë£Œ!")
            logger.info(f"  - ìµœê³  Box mAP50: {self.best_metrics['mAP50']:.4f}")
            logger.info(f"  - ìµœê³  Mask mAP50: {self.best_metrics.get('mask_mAP50', 0):.4f}")
            logger.info(f"  - í•™ìŠµ ì‹œê°„: {training_time/60:.2f}ë¶„")

            return report

        except Exception as e:
            logger.error(f"í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise
        finally:
            # ì •ë¦¬
            self._cleanup()

    def _save_training_history(self):
        """í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥"""
        history_file = self.output_dir / 'training_history.json'

        history = {k: [float(v) if not isinstance(v, (list, dict)) else v
                      for v in values]
                  for k, values in self.training_history.items()}

        with open(history_file, 'w') as f:
            json.dump(history, f, indent=4)

        logger.info(f"í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥: {history_file}")

    def _generate_training_report(self, results: Any, training_time: float) -> Dict[str, Any]:
        """ì¢…í•© í•™ìŠµ ë¦¬í¬íŠ¸ ìƒì„±"""

        # ê²°ê³¼ ë©”íŠ¸ë¦­ ì¶”ì¶œ (Segmentation)
        try:
            results_dict = results.results_dict if hasattr(results, 'results_dict') else {}

            final_metrics = {
                # Box metrics
                'box_mAP50': results_dict.get('metrics/mAP50(B)', 0),
                'box_mAP50-95': results_dict.get('metrics/mAP50-95(B)', 0),
                'box_precision': results_dict.get('metrics/precision(B)', 0),
                'box_recall': results_dict.get('metrics/recall(B)', 0),

                # Mask metrics (Segmentation)
                'mask_mAP50': results_dict.get('metrics/mAP50(M)', 0),
                'mask_mAP50-95': results_dict.get('metrics/mAP50-95(M)', 0),
                'mask_precision': results_dict.get('metrics/precision(M)', 0),
                'mask_recall': results_dict.get('metrics/recall(M)', 0),
            }

            # best_metrics ì—…ë°ì´íŠ¸
            if final_metrics['mask_mAP50'] > self.best_metrics['mask_mAP50']:
                self.best_metrics['mask_mAP50'] = final_metrics['mask_mAP50']
                self.best_metrics['mask_mAP50-95'] = final_metrics['mask_mAP50-95']

        except Exception as e:
            logger.warning(f"ë©”íŠ¸ë¦­ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            final_metrics = {}

        report = {
            'timestamp': datetime.now().isoformat(),
            'task': 'segmentation',
            'model': self.config.model_size.value,
            'data': self.config.data_yaml,

            'config': {
                'epochs': self.config.epochs,
                'batch_size': self.config.batch_size,
                'imgsz': self.config.imgsz,
                'optimizer': self.config.optimizer,
                'base_lr': self.config.base_lr,
                'strategy': self.config.strategy.value,
            },

            'best_metrics': self.best_metrics,
            'final_metrics': final_metrics,

            'training_time_minutes': training_time / 60,
            'training_time_hours': training_time / 3600,

            'hardware_info': {
                'gpu': torch.cuda.get_device_name() if self.device.type == 'cuda' else 'CPU',
                'gpu_count': torch.cuda.device_count(),
                'cpu_count': psutil.cpu_count(),
                'ram_gb': psutil.virtual_memory().total / 1024**3
            },

            'loss_weights': self.loss_optimizer.adaptive_weights
        }

        # ë¦¬í¬íŠ¸ ì €ì¥
        report_file = self.output_dir / 'training_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False)

        logger.info(f"í•™ìŠµ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")

        return report

    def _cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
        gc.collect()
        logger.info("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


# ================== Main Function ==================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # ì„¤ì •
    config = SegmentationTrainingConfig(
        # ëª¨ë¸
        model_size=SegModelSize.NANO,  # nano, small, medium, large, xlarge

        # ë°ì´í„°
        data_yaml=r"C:\Users\LX\Nong-View\model3_greenhouse_seg_processed\data.yaml",

        # í•™ìŠµ ì„¤ì •
        epochs=100,
        batch_size=16,
        imgsz=640,

        # ìµœì í™”
        optimizer="AdamW",
        base_lr=0.001,
        final_lr=0.00001,
        warmup_epochs=3,

        # ì „ëµ
        strategy=TrainingStrategy.PROGRESSIVE,

        # ì†ì‹¤ ê°€ì¤‘ì¹˜ (Segmentation ìµœì í™”)
        box_loss_weight=7.5,
        cls_loss_weight=0.5,
        dfl_loss_weight=1.5,
        mask_loss_weight=2.5,  # Segmentation mask loss

        # ë°ì´í„° ì¦ê°•
        mosaic=1.0,
        mixup=0.15,
        copy_paste=0.3,

        # í”„ë¡œì íŠ¸
        project="runs/segment",
        name="model3_greenhouse",
        exist_ok=False,

        # ê¸°íƒ€
        patience=30,
        save_period=5,
        plots=True,
        verbose=True
    )

    # í•™ìŠµ ì‹¤í–‰
    trainer = OptimizedSegmentationTrainer(config)
    report = trainer.train()

    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    logger.info("=" * 60)
    logger.info(f"ìµœê³  Box mAP50: {report['best_metrics']['mAP50']:.4f}")
    logger.info(f"ìµœê³  Mask mAP50: {report['best_metrics']['mask_mAP50']:.4f}")
    logger.info(f"í•™ìŠµ ì‹œê°„: {report['training_time_hours']:.2f}ì‹œê°„")
    logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {config.project}/{config.name}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
