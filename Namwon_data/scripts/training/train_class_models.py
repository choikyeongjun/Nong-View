#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í´ë˜ìŠ¤ë³„ YOLOv11 Segmentation ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ë¹„ë‹í•˜ìš°ìŠ¤ ë‹¨ë™, ë¹„ë‹í•˜ìš°ìŠ¤ ë‹¤ë™, ê³¤í¬ì‚¬ì¼ë¦¬ì§€ ê°ê° ë³„ë„ ëª¨ë¸ í•™ìŠµ
"""

from ultralytics import YOLO
import torch
import os
import yaml
from pathlib import Path
from datetime import datetime
import warnings
import time

# TIF ì§€ì›ì„ ìœ„í•œ í™˜ê²½ ì„¤ì •
os.environ['OPENCV_IO_MAX_IMAGE_PIXELS'] = str(2**31-1)
warnings.filterwarnings('ignore', category=UserWarning)

# ============================================================================
# ë°ì´í„°ì…‹ ì„¤ì •
# ============================================================================
DATASETS = {
    'greenhouse_single': {
        'name': 'ë¹„ë‹í•˜ìš°ìŠ¤_ë‹¨ë™',
        'data_path': 'dataset_greenhouse_single/dataset.yaml',
        'project_name': 'greenhouse_single_training',
    },
    'greenhouse_multi': {
        'name': 'ë¹„ë‹í•˜ìš°ìŠ¤_ë‹¤ë™',
        'data_path': 'dataset_greenhouse_multi/dataset.yaml',
        'project_name': 'greenhouse_multi_training',
    },
    'silage_bale': {
        'name': 'ê³¤í¬ì‚¬ì¼ë¦¬ì§€',
        'data_path': 'dataset_silage_bale/dataset.yaml',
        'project_name': 'silage_bale_training',
    }
}

# ============================================================================
# í•˜ë“œì›¨ì–´ ì„¤ì •
# ============================================================================
HARDWARE_CONFIG = {
    'vram_gb': 48,
    'gpu_name': 'RTX A6000',
    'cpu_cores': os.cpu_count() or 8,
}

# ============================================================================
# í•™ìŠµ ì„¤ì •
# ============================================================================
TRAINING_CONFIG = {
    'epochs': 100,
    'device': 'cuda',
    'model': 'yolo11x-seg.pt',
    'imgsz': 1024,
    'patience': 30,
}

# ìë™ íŠœë‹ ì„¤ì •
AUTO_TUNE_CONFIG = {
    'enable': True,
    'start_micro': 2,
    'start_target': 32,
    'max_target': 128,
    'increment_step': 32,
    'test_iterations': 5,
    'safe_margin': 0.85,
}

# ============================================================================
# TIF 4ì±„ë„ â†’ 3ì±„ë„ ë³€í™˜
# ============================================================================
def convert_tif_to_3channel(data_path):
    """TIF ë°ì´í„°ì…‹ì˜ 4ì±„ë„ ì´ë¯¸ì§€ë¥¼ 3ì±„ë„ë¡œ ë³€í™˜"""
    from PIL import Image

    print("="*70)
    print("ğŸ”§ TIF ì´ë¯¸ì§€ ì±„ë„ ë³€í™˜ (4ì±„ë„ â†’ 3ì±„ë„)")
    print("="*70)

    with open(data_path, 'r', encoding='utf-8') as f:
        dataset_info = yaml.safe_load(f)

    dataset_dir = Path(data_path).parent
    image_dirs = []

    if 'train' in dataset_info:
        image_dirs.append(dataset_dir / dataset_info['train'])
    if 'val' in dataset_info:
        image_dirs.append(dataset_dir / dataset_info['val'])
    if 'test' in dataset_info:
        image_dirs.append(dataset_dir / dataset_info['test'])

    total_converted = 0
    total_checked = 0

    for img_dir in image_dirs:
        if not img_dir.exists():
            continue

        print(f"\nğŸ“‚ ì²˜ë¦¬ ì¤‘: {img_dir}")

        tif_files = list(img_dir.glob('*.tif')) + list(img_dir.glob('*.tiff'))

        for tif_file in tif_files:
            total_checked += 1

            try:
                img = Image.open(tif_file)

                if img.mode == 'RGBA' or (hasattr(img, 'n_frames') and img.n_frames > 3):
                    print(f"   ğŸ”„ ë³€í™˜: {tif_file.name} ({img.mode} â†’ RGB)")

                    rgb_img = img.convert('RGB')
                    rgb_img.save(tif_file, compression='tiff_lzw')

                    total_converted += 1
                    img.close()
                    rgb_img.close()

            except Exception as e:
                print(f"   âš ï¸  ì˜¤ë¥˜: {tif_file.name} - {e}")

    print(f"\nâœ… ë³€í™˜ ì™„ë£Œ: {total_checked}ê°œ í™•ì¸, {total_converted}ê°œ ë³€í™˜")

# ============================================================================
# ë°°ì¹˜ í¬ê¸° ìë™ íŠœë‹
# ============================================================================
def auto_tune_batch_size(data_path, imgsz, device, model_path):
    """ë°°ì¹˜ í¬ê¸° ìë™ íŠœë‹"""

    print("="*70)
    print("ğŸ” ë°°ì¹˜ í¬ê¸° ìë™ íŠœë‹ ì‹œì‘")
    print("="*70)

    if device != 'cuda' or not torch.cuda.is_available():
        print("âš ï¸ GPU ì—†ìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©: micro=1, target=1")
        return 1, 1

    vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"\nğŸ’¾ VRAM ì´ëŸ‰: {vram_total:.1f}GB")
    print(f"   ì•ˆì „ ë§ˆì§„: {AUTO_TUNE_CONFIG['safe_margin']*100:.0f}%")

    print(f"\nğŸ¤– í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ë¡œë“œ: {model_path}")
    model = YOLO(model_path)

    best_micro = AUTO_TUNE_CONFIG['start_micro']
    best_target = AUTO_TUNE_CONFIG['start_target']
    current_target = AUTO_TUNE_CONFIG['start_target']

    print(f"\nğŸ¯ íŠœë‹ ì‹œì‘ì : micro={best_micro}, target={best_target}")

    test_results = []

    while current_target <= AUTO_TUNE_CONFIG['max_target']:
        accumulate = current_target // best_micro

        print(f"\n{'='*60}")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì¤‘: micro={best_micro} Ã— {accumulate} = target={current_target}")

        success = True
        max_vram_used = 0

        try:
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

            print(f"   ë°˜ë³µ í…ŒìŠ¤íŠ¸ {AUTO_TUNE_CONFIG['test_iterations']}íšŒ ìˆ˜í–‰ ì¤‘...")

            for i in range(AUTO_TUNE_CONFIG['test_iterations']):
                results = model.train(
                    data=data_path,
                    epochs=1,
                    batch=best_micro,
                    imgsz=imgsz,
                    device=device,
                    nbs=current_target,
                    cache=False,
                    workers=2,
                    verbose=False,
                    plots=False,
                    save=False,
                    exist_ok=True,
                    project='temp_tune',
                    name=f'test_{current_target}',
                    patience=0,
                    val=False,
                )

                vram_used = torch.cuda.max_memory_allocated() / 1024**3
                max_vram_used = max(max_vram_used, vram_used)

                print(f"      [{i+1}/{AUTO_TUNE_CONFIG['test_iterations']}] VRAM: {vram_used:.2f}GB / {vram_total:.1f}GB ({vram_used/vram_total*100:.1f}%)")

                if vram_used > vram_total * AUTO_TUNE_CONFIG['safe_margin']:
                    print(f"      âš ï¸ ì•ˆì „ ë§ˆì§„ ì´ˆê³¼!")
                    success = False
                    break

                torch.cuda.empty_cache()
                time.sleep(0.5)

            if success:
                vram_percent = max_vram_used / vram_total * 100
                print(f"   âœ… ì„±ê³µ! ìµœëŒ€ VRAM: {max_vram_used:.2f}GB ({vram_percent:.1f}%)")

                test_results.append({
                    'micro': best_micro,
                    'target': current_target,
                    'accumulate': accumulate,
                    'vram_used': max_vram_used,
                    'vram_percent': vram_percent,
                    'success': True
                })

                best_target = current_target
                current_target += AUTO_TUNE_CONFIG['increment_step']
            else:
                print(f"   âŒ ì‹¤íŒ¨! ì´ì „ ì„¤ì •ìœ¼ë¡œ ë¡¤ë°±")
                break

        except torch.cuda.OutOfMemoryError:
            print(f"   âŒ OOM ì—ëŸ¬! ë©”ëª¨ë¦¬ ë¶€ì¡±")
            break

        except Exception as e:
            print(f"   âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            break

    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    try:
        import shutil
        if os.path.exists('temp_tune'):
            shutil.rmtree('temp_tune')
    except:
        pass

    torch.cuda.empty_cache()

    print("\n" + "="*70)
    print("ğŸ“Š ìë™ íŠœë‹ ê²°ê³¼")
    print("="*70)

    if test_results:
        print("\nì„±ê³µí•œ í…ŒìŠ¤íŠ¸:")
        print(f"{'Target':>8} {'Micro':>8} {'Accumulate':>12} {'VRAM':>12} {'ë¹„ìœ¨':>10}")
        print("-" * 60)
        for result in test_results:
            if result['success']:
                print(f"{result['target']:>8} {result['micro']:>8} {result['accumulate']:>12} "
                      f"{result['vram_used']:>10.2f}GB {result['vram_percent']:>9.1f}%")

    print(f"\nâœ¨ ìµœì  ì„¤ì •:")
    print(f"   ë§ˆì´í¬ë¡œë°°ì¹˜: {best_micro}")
    print(f"   íƒ€ê¹ƒ ë¯¸ë‹ˆë°°ì¹˜: {best_target}")
    print(f"   ê·¸ë¼ë””ì–¸íŠ¸ ëˆ„ì : {best_target // best_micro}")

    time.sleep(2)
    return best_micro, best_target

# ============================================================================
# ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
# ============================================================================
def train_model(dataset_key, mbp_micro, mbp_target):
    """ëª¨ë¸ í•™ìŠµ"""

    dataset_config = DATASETS[dataset_key]
    data_path = dataset_config['data_path']
    project_name = dataset_config['project_name']

    print("\n" + "="*70)
    print(f"ğŸŒ± {dataset_config['name']} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("="*70)

    accumulate = mbp_target // mbp_micro

    print("\nğŸ§® MBP (Micro-Batch Processing) ì„¤ì •")
    print(f"   ë§ˆì´í¬ë¡œë°°ì¹˜ í¬ê¸°: {mbp_micro}")
    print(f"   íƒ€ê¹ƒ ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°: {mbp_target}")
    print(f"   ê·¸ë¼ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…: {accumulate}")

    # CUDA ìµœì í™”
    if TRAINING_CONFIG['device'] == 'cuda' and torch.cuda.is_available():
        print("\nâš¡ CUDA ìµœì í™” í™œì„±í™”")
        try:
            import torch.backends.cudnn as cudnn
            cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("   âœ“ cuDNN benchmark, TF32 í™œì„±í™”")
        except:
            pass

        print(f"\nğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   VRAM: {vram:.1f}GB")

    # ë°ì´í„°ì…‹ ì •ë³´
    print(f"\nğŸ“‚ ë°ì´í„°ì…‹: {data_path}")
    with open(data_path, 'r', encoding='utf-8') as f:
        dataset_info = yaml.safe_load(f)
        print(f"   í´ë˜ìŠ¤ ìˆ˜: {dataset_info['nc']}")
        print(f"   í´ë˜ìŠ¤: {dataset_info['names']}")

        if 'dataset_stats' in dataset_info:
            stats = dataset_info['dataset_stats']
            print(f"\n   ğŸ“Š ë°ì´í„° í†µê³„:")
            print(f"      Train: {stats.get('train_images', 'N/A')}ê°œ")
            print(f"      Val: {stats.get('val_images', 'N/A')}ê°œ")
            print(f"      Test: {stats.get('test_images', 'N/A')}ê°œ")
            print(f"      ì´ ê°ì²´: {stats.get('total_objects', 'N/A')}ê°œ")

    # ëª¨ë¸ ì´ˆê¸°í™”
    print(f"\nğŸ¤– ëª¨ë¸ ì´ˆê¸°í™”: {TRAINING_CONFIG['model']}")
    model = YOLO(TRAINING_CONFIG['model'])

    # Workers ë™ì  ì¡°ì •
    workers = max(4, min(8, HARDWARE_CONFIG['cpu_cores'] // 2))
    if mbp_target >= 64:
        workers = max(6, workers)

    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    training_args = {
        'data': data_path,
        'epochs': TRAINING_CONFIG['epochs'],
        'batch': mbp_micro,
        'imgsz': TRAINING_CONFIG['imgsz'],
        'device': TRAINING_CONFIG['device'],
        'project': project_name,
        'name': f'run_{datetime.now().strftime("%Y%m%d_%H%M%S")}',

        # MBP ì„¤ì •
        'nbs': mbp_target,

        # ì˜µí‹°ë§ˆì´ì €
        'optimizer': 'AdamW',
        'lr0': 0.0008,
        'lrf': 0.01,
        'momentum': 0.937,
        'weight_decay': 0.0005,

        # ì¦ê°• ì„¤ì •
        'hsv_h': 0.010,
        'hsv_s': 0.5,
        'hsv_v': 0.3,
        'degrees': 5,
        'translate': 0.05,
        'scale': 0.3,
        'shear': 1.0,
        'perspective': 0.0,
        'flipud': 0.5,
        'fliplr': 0.5,
        'mosaic': 0.2,
        'mixup': 0.05,
        'copy_paste': 0.5,
        'close_mosaic': 40,

        # í•™ìŠµ ì„¤ì •
        'patience': TRAINING_CONFIG['patience'],
        'save': True,
        'save_period': 10,
        'cache': False,
        'workers': workers,
        'exist_ok': True,
        'pretrained': True,
        'amp': True,
        'val': True,
        'plots': True,

        # ì†ì‹¤ ê°€ì¤‘ì¹˜
        'box': 7.5,
        'cls': 0.5,
        'dfl': 1.5,

        # ê¸°íƒ€
        'rect': False,
        'cos_lr': True,
    }

    print("\nğŸ“Š í•™ìŠµ ì„¤ì •:")
    print(f"   ì—í¬í¬: {TRAINING_CONFIG['epochs']}")
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {TRAINING_CONFIG['imgsz']}x{TRAINING_CONFIG['imgsz']}")
    print(f"   í•™ìŠµë¥ : {training_args['lr0']}")
    print(f"   ì˜µí‹°ë§ˆì´ì €: {training_args['optimizer']}")
    print(f"   DataLoader workers: {workers}")

    print("\n" + "="*70)
    print("ğŸš€ í•™ìŠµ ì‹œì‘!")
    print("="*70)

    try:
        torch.cuda.empty_cache()

        results = model.train(**training_args)

        print("\n" + "="*70)
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        print("="*70)

        # ëª¨ë¸ ê²½ë¡œ
        best_model_path = Path(project_name) / training_args['name'] / 'weights' / 'best.pt'
        last_model_path = Path(project_name) / training_args['name'] / 'weights' / 'last.pt'

        print(f"\nğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜:")
        print(f"   ìµœê³  ì„±ëŠ¥: {best_model_path}")
        print(f"   ë§ˆì§€ë§‰: {last_model_path}")

        return best_model_path

    except torch.cuda.OutOfMemoryError:
        print("\nâŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±!")
        print("   í•´ê²°: AUTO_TUNE_CONFIG['safe_margin']ì„ ë‚®ì¶”ì„¸ìš”.")
        return None

    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================
def main():
    print("="*70)
    print("ğŸš€ í´ë˜ìŠ¤ë³„ YOLOv11 Segmentation ëª¨ë¸ í•™ìŠµ")
    print("="*70)

    print(f"\nğŸ“‹ í•™ìŠµí•  ëª¨ë¸: {len(DATASETS)}ê°œ")
    for key, config in DATASETS.items():
        print(f"   - {config['name']}: {config['data_path']}")

    print(f"\nâš™ï¸  í•™ìŠµ ì„¤ì •:")
    print(f"   ì—í¬í¬: {TRAINING_CONFIG['epochs']}")
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {TRAINING_CONFIG['imgsz']}")
    print(f"   ëª¨ë¸: {TRAINING_CONFIG['model']}")
    print(f"   ìë™ íŠœë‹: {'í™œì„±í™” âœ“' if AUTO_TUNE_CONFIG['enable'] else 'ë¹„í™œì„±í™” âœ—'}")

    # GPU ì²´í¬
    if not torch.cuda.is_available():
        print("\nâŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return

    print(f"\nğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"   VRAM: {vram:.1f}GB")

    # ê° ë°ì´í„°ì…‹ë³„ë¡œ ìˆœì°¨ í•™ìŠµ
    results = {}

    for dataset_key, dataset_config in DATASETS.items():
        print("\n" + "="*70)
        print(f"ğŸ“¦ {dataset_config['name']} ì²˜ë¦¬ ì¤‘...")
        print("="*70)

        data_path = dataset_config['data_path']

        # TIF ì±„ë„ ë³€í™˜
        print("\n")
        convert_tif_to_3channel(data_path)

        # ìë™ íŠœë‹
        if AUTO_TUNE_CONFIG['enable']:
            mbp_micro, mbp_target = auto_tune_batch_size(
                data_path=data_path,
                imgsz=TRAINING_CONFIG['imgsz'],
                device=TRAINING_CONFIG['device'],
                model_path=TRAINING_CONFIG['model']
            )
        else:
            mbp_micro = AUTO_TUNE_CONFIG['start_micro']
            mbp_target = AUTO_TUNE_CONFIG['start_target']

        # í•™ìŠµ ì‹œì‘
        best_model = train_model(dataset_key, mbp_micro, mbp_target)
        results[dataset_key] = best_model

        print(f"\nâœ… {dataset_config['name']} ì™„ë£Œ!")
        if best_model:
            print(f"   ëª¨ë¸: {best_model}")

    # ìµœì¢… ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ!")
    print("="*70)

    for key, model_path in results.items():
        dataset_name = DATASETS[key]['name']
        if model_path:
            print(f"\nâœ… {dataset_name}")
            print(f"   ğŸ“ {model_path}")
        else:
            print(f"\nâŒ {dataset_name}: í•™ìŠµ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
