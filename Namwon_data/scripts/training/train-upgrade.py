#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TIF ì´ë¯¸ì§€ ì§ì ‘ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (MBP ìë™ ìµœì í™” ë²„ì „ + CLI ì§€ì›)
ê³ í•´ìƒë„ GeoTIFF íŒŒì¼ì„ ì‚¬ìš©í•œ ì‚¬ë£Œì‘ë¬¼ íƒì§€ ëª¨ë¸ í•™ìŠµ
+ MBP (Micro-Batch Processing): í° ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë§ˆì´í¬ë¡œë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ VRAM í•œê³„ ê·¹ë³µ
+ GMM ê¸°ë°˜ ì „ì†¡ ìµœì í™”: pinned memory, CUDA ìµœì í™”
+ Auto-Tuning: ë„¤íŠ¸ì›Œí¬ í•‘ì²˜ëŸ¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ìµœì ê°’ íƒìƒ‰
+ CLI ì§€ì›: ë°ì´í„°ì…‹ í´ë”ëª…ì„ ì¸ìë¡œ ë°›ì•„ ìœ ì—°í•˜ê²Œ í•™ìŠµ ê°€ëŠ¥

ì‚¬ìš© ì˜ˆì‹œ:
    python train-upgrade.py --dataset dataset_greenhouse_multi --epochs 100
    python train-upgrade.py --dataset growth_tif_dataset --imgsz 1024 --auto-tune
"""

from ultralytics import YOLO
import torch
import os
import yaml
from pathlib import Path
from datetime import datetime
import warnings
import time
import argparse
import sys

# TIF ì§€ì›ì„ ìœ„í•œ í™˜ê²½ ì„¤ì •
os.environ['OPENCV_IO_MAX_IMAGE_PIXELS'] = str(2**31-1)  # OpenCV í”½ì…€ ì œí•œ í•´ì œ
warnings.filterwarnings('ignore', category=UserWarning)  # TIF ê²½ê³  ë¬´ì‹œ

# ============================================================================
# RTX A6000 48GB í•˜ë“œì½”ë”© ì„¤ì •
# ============================================================================
HARDWARE_CONFIG = {
    'vram_gb': 48,
    'gpu_name': 'RTX A6000',
    'cpu_cores': os.cpu_count() or 8,
}

DATASET_CONFIG = {
    'data_path': 'growth_tif_dataset/dataset.yaml',
    'imgsz': 1024,
}

TRAINING_CONFIG = {
    'epochs': 100,
    'device': 'cuda',
    'model': 'yolo11x-seg.pt',
    'project_name': 'growth_tif_training',
}

# ìë™ íŠœë‹ ì„¤ì •
AUTO_TUNE_CONFIG = {
    'enable': True,  # ìë™ íŠœë‹ í™œì„±í™”
    'start_micro': 2,  # ì‹œì‘ ë§ˆì´í¬ë¡œë°°ì¹˜ í¬ê¸°
    'start_target': 32,  # ì‹œì‘ íƒ€ê¹ƒ ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
    'max_target': 128,  # ìµœëŒ€ íƒ€ê¹ƒ ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
    'increment_step': 32,  # ì¦ê°€ ë‹¨ê³„ (32ì”© ì¦ê°€)
    'test_iterations': 5,  # í…ŒìŠ¤íŠ¸ ë°˜ë³µ íšŸìˆ˜ (ì•ˆì •ì„± í™•ì¸)
    'safe_margin': 0.85,  # ì•ˆì „ ë§ˆì§„ (85% VRAM ì‚¬ìš©ê¹Œì§€ë§Œ)
}

# ============================================================================
# TIF 4ì±„ë„ â†’ 3ì±„ë„ ë³€í™˜ ì „ì²˜ë¦¬
# ============================================================================
def convert_tif_to_3channel(data_path):
    """
    TIF ë°ì´í„°ì…‹ì˜ 4ì±„ë„ ì´ë¯¸ì§€ë¥¼ 3ì±„ë„ë¡œ ë³€í™˜
    """
    from PIL import Image
    import glob
    
    print("="*70)
    print("ğŸ”§ TIF ì´ë¯¸ì§€ ì±„ë„ ë³€í™˜ (4ì±„ë„ â†’ 3ì±„ë„)")
    print("="*70)
    
    # dataset.yaml ì½ê¸°
    with open(data_path, 'r', encoding='utf-8') as f:
        dataset_info = yaml.safe_load(f)
    
    # ì´ë¯¸ì§€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    dataset_dir = Path(data_path).parent
    image_dirs = []
    
    if 'train' in dataset_info:
        image_dirs.append(dataset_dir / dataset_info['train'])
    if 'val' in dataset_info:
        image_dirs.append(dataset_dir / dataset_info['val'])
    if 'test' in dataset_info:
        image_dirs.append(dataset_dir / dataset_info['test'])
    
    total_converted = 0
    total_checked = 0
    
    for img_dir in image_dirs:
        if not img_dir.exists():
            continue
            
        print(f"\nğŸ“‚ ì²˜ë¦¬ ì¤‘: {img_dir}")
        
        # TIF íŒŒì¼ ì°¾ê¸°
        tif_files = list(img_dir.glob('*.tif')) + list(img_dir.glob('*.tiff'))
        
        for tif_file in tif_files:
            total_checked += 1
            
            try:
                img = Image.open(tif_file)
                
                # 4ì±„ë„ (RGBA) ì²´í¬
                if img.mode == 'RGBA' or img.n_frames > 3:
                    print(f"   ğŸ”„ ë³€í™˜: {tif_file.name} ({img.mode} â†’ RGB)")
                    
                    # RGBë¡œ ë³€í™˜
                    rgb_img = img.convert('RGB')
                    
                    # ì›ë³¸ ë®ì–´ì“°ê¸°
                    rgb_img.save(tif_file, compression='tiff_lzw')
                    
                    total_converted += 1
                    
                    img.close()
                    rgb_img.close()
                    
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {tif_file.name} - {e}")
    
    print(f"\nâœ… ë³€í™˜ ì™„ë£Œ!")
    print(f"   ì´ í™•ì¸: {total_checked}ê°œ")
    print(f"   ë³€í™˜ë¨: {total_converted}ê°œ")
    
    if total_converted > 0:
        print(f"\n   â€» {total_converted}ê°œ ì´ë¯¸ì§€ê°€ 3ì±„ë„ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   â€» ì´ì œ YOLO ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


# ============================================================================
# ë°°ì¹˜ í¬ê¸° ìë™ íŠœë‹ í•¨ìˆ˜ (ë„¤íŠ¸ì›Œí¬ í•‘ ë°©ì‹)
# ============================================================================
def auto_tune_batch_size(data_path, imgsz, device, model_path):
    """
    ë„¤íŠ¸ì›Œí¬ í•‘ì²˜ëŸ¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ì ì§„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ìµœì ê°’ ì°¾ê¸°
    
    Returns:
        tuple: (ìµœì _ë§ˆì´í¬ë¡œë°°ì¹˜, ìµœì _íƒ€ê¹ƒë¯¸ë‹ˆë°°ì¹˜)
    """
    print("="*70)
    print("ğŸ” ë°°ì¹˜ í¬ê¸° ìë™ íŠœë‹ ì‹œì‘ (í•‘ í…ŒìŠ¤íŠ¸ ë°©ì‹)")
    print("="*70)
    
    if device != 'cuda' or not torch.cuda.is_available():
        print("âš ï¸ GPU ì—†ìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©: micro=1, target=1")
        return 1, 1
    
    vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"\nğŸ’¾ VRAM ì´ëŸ‰: {vram_total:.1f}GB")
    print(f"   ì•ˆì „ ë§ˆì§„: {AUTO_TUNE_CONFIG['safe_margin']*100:.0f}% ({vram_total*AUTO_TUNE_CONFIG['safe_margin']:.1f}GBê¹Œì§€ ì‚¬ìš©)")
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ¤– í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ë¡œë“œ: {model_path}")
    model = YOLO(model_path)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ: {data_path}")
    
    best_micro = AUTO_TUNE_CONFIG['start_micro']
    best_target = AUTO_TUNE_CONFIG['start_target']
    current_target = AUTO_TUNE_CONFIG['start_target']
    
    print(f"\nğŸ¯ íŠœë‹ ì‹œì‘ì : micro={best_micro}, target={best_target}")
    print(f"   ìµœëŒ€ ëª©í‘œ: target={AUTO_TUNE_CONFIG['max_target']}")
    print(f"   ì¦ê°€ ë‹¨ê³„: {AUTO_TUNE_CONFIG['increment_step']}ì”©")
    
    test_results = []
    
    while current_target <= AUTO_TUNE_CONFIG['max_target']:
        accumulate = current_target // best_micro
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì¤‘: micro={best_micro} Ã— {accumulate} = target={current_target}")
        print(f"{'='*60}")
        
        success = True
        max_vram_used = 0
        
        try:
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            # ì§§ì€ í…ŒìŠ¤íŠ¸ í•™ìŠµ (ëª‡ ë²ˆì˜ iterationë§Œ)
            print(f"   ë°˜ë³µ í…ŒìŠ¤íŠ¸ {AUTO_TUNE_CONFIG['test_iterations']}íšŒ ìˆ˜í–‰ ì¤‘...")
            
            for i in range(AUTO_TUNE_CONFIG['test_iterations']):
                results = model.train(
                    data=data_path,
                    epochs=1,  # 1 ì—í­ë§Œ
                    batch=best_micro,
                    imgsz=imgsz,
                    device=device,
                    nbs=current_target,  # nominal batch sizeë¡œ ìë™ accumulate ê³„ì‚°
                    cache=False,
                    workers=2,  # í…ŒìŠ¤íŠ¸ëŠ” ì›Œì»¤ ìµœì†Œí™”
                    verbose=False,  # ë¡œê·¸ ìµœì†Œí™”
                    plots=False,
                    save=False,
                    exist_ok=True,
                    project='temp_tune',
                    name=f'test_{current_target}',
                    patience=0,
                    val=False,  # ê²€ì¦ ìŠ¤í‚µ
                )
                
                # VRAM ì‚¬ìš©ëŸ‰ ì²´í¬
                vram_used = torch.cuda.max_memory_allocated() / 1024**3
                max_vram_used = max(max_vram_used, vram_used)
                
                print(f"      [{i+1}/{AUTO_TUNE_CONFIG['test_iterations']}] VRAM: {vram_used:.2f}GB / {vram_total:.1f}GB ({vram_used/vram_total*100:.1f}%)")
                
                # ì•ˆì „ ë§ˆì§„ ì²´í¬
                if vram_used > vram_total * AUTO_TUNE_CONFIG['safe_margin']:
                    print(f"      âš ï¸ ì•ˆì „ ë§ˆì§„ ì´ˆê³¼! ({vram_used/vram_total*100:.1f}% > {AUTO_TUNE_CONFIG['safe_margin']*100:.0f}%)")
                    success = False
                    break
                
                torch.cuda.empty_cache()
                time.sleep(0.5)
            
            if success:
                vram_percent = max_vram_used / vram_total * 100
                print(f"   âœ… ì„±ê³µ! ìµœëŒ€ VRAM: {max_vram_used:.2f}GB ({vram_percent:.1f}%)")
                
                test_results.append({
                    'micro': best_micro,
                    'target': current_target,
                    'accumulate': accumulate,
                    'vram_used': max_vram_used,
                    'vram_percent': vram_percent,
                    'success': True
                })
                
                # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ
                best_target = current_target
                current_target += AUTO_TUNE_CONFIG['increment_step']
            else:
                print(f"   âŒ ì‹¤íŒ¨! ì´ì „ ì„¤ì •ìœ¼ë¡œ ë¡¤ë°±")
                break
                
        except torch.cuda.OutOfMemoryError:
            print(f"   âŒ OOM ì—ëŸ¬! ë©”ëª¨ë¦¬ ë¶€ì¡±")
            test_results.append({
                'micro': best_micro,
                'target': current_target,
                'accumulate': accumulate,
                'success': False,
                'error': 'OOM'
            })
            break
            
        except Exception as e:
            print(f"   âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            break
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    try:
        import shutil
        if os.path.exists('temp_tune'):
            shutil.rmtree('temp_tune')
    except:
        pass
    
    torch.cuda.empty_cache()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ“Š ìë™ íŠœë‹ ê²°ê³¼ ìš”ì•½")
    print("="*70)
    
    if test_results:
        print("\nì„±ê³µí•œ í…ŒìŠ¤íŠ¸:")
        print(f"{'Target':>8} {'Micro':>8} {'Accumulate':>12} {'VRAM':>12} {'ë¹„ìœ¨':>10}")
        print("-" * 60)
        for result in test_results:
            if result['success']:
                print(f"{result['target']:>8} {result['micro']:>8} {result['accumulate']:>12} "
                      f"{result['vram_used']:>10.2f}GB {result['vram_percent']:>9.1f}%")
    
    print(f"\nâœ¨ ìµœì  ì„¤ì • ì„ íƒ:")
    print(f"   ë§ˆì´í¬ë¡œë°°ì¹˜ (micro): {best_micro}")
    print(f"   íƒ€ê¹ƒ ë¯¸ë‹ˆë°°ì¹˜ (target): {best_target}")
    print(f"   ê·¸ë¼ë””ì–¸íŠ¸ ëˆ„ì  (accumulate): {best_target // best_micro}")
    
    expected_vram = test_results[-1]['vram_used'] if test_results and test_results[-1]['success'] else 0
    if expected_vram > 0:
        print(f"   ì˜ˆìƒ VRAM ì‚¬ìš©ëŸ‰: {expected_vram:.2f}GB ({expected_vram/vram_total*100:.1f}%)")
    
    print(f"\nğŸ’¡ íŠœë‹ ì™„ë£Œ! ì´ì œ ì „ì²´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    time.sleep(2)  # ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ í™•ì¸í•  ì‹œê°„
    return best_micro, best_target


# ============================================================================
# ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
# ============================================================================
def train_tif_model_mbp(
    data_path,
    epochs,
    imgsz,
    device,
    model_path,
    project_name,
    mbp_micro,
    mbp_target
):
    """
    TIF ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì‚¬ìš©í•œ ëª¨ë¸ í•™ìŠµ (MBP ìµœì í™”)
    """
    
    print("\n" + "="*70)
    print("ğŸŒ± ì‚¬ë£Œì‘ë¬¼ ìƒìœ¡ê¸° ëª¨ë¸ í•™ìŠµ (TIF + MBP ìµœì í™”)")
    print("="*70)
    
    # === MBP ì„¤ì • ===
    accumulate = mbp_target // mbp_micro
    
    print("\nğŸ§® MBP (Micro-Batch Processing) ìµœì¢… ì„¤ì •")
    print(f"   ë§ˆì´í¬ë¡œë°°ì¹˜ í¬ê¸° (ì‹¤ì œ batch): {mbp_micro}")
    print(f"   íƒ€ê¹ƒ ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°: {mbp_target}")
    print(f"   ê·¸ë¼ë””ì–¸íŠ¸ ëˆ„ì (accumulate) ìŠ¤í…: {accumulate}")
    print(f"   íš¨ê³¼: VRAMì€ micro={mbp_micro} ê¸°ì¤€, ì„±ëŠ¥ì€ target={mbp_target} ê¸°ì¤€")
    print(f"   â€» ê° ë§ˆì´í¬ë¡œë°°ì¹˜ ì†ì‹¤ì€ 1/NSÂµ={1/accumulate:.4f}ë¡œ ì •ê·œí™”ë¨")
    
    # === CUDA ìµœì í™” ì„¤ì • (GMM ì•„ì´ë””ì–´ ì ìš©) ===
    if device == 'cuda' and torch.cuda.is_available():
        print("\nâš¡ CUDA ì „ì†¡ ìµœì í™” í™œì„±í™”")
        try:
            import torch.backends.cudnn as cudnn
            cudnn.benchmark = True
            print("   âœ“ cuDNN benchmark í™œì„±í™”")
        except Exception:
            pass
        
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        print("   âœ“ TF32 í™œì„±í™” (Ampere+ GPU ê°€ì†)")
        print("   âœ“ Pinned memory: DataLoaderì—ì„œ ìë™ í™œì„±í™”")
        print("   â†’ CPUâ†’GPU ì „ì†¡ ìµœì í™”ë¡œ I/O ë³‘ëª© ì™„í™”")
    
    # === í•˜ë“œì›¨ì–´ ì •ë³´ ===
    print(f"\nğŸ–¥ï¸ í•˜ë“œì›¨ì–´ ì •ë³´")
    if device == 'cuda' and torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   VRAM: {vram:.1f}GB")
        print(f"   CPU ì½”ì–´: {HARDWARE_CONFIG['cpu_cores']}ê°œ")
    
    # === ë°ì´í„°ì…‹ ì •ë³´ ===
    print(f"\nğŸ“‚ ë°ì´í„°ì…‹: {data_path}")
    with open(data_path, 'r', encoding='utf-8') as f:
        dataset_info = yaml.safe_load(f)
        print(f"   ì´ë¯¸ì§€ í˜•ì‹: TIF (GeoTIFF)")
        print(f"   ì´ë¯¸ì§€ í¬ê¸°: {dataset_info.get('img_size', [imgsz, imgsz])}")
        print(f"   í´ë˜ìŠ¤ ìˆ˜: {dataset_info['nc']}")
        print(f"   í´ë˜ìŠ¤: {dataset_info['names']}")
        
        if 'dataset_stats' in dataset_info:
            stats = dataset_info['dataset_stats']
            print(f"\n   ğŸ“Š ë°ì´í„° í†µê³„:")
            print(f"      Train: {stats.get('train', 'N/A')}ê°œ")
            print(f"      Val: {stats.get('val', 'N/A')}ê°œ")
            print(f"      Test: {stats.get('test', 'N/A')}ê°œ")
    
    # === ëª¨ë¸ ì´ˆê¸°í™” ===
    print(f"\nğŸ¤– ëª¨ë¸ ì´ˆê¸°í™”: {model_path}")
    model = YOLO(model_path)
    
    # === Workers ë™ì  ì¡°ì • ===
    workers = max(4, min(8, HARDWARE_CONFIG['cpu_cores'] // 2))
    if mbp_target >= 64:
        workers = max(6, workers)
    
    # === í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ===
    training_args = {
        'data': data_path,
        'epochs': epochs,
        'batch': mbp_micro,
        'imgsz': imgsz,
        'device': device,
        'project': project_name,
        'name': f'mbp_run_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
        
        # MBP í•µì‹¬ ì„¤ì •
        # UltralyticsëŠ” nbs(nominal batch size)ë¡œ ìë™ accumulate ê³„ì‚°: accumulate = nbs / batch
        'nbs': mbp_target,  # nominal batch size = íƒ€ê¹ƒ ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
        
        # ì˜µí‹°ë§ˆì´ì €
        'optimizer': 'AdamW',
        'lr0': 0.0008,
        'lrf': 0.01,
        'momentum': 0.937,
        'weight_decay': 0.0005,
        
        # ì¦ê°• ì„¤ì •
        'hsv_h': 0.010,
        'hsv_s': 0.5,
        'hsv_v': 0.3,
        'degrees': 5,
        'translate': 0.05,
        'scale': 0.3,
        'shear': 1.0,
        'perspective': 0.0,
        'flipud': 0.5,
        'fliplr': 0.5,
        'mosaic': 0.2,
        'mixup': 0.05,
        'copy_paste': 0.5,
        'close_mosaic': 40,
        
        # í•™ìŠµ ì„¤ì •
        'patience': 30,
        'save': True,
        'save_period': 10,
        'cache': False,
        'workers': workers,
        'exist_ok': True,
        'pretrained': True,
        'amp': True,
        'val': True,
        'plots': True,
        
        # ì†ì‹¤ ê°€ì¤‘ì¹˜
        'box': 7.5,
        'cls': 0.5,
        'dfl': 1.5,
        
        # ê¸°íƒ€
        'rect': False,
        'cos_lr': True,
    }
    
    print("\nğŸ“Š í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    print(f"   ì—í¬í¬: {epochs}")
    print(f"   ë§ˆì´í¬ë¡œë°°ì¹˜ (batch): {mbp_micro}")
    print(f"   íƒ€ê¹ƒ ë¯¸ë‹ˆë°°ì¹˜ (nbs): {mbp_target}")
    print(f"   ê·¸ë¼ë””ì–¸íŠ¸ ëˆ„ì : {accumulate} ìŠ¤í… (ìë™ ê³„ì‚°: nbs/batch = {mbp_target}/{mbp_micro})")
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {imgsz}x{imgsz}")
    print(f"   í•™ìŠµë¥  (lr0): {training_args['lr0']}")
    print(f"   ì˜µí‹°ë§ˆì´ì €: {training_args['optimizer']}")
    print(f"   AMP (í˜¼í•© ì •ë°€ë„): {training_args['amp']}")
    print(f"   DataLoader workers: {workers}")
    
    print("\nğŸ’¡ MBP ì‘ë™ ì›ë¦¬:")
    print(f"   1. {mbp_target}ê°œ ì´ë¯¸ì§€ë¥¼ {accumulate}ë²ˆì— ê±¸ì³ {mbp_micro}ê°œì”© ì²˜ë¦¬")
    print(f"   2. ê° ë§ˆì´í¬ë¡œë°°ì¹˜ë§ˆë‹¤ backward()ë¡œ ê·¸ë¼ë””ì–¸íŠ¸ ëˆ„ì ")
    print(f"   3. {accumulate}ë²ˆ ëˆ„ì  í›„ 1íšŒ optimizer.step() ì‹¤í–‰")
    print(f"   4. ì†ì‹¤ì€ ìë™ìœ¼ë¡œ 1/{accumulate}={1/accumulate:.4f}ë¡œ ì •ê·œí™”")
    print(f"   â†’ VRAMì€ {mbp_micro}ê°œ ê¸°ì¤€, ì„±ëŠ¥ì€ {mbp_target}ê°œ ë°°ì¹˜ íš¨ê³¼!")
    print(f"   â€» Ultralyticsê°€ nbs={mbp_target}ì™€ batch={mbp_micro}ë¡œ ìë™ ê³„ì‚°")
    
    # === í•™ìŠµ ì‹œì‘ ===
    print("\n" + "="*70)
    print("ğŸš€ í•™ìŠµ ì‹œì‘!")
    print("="*70)
    
    try:
        torch.cuda.empty_cache()
        
        results = model.train(**training_args)
        
        print("\n" + "="*70)
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        print("="*70)
        
        # ëª¨ë¸ ê²½ë¡œ
        best_model_path = Path(project_name) / training_args['name'] / 'weights' / 'best.pt'
        last_model_path = Path(project_name) / training_args['name'] / 'weights' / 'last.pt'
        
        print(f"\nğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜:")
        print(f"   ìµœê³  ì„±ëŠ¥: {best_model_path}")
        print(f"   ë§ˆì§€ë§‰: {last_model_path}")
        
        # ê²€ì¦ ê²°ê³¼
        if hasattr(results, 'results_dict'):
            print(f"\nğŸ“ˆ ê²€ì¦ ê²°ê³¼:")
            metrics = results.results_dict
            if 'metrics/mAP50-95(M)' in metrics:
                print(f"   mAP@0.5-0.95 (Mask): {metrics['metrics/mAP50-95(M)']:.4f}")
            if 'metrics/mAP50(M)' in metrics:
                print(f"   mAP@0.5 (Mask): {metrics['metrics/mAP50(M)']:.4f}")
        
        print("\nğŸ“Š MBP í•™ìŠµ ë¶„ì„ í¬ì¸íŠ¸:")
        print("   - ë¡œê·¸ì—ì„œ optimizer step ë¹ˆë„ê°€ accumulateì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸")
        print("   - loss scaleì´ ì•ˆì •ì ì¸ì§€ í™•ì¸")
        print("   - ì¼ë°˜ í•™ìŠµ ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ (mAP, IoU)")
        
        return best_model_path
    
    except torch.cuda.OutOfMemoryError:
        print("\nâŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±!")
        print("   ìë™ íŠœë‹ì—ì„œ ì„ íƒí•œ ì„¤ì •ë„ ì‹¤ì œ í•™ìŠµì—ì„œëŠ” ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   í•´ê²°: ì½”ë“œì—ì„œ AUTO_TUNE_CONFIG['safe_margin']ì„ 0.8 ë˜ëŠ” 0.75ë¡œ ë‚®ì¶”ì„¸ìš”.")
        return None
    
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============================================================================
# CLI ì¸ì íŒŒì‹±
# ============================================================================
def parse_args():
    """
    CLI ì¸ì íŒŒì‹±
    """
    parser = argparse.ArgumentParser(
        description='YOLOv11 Segmentation í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (CLI ì§€ì›)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ì‚¬ìš© ì˜ˆì‹œ:
  python train-upgrade.py --dataset dataset_greenhouse_multi --epochs 100
  python train-upgrade.py --dataset growth_tif_dataset --imgsz 1024 --auto-tune
  python train-upgrade.py --dataset dataset_greenhouse_single --epochs 50 --model yolo11n-seg.pt
        '''
    )
    
    parser.add_argument('--dataset', type=str, required=True,
                        help='ë°ì´í„°ì…‹ í´ë” ê²½ë¡œ (ì˜ˆ: dataset_greenhouse_multi, growth_tif_dataset)')
    parser.add_argument('--epochs', type=int, default=100,
                        help='í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸ê°’: 100)')
    parser.add_argument('--imgsz', type=int, default=1024,
                        help='ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 1024)')
    parser.add_argument('--model', type=str, default='yolo11x-seg.pt',
                        help='YOLO ëª¨ë¸ íŒŒì¼ (ê¸°ë³¸ê°’: yolo11x-seg.pt)')
    parser.add_argument('--project', type=str, default=None,
                        help='í”„ë¡œì íŠ¸ ì´ë¦„ (ê¸°ë³¸ê°’: {dataset}_training)')
    parser.add_argument('--device', type=str, default='cuda',
                        help='ë””ë°”ì´ìŠ¤ (cuda ë˜ëŠ” cpu, ê¸°ë³¸ê°’: cuda)')
    parser.add_argument('--auto-tune', action='store_true',
                        help='ë°°ì¹˜ í¬ê¸° ìë™ íŠœë‹ í™œì„±í™”')
    parser.add_argument('--no-auto-tune', dest='auto_tune', action='store_false',
                        help='ë°°ì¹˜ í¬ê¸° ìë™ íŠœë‹ ë¹„í™œì„±í™”')
    parser.set_defaults(auto_tune=True)
    
    args = parser.parse_args()
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ ê²€ì¦
    dataset_path = Path(args.dataset)
    if not dataset_path.exists():
        print(f"âŒ ì˜¤ë¥˜: ë°ì´í„°ì…‹ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.dataset}")
        sys.exit(1)
    
    # dataset.yaml íŒŒì¼ í™•ì¸
    yaml_path = dataset_path / 'dataset.yaml'
    if not yaml_path.exists():
        print(f"âŒ ì˜¤ë¥˜: dataset.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {yaml_path}")
        sys.exit(1)
    
    # í”„ë¡œì íŠ¸ ì´ë¦„ ìë™ ìƒì„±
    if args.project is None:
        args.project = f"{dataset_path.name}_training"
    
    return args


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================
def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    # CLI ì¸ì íŒŒì‹±
    args = parse_args()
    
    # ì„¤ì • ì ìš©
    data_path = str(Path(args.dataset) / 'dataset.yaml')
    
    print("="*70)
    print("ğŸš€ TIF ì´ë¯¸ì§€ ì§ì ‘ í•™ìŠµ ì‹œì‘ (MBP ìë™ ìµœì í™” + CLI ì§€ì›)")
    print("="*70)
    print(f"\nğŸ“‹ í•™ìŠµ ì„¤ì •:")
    print(f"   GPU: {HARDWARE_CONFIG['gpu_name']} ({HARDWARE_CONFIG['vram_gb']}GB)")
    print(f"   ë°ì´í„°ì…‹: {args.dataset}")
    print(f"   YAML: {data_path}")
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {args.imgsz}x{args.imgsz}")
    print(f"   ì—í¬í¬: {args.epochs}")
    print(f"   ëª¨ë¸: {args.model}")
    print(f"   í”„ë¡œì íŠ¸: {args.project}")
    print(f"   ë””ë°”ì´ìŠ¤: {args.device}")
    print(f"\n   ìë™ íŠœë‹: {'í™œì„±í™” âœ“' if args.auto_tune else 'ë¹„í™œì„±í™” âœ—'}")
    
    if args.auto_tune:
        print(f"      ì‹œì‘ì : micro={AUTO_TUNE_CONFIG['start_micro']}, target={AUTO_TUNE_CONFIG['start_target']}")
        print(f"      ìµœëŒ€ê°’: target={AUTO_TUNE_CONFIG['max_target']}")
        print(f"      ì¦ê°€í­: {AUTO_TUNE_CONFIG['increment_step']}")
        print(f"      ì•ˆì „ë§ˆì§„: {AUTO_TUNE_CONFIG['safe_margin']*100:.0f}%")
    
    # GPU ì²´í¬
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("\nâŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # TIF 4ì±„ë„ â†’ 3ì±„ë„ ë³€í™˜ (í•™ìŠµ ì „ í•„ìˆ˜)
    print("\n")
    convert_tif_to_3channel(data_path)
    
    # ìë™ íŠœë‹ìœ¼ë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
    if args.auto_tune:
        mbp_micro, mbp_target = auto_tune_batch_size(
            data_path=data_path,
            imgsz=args.imgsz,
            device=args.device,
            model_path=args.model
        )
    else:
        # ìë™ íŠœë‹ ë¹„í™œì„±í™” ì‹œ ê¸°ë³¸ê°’
        mbp_micro = AUTO_TUNE_CONFIG['start_micro']
        mbp_target = AUTO_TUNE_CONFIG['start_target']
        print(f"\nğŸ“Œ ìë™ íŠœë‹ ë¹„í™œì„±í™”. ê¸°ë³¸ê°’ ì‚¬ìš©: micro={mbp_micro}, target={mbp_target}")
    
    # ì‹¤ì œ í•™ìŠµ ì‹œì‘
    train_tif_model_mbp(
        data_path=data_path,
        epochs=args.epochs,
        imgsz=args.imgsz,
        device=args.device,
        model_path=args.model,
        project_name=args.project,
        mbp_micro=mbp_micro,
        mbp_target=mbp_target
    )


if __name__ == "__main__":
    main()
