#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í´ë˜ìŠ¤ë³„ YOLOv11 Segmentation ë°ì´í„°ì…‹ ìƒì„±
ê° í´ë˜ìŠ¤(ë¹„ë‹í•˜ìš°ìŠ¤ ë‹¨ë™, ë¹„ë‹í•˜ìš°ìŠ¤ ë‹¤ë™, ê³¤í¬ì‚¬ì¼ë¦¬ì§€)ë³„ë¡œ ë³„ë„ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±
"""

import os
import shutil
import random
from pathlib import Path
from collections import defaultdict
import yaml
from tqdm import tqdm

# íƒ€ê²Ÿ í´ë˜ìŠ¤ ì •ì˜
TARGET_CLASSES = {
    9: {
        'name': 'ë¹„ë‹í•˜ìš°ìŠ¤_ë‹¨ë™',
        'output_dir': 'dataset_greenhouse_single',
        'description': 'Greenhouse Single (ë‹¨ë™ ë¹„ë‹í•˜ìš°ìŠ¤)'
    },
    10: {
        'name': 'ë¹„ë‹í•˜ìš°ìŠ¤_ë‹¤ë™',
        'output_dir': 'dataset_greenhouse_multi',
        'description': 'Greenhouse Multi (ë‹¤ë™ ë¹„ë‹í•˜ìš°ìŠ¤)'
    },
    11: {
        'name': 'ê³¤í¬ì‚¬ì¼ë¦¬ì§€',
        'output_dir': 'dataset_silage_bale',
        'description': 'Silage Bale (ê³¤í¬ì‚¬ì¼ë¦¬ì§€)'
    }
}

# ë°ì´í„° í´ë” ë¦¬ìŠ¤íŠ¸
DATA_FOLDERS = [
    '250903-ë°ì´í„°',
    '250910-ìµœì¢…ë‚©í’ˆ',
    '250911-ìµœì¢…ë‚©í’ˆ',
    '250917-ìµœì¢…ë‚©í’ˆ',
    '250918-ìµœì¢…ë‚©í’ˆ',
    '251014_ë°ì´í„°',
    '251020_ë°ì´í„°'
]

def collect_class_data(target_class_id):
    """íŠ¹ì • í´ë˜ìŠ¤ì˜ ì´ë¯¸ì§€-ë¼ë²¨ ìŒ ìˆ˜ì§‘"""

    print(f"\nğŸ“‚ í´ë˜ìŠ¤ {target_class_id} ({TARGET_CLASSES[target_class_id]['name']}) ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

    all_pairs = []

    for folder_name in DATA_FOLDERS:
        folder_path = Path(folder_name)

        label_dir = folder_path / 'ë¼ë²¨ë§ë°ì´í„°' / 'pixeljson'
        image_dir = folder_path / 'ì´ë¯¸ì§€ë°ì´í„°'

        if not label_dir.exists() or not image_dir.exists():
            continue

        # ë¼ë²¨ íŒŒì¼ íƒìƒ‰
        label_files = list(label_dir.glob('*.txt'))

        for label_file in label_files:
            # ë¼ë²¨ íŒŒì¼ì—ì„œ íƒ€ê²Ÿ í´ë˜ìŠ¤ í¬í•¨ ì—¬ë¶€ í™•ì¸
            has_target_class = False
            label_lines = []

            try:
                with open(label_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            parts = line.split()
                            if len(parts) >= 3:
                                class_id = int(parts[0])
                                if class_id == target_class_id:
                                    has_target_class = True
                                    label_lines.append(line)

                if has_target_class:
                    # ëŒ€ì‘í•˜ëŠ” TIF ì´ë¯¸ì§€ ì°¾ê¸°
                    image_name = label_file.stem
                    image_path = None

                    for ext in ['.tif', '.tiff', '.TIF', '.TIFF']:
                        candidate = image_dir / f"{image_name}{ext}"
                        if candidate.exists():
                            image_path = candidate
                            break

                    if image_path and image_path.exists():
                        all_pairs.append({
                            'image_path': image_path,
                            'label_lines': label_lines,
                            'stem': image_name,
                            'source_folder': folder_name
                        })

            except Exception as e:
                print(f"   âš ï¸  íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {label_file}: {e}")

    print(f"   âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(all_pairs)}ê°œ ì´ë¯¸ì§€")
    return all_pairs

def split_dataset(pairs, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):
    """ë°ì´í„°ì…‹ì„ train:val:testë¡œ ë¶„í• """

    random.shuffle(pairs)

    total = len(pairs)
    train_size = int(total * train_ratio)
    val_size = int(total * val_ratio)

    train_pairs = pairs[:train_size]
    val_pairs = pairs[train_size:train_size + val_size]
    test_pairs = pairs[train_size + val_size:]

    print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"   Train: {len(train_pairs)}ê°œ ({len(train_pairs)/total*100:.1f}%)")
    print(f"   Val:   {len(val_pairs)}ê°œ ({len(val_pairs)/total*100:.1f}%)")
    print(f"   Test:  {len(test_pairs)}ê°œ ({len(test_pairs)/total*100:.1f}%)")

    return train_pairs, val_pairs, test_pairs

def create_dataset(target_class_id):
    """íŠ¹ì • í´ë˜ìŠ¤ì˜ ë°ì´í„°ì…‹ ìƒì„±"""

    class_info = TARGET_CLASSES[target_class_id]
    output_dir = Path(class_info['output_dir'])

    print("\n" + "="*80)
    print(f"ğŸš€ {class_info['name']} ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print("="*80)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
    for split in ['train', 'val', 'test']:
        (output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)
        (output_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)

    # ë°ì´í„° ìˆ˜ì§‘
    all_pairs = collect_class_data(target_class_id)

    if len(all_pairs) == 0:
        print(f"âŒ {class_info['name']} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None

    # ë°ì´í„° ë¶„í• 
    random.seed(42)  # ì¬í˜„ ê°€ëŠ¥ì„±
    train_pairs, val_pairs, test_pairs = split_dataset(all_pairs)

    # íŒŒì¼ ë³µì‚¬ ë° ë¼ë²¨ ìƒì„±
    print("\nğŸ“‹ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")

    total_objects = 0

    for split_name, split_data in [('train', train_pairs), ('val', val_pairs), ('test', test_pairs)]:
        print(f"\n   {split_name.upper()} ì²˜ë¦¬ ì¤‘...")

        for item in tqdm(split_data, desc=f"   {split_name}"):
            # ì´ë¯¸ì§€ ë³µì‚¬
            img_dest = output_dir / 'images' / split_name / f"{item['stem']}.tif"

            try:
                if not img_dest.exists():
                    shutil.copy2(item['image_path'], img_dest)
            except Exception as e:
                print(f"      âš ï¸  ì´ë¯¸ì§€ ë³µì‚¬ ì‹¤íŒ¨ {item['stem']}: {e}")
                continue

            # ë¼ë²¨ ì €ì¥ (í´ë˜ìŠ¤ IDë¥¼ 0ìœ¼ë¡œ ë³€ê²½)
            label_dest = output_dir / 'labels' / split_name / f"{item['stem']}.txt"

            yolo_labels = []
            for label_line in item['label_lines']:
                # í´ë˜ìŠ¤ IDë¥¼ 0ìœ¼ë¡œ ë³€ê²½ (ë‹¨ì¼ í´ë˜ìŠ¤ ëª¨ë¸)
                parts = label_line.split()
                parts[0] = '0'  # í´ë˜ìŠ¤ IDë¥¼ 0ìœ¼ë¡œ
                yolo_labels.append(' '.join(parts))
                total_objects += 1

            with open(label_dest, 'w', encoding='utf-8') as f:
                f.write('\n'.join(yolo_labels))

    # dataset.yaml ìƒì„±
    yaml_content = {
        'path': str(output_dir.absolute()),
        'train': 'images/train',
        'val': 'images/val',
        'test': 'images/test',
        'nc': 1,  # ë‹¨ì¼ í´ë˜ìŠ¤
        'names': [class_info['name']],

        # ë©”íƒ€ë°ì´í„°
        'description': class_info['description'],
        'original_class_id': target_class_id,
        'task': 'segment',
        'image_format': 'tif',

        # í†µê³„
        'dataset_stats': {
            'total_images': len(all_pairs),
            'total_objects': total_objects,
            'train_images': len(train_pairs),
            'val_images': len(val_pairs),
            'test_images': len(test_pairs),
        }
    }

    yaml_path = output_dir / 'dataset.yaml'
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True)

    print("\n" + "="*80)
    print(f"âœ… {class_info['name']} ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print("="*80)
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir.absolute()}")
    print(f"ğŸ“„ ì„¤ì • íŒŒì¼: {yaml_path}")
    print(f"ğŸ“Š ì´ ì´ë¯¸ì§€: {len(all_pairs)}ê°œ")
    print(f"ğŸ“Š ì´ ê°ì²´: {total_objects}ê°œ")
    print(f"   - Train: {len(train_pairs)}ê°œ ì´ë¯¸ì§€")
    print(f"   - Val:   {len(val_pairs)}ê°œ ì´ë¯¸ì§€")
    print(f"   - Test:  {len(test_pairs)}ê°œ ì´ë¯¸ì§€")

    # í†µê³„ JSON ì €ì¥
    import json
    stats_path = output_dir / 'dataset_stats.json'
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(yaml_content['dataset_stats'], f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ˆ í†µê³„ íŒŒì¼: {stats_path}")

    return output_dir

def main():
    """ë©”ì¸ í•¨ìˆ˜ - 3ê°œ í´ë˜ìŠ¤ ëª¨ë‘ ì²˜ë¦¬"""

    print("="*80)
    print("ğŸŒŸ YOLOv11 Segmentation í´ë˜ìŠ¤ë³„ ë°ì´í„°ì…‹ ìƒì„±")
    print("="*80)
    print("\nğŸ“‹ ìƒì„±í•  ë°ì´í„°ì…‹:")
    for class_id, info in TARGET_CLASSES.items():
        print(f"   {class_id}. {info['name']} -> {info['output_dir']}/")

    print(f"\nğŸ“‚ ì†ŒìŠ¤ ë°ì´í„° í´ë”: {len(DATA_FOLDERS)}ê°œ")
    for folder in DATA_FOLDERS:
        print(f"   - {folder}")

    print("\nâš™ï¸  ì„¤ì •:")
    print("   - ë°ì´í„° ë¶„í• : Train:Val:Test = 8:1:1")
    print("   - ê° í´ë˜ìŠ¤: nc=1 (ë‹¨ì¼ í´ë˜ìŠ¤ ëª¨ë¸)")
    print("   - ì´ë¯¸ì§€ í˜•ì‹: TIF (GeoTIFF)")
    print("   - ë¼ë²¨ í˜•ì‹: YOLO Segmentation (polygon)")

    # ê° í´ë˜ìŠ¤ë³„ ë°ì´í„°ì…‹ ìƒì„±
    results = {}

    for class_id in TARGET_CLASSES.keys():
        try:
            output_dir = create_dataset(class_id)
            results[class_id] = output_dir
        except Exception as e:
            print(f"\nâŒ í´ë˜ìŠ¤ {class_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    # ìµœì¢… ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ‰ ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print("="*80)

    for class_id, output_dir in results.items():
        if output_dir:
            class_info = TARGET_CLASSES[class_id]
            print(f"\nâœ… {class_info['name']}")
            print(f"   ğŸ“ {output_dir}/")
            print(f"   ğŸ“„ {output_dir}/dataset.yaml")

    print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. ê° ë°ì´í„°ì…‹ì˜ í†µê³„ í™•ì¸")
    print("   2. YOLOv11 seg ëª¨ë¸ í•™ìŠµ:")
    for class_id, output_dir in results.items():
        if output_dir:
            print(f"      - yolo segment train data={output_dir}/dataset.yaml model=yolo11x-seg.pt")

    print("\n" + "="*80)

if __name__ == "__main__":
    main()
