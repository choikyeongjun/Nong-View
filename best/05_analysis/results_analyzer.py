#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Nong-View Best Performance - ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™” ì‹œìŠ¤í…œ

ê¸°ì¡´ Jupyter ë…¸íŠ¸ë¶ì˜ ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ì„ ì‹¤ìš©ì ì¸ Python ìŠ¤í¬ë¦½íŠ¸ë¡œ êµ¬í˜„:
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìë™ ê³„ì‚° (mAP, precision, recall, F1)
- ëŒ€í™”í˜• ì‹œê°í™” ë° ì°¨íŠ¸ ìƒì„±
- ëª¨ë¸/ë°ì´í„°ì…‹ ê°„ ë¹„êµ ë¶„ì„
- ìë™ ë¦¬í¬íŠ¸ ìƒì„±
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

ë‹´ë‹¹: Claude Sonnet (Data Processing & Integration)
ê°œë°œ ë‚ ì§œ: 2025-10-28
"""

import os
import json
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from collections import defaultdict, Counter
import logging

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.offline as pyo
from scipy import stats
from sklearn.metrics import precision_recall_curve, average_precision_score
import cv2
from PIL import Image, ImageDraw, ImageFont

# ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
import sys
sys.path.append(str(Path(__file__).parent.parent))
from configs.best_config import CONFIG, logger

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ í´ë˜ìŠ¤"""
    # ì •í™•ë„ ë©”íŠ¸ë¦­
    map_50: float = 0.0
    map_75: float = 0.0
    map_50_95: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    
    # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
    class_metrics: Dict[str, Dict[str, float]] = field(default_factory=dict)
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    avg_inference_time: float = 0.0
    throughput_fps: float = 0.0
    memory_usage_mb: float = 0.0
    
    # ì‹ ë¢°ë„ ë©”íŠ¸ë¦­
    avg_confidence: float = 0.0
    confidence_distribution: Dict[str, int] = field(default_factory=dict)
    
    # ê²€ì¶œ í†µê³„
    total_detections: int = 0
    detection_density: float = 0.0  # ì´ë¯¸ì§€ë‹¹ í‰ê·  ê²€ì¶œ ìˆ˜

@dataclass
class AnalysisReport:
    """ë¶„ì„ ë¦¬í¬íŠ¸ í´ë˜ìŠ¤"""
    dataset_name: str
    model_name: str
    analysis_timestamp: str
    
    # ì „ì²´ ì„±ëŠ¥
    overall_metrics: PerformanceMetrics = field(default_factory=PerformanceMetrics)
    
    # ìƒì„¸ ë¶„ì„
    detailed_analysis: Dict[str, Any] = field(default_factory=dict)
    
    # ì‹œê°í™” ê²½ë¡œ
    visualization_paths: List[str] = field(default_factory=list)
    
    # ê¶Œì¥ì‚¬í•­
    recommendations: List[str] = field(default_factory=list)

class ResultsAnalyzer:
    """ê²°ê³¼ ë¶„ì„ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = Path(output_dir) if output_dir else Path(CONFIG.data.output_path) / "analysis"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        
        # ì‹œê°í™” ì„¤ì •
        self._setup_visualization_style()
        
        self.logger.info(f"ê²°ê³¼ ë¶„ì„ê¸° ì´ˆê¸°í™”: {self.output_dir}")
    
    def _setup_visualization_style(self):
        """ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •"""
        # Matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
        plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'sans-serif']
        plt.rcParams['axes.unicode_minus'] = False
        
        # Seaborn ìŠ¤íƒ€ì¼ ì„¤ì •
        sns.set_style("whitegrid")
        sns.set_palette("husl")
        
        # ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ì •ì˜
        self.colors = {
            'primary': '#2E86AB',
            'secondary': '#A23B72', 
            'accent': '#F18F01',
            'success': '#C73E1D',
            'info': '#6C757D'
        }
    
    def analyze_inference_results(self, results_dir: str, 
                                ground_truth_dir: str = None,
                                model_name: str = "Unknown",
                                dataset_name: str = "Unknown") -> AnalysisReport:
        """ì¶”ë¡  ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        
        self.logger.info(f"ê²°ê³¼ ë¶„ì„ ì‹œì‘: {results_dir}")
        
        # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        inference_results = self._load_inference_results(results_dir)
        
        if not inference_results:
            raise ValueError(f"ì¶”ë¡  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {results_dir}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._calculate_performance_metrics(
            inference_results, ground_truth_dir
        )
        
        # ìƒì„¸ ë¶„ì„
        detailed_analysis = self._perform_detailed_analysis(inference_results)
        
        # ì‹œê°í™” ìƒì„±
        visualization_paths = self._create_visualizations(
            inference_results, metrics, model_name, dataset_name
        )
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(metrics, detailed_analysis)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = AnalysisReport(
            dataset_name=dataset_name,
            model_name=model_name,
            analysis_timestamp=pd.Timestamp.now().isoformat(),
            overall_metrics=metrics,
            detailed_analysis=detailed_analysis,
            visualization_paths=visualization_paths,
            recommendations=recommendations
        )
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        self._save_analysis_report(report)
        
        self.logger.info("ê²°ê³¼ ë¶„ì„ ì™„ë£Œ")
        
        return report
    
    def compare_models(self, model_results: Dict[str, str],
                      ground_truth_dir: str = None,
                      dataset_name: str = "Comparison") -> AnalysisReport:
        """ëª¨ë¸ ê°„ ë¹„êµ ë¶„ì„"""
        
        self.logger.info(f"ëª¨ë¸ ë¹„êµ ë¶„ì„ ì‹œì‘: {len(model_results)}ê°œ ëª¨ë¸")
        
        comparison_data = {}
        
        # ê° ëª¨ë¸ ê²°ê³¼ ë¶„ì„
        for model_name, results_dir in model_results.items():
            self.logger.info(f"ë¶„ì„ ì¤‘: {model_name}")
            
            try:
                report = self.analyze_inference_results(
                    results_dir, ground_truth_dir, model_name, dataset_name
                )
                comparison_data[model_name] = report
                
            except Exception as e:
                self.logger.error(f"ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨ {model_name}: {e}")
                continue
        
        # ë¹„êµ ì‹œê°í™” ìƒì„±
        comparison_viz_paths = self._create_comparison_visualizations(
            comparison_data, dataset_name
        )
        
        # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
        comparison_report = self._generate_comparison_report(
            comparison_data, comparison_viz_paths, dataset_name
        )
        
        self.logger.info("ëª¨ë¸ ë¹„êµ ë¶„ì„ ì™„ë£Œ")
        
        return comparison_report
    
    def _load_inference_results(self, results_dir: str) -> List[Dict]:
        """ì¶”ë¡  ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        results_path = Path(results_dir)
        inference_results = []
        
        # JSON ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ
        for json_file in results_path.glob("*_result.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                    inference_results.append(result_data)
                    
            except Exception as e:
                self.logger.error(f"ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {json_file}: {e}")
                continue
        
        self.logger.info(f"ì¶”ë¡  ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(inference_results)}ê°œ íŒŒì¼")
        
        return inference_results
    
    def _calculate_performance_metrics(self, inference_results: List[Dict],
                                     ground_truth_dir: str = None) -> PerformanceMetrics:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        metrics = PerformanceMetrics()
        
        if not inference_results:
            return metrics
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        total_detections = 0
        total_images = len(inference_results)
        inference_times = []
        confidences = []
        class_counts = Counter()
        
        for result in inference_results:
            # ê²€ì¶œ í†µê³„
            detections = result.get('detections', [])
            total_detections += len(detections)
            
            # ì„±ëŠ¥ í†µê³„
            if 'processing_time' in result:
                inference_times.append(result['processing_time'])
            
            # ì‹ ë¢°ë„ í†µê³„
            for detection in detections:
                conf = detection.get('confidence', 0)
                confidences.append(conf)
                
                class_name = detection.get('class_name', 'unknown')
                class_counts[class_name] += 1
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics.total_detections = total_detections
        metrics.detection_density = total_detections / total_images if total_images > 0 else 0
        
        if inference_times:
            metrics.avg_inference_time = np.mean(inference_times)
            metrics.throughput_fps = 1.0 / metrics.avg_inference_time if metrics.avg_inference_time > 0 else 0
        
        if confidences:
            metrics.avg_confidence = np.mean(confidences)
            
            # ì‹ ë¢°ë„ ë¶„í¬ (êµ¬ê°„ë³„)
            confidence_bins = np.histogram(confidences, bins=[0, 0.3, 0.5, 0.7, 0.9, 1.0])
            bin_labels = ['0.0-0.3', '0.3-0.5', '0.5-0.7', '0.7-0.9', '0.9-1.0']
            
            metrics.confidence_distribution = dict(zip(bin_labels, confidence_bins[0].tolist()))
        
        # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ (ê¸°ë³¸ í†µê³„)
        for class_name, count in class_counts.items():
            metrics.class_metrics[class_name] = {
                'count': count,
                'percentage': count / total_detections * 100 if total_detections > 0 else 0
            }
        
        # Ground Truthê°€ ìˆëŠ” ê²½ìš° ì •í™•ë„ ë©”íŠ¸ë¦­ ê³„ì‚°
        if ground_truth_dir:
            accuracy_metrics = self._calculate_accuracy_metrics(
                inference_results, ground_truth_dir
            )
            metrics.map_50 = accuracy_metrics.get('map_50', 0.0)
            metrics.map_75 = accuracy_metrics.get('map_75', 0.0)
            metrics.map_50_95 = accuracy_metrics.get('map_50_95', 0.0)
            metrics.precision = accuracy_metrics.get('precision', 0.0)
            metrics.recall = accuracy_metrics.get('recall', 0.0)
            metrics.f1_score = accuracy_metrics.get('f1_score', 0.0)
        
        return metrics
    
    def _calculate_accuracy_metrics(self, inference_results: List[Dict],
                                  ground_truth_dir: str) -> Dict[str, float]:
        """ì •í™•ë„ ë©”íŠ¸ë¦­ ê³„ì‚° (Ground Truth ë¹„êµ)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” YOLO ë©”íŠ¸ë¦­ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ì¸ êµ¬ì¡°ë§Œ ì œê³µ
        
        metrics = {
            'map_50': 0.0,
            'map_75': 0.0, 
            'map_50_95': 0.0,
            'precision': 0.0,
            'recall': 0.0,
            'f1_score': 0.0
        }
        
        # TODO: ì‹¤ì œ mAP ê³„ì‚° êµ¬í˜„
        # - Ground Truth ë¼ë²¨ ë¡œë“œ
        # - IoU ê³„ì‚°
        # - Precision-Recall ê³¡ì„ 
        # - mAP@0.5, mAP@0.75, mAP@0.5:0.95 ê³„ì‚°
        
        self.logger.info("ì •í™•ë„ ë©”íŠ¸ë¦­ ê³„ì‚° (Ground Truth ê¸°ë°˜)")
        
        return metrics
    
    def _perform_detailed_analysis(self, inference_results: List[Dict]) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„ ìˆ˜í–‰"""
        
        analysis = {
            'image_analysis': {},
            'detection_analysis': {},
            'performance_analysis': {},
            'quality_analysis': {}
        }
        
        if not inference_results:
            return analysis
        
        # ì´ë¯¸ì§€ë³„ ë¶„ì„
        image_sizes = []
        processing_times = []
        detection_counts = []
        
        for result in inference_results:
            detections = result.get('detections', [])
            detection_counts.append(len(detections))
            
            if 'processing_time' in result:
                processing_times.append(result['processing_time'])
        
        analysis['image_analysis'] = {
            'total_images': len(inference_results),
            'avg_detections_per_image': np.mean(detection_counts) if detection_counts else 0,
            'detection_count_std': np.std(detection_counts) if detection_counts else 0,
            'images_with_detections': len([x for x in detection_counts if x > 0]),
            'detection_coverage': len([x for x in detection_counts if x > 0]) / len(detection_counts) * 100
        }
        
        # ê²€ì¶œ ë¶„ì„
        all_detections = []
        for result in inference_results:
            all_detections.extend(result.get('detections', []))
        
        if all_detections:
            confidences = [d.get('confidence', 0) for d in all_detections]
            areas = [d.get('area', 0) for d in all_detections]
            
            analysis['detection_analysis'] = {
                'total_detections': len(all_detections),
                'confidence_stats': {
                    'mean': np.mean(confidences),
                    'std': np.std(confidences),
                    'min': np.min(confidences),
                    'max': np.max(confidences),
                    'q25': np.percentile(confidences, 25),
                    'q50': np.percentile(confidences, 50),
                    'q75': np.percentile(confidences, 75)
                },
                'area_stats': {
                    'mean': np.mean(areas) if areas else 0,
                    'std': np.std(areas) if areas else 0,
                    'min': np.min(areas) if areas else 0,
                    'max': np.max(areas) if areas else 0
                }
            }
        
        # ì„±ëŠ¥ ë¶„ì„
        if processing_times:
            analysis['performance_analysis'] = {
                'processing_time_stats': {
                    'mean': np.mean(processing_times),
                    'std': np.std(processing_times),
                    'min': np.min(processing_times),
                    'max': np.max(processing_times),
                    'q95': np.percentile(processing_times, 95)
                },
                'throughput_fps': len(processing_times) / sum(processing_times),
                'efficiency_score': self._calculate_efficiency_score(processing_times, detection_counts)
            }
        
        return analysis
    
    def _calculate_efficiency_score(self, processing_times: List[float], 
                                  detection_counts: List[int]) -> float:
        """íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not processing_times or not detection_counts:
            return 0.0
        
        # ê²€ì¶œ ìˆ˜ ëŒ€ë¹„ ì²˜ë¦¬ ì‹œê°„ íš¨ìœ¨ì„±
        total_detections = sum(detection_counts)
        total_time = sum(processing_times)
        
        if total_time == 0:
            return 0.0
        
        detections_per_second = total_detections / total_time
        
        # 0-100 ì ìˆ˜ë¡œ ì •ê·œí™” (ì„ì˜ ê¸°ì¤€: 10 detections/sec = 100ì )
        efficiency_score = min(100, detections_per_second / 10 * 100)
        
        return efficiency_score
    
    def _create_visualizations(self, inference_results: List[Dict],
                             metrics: PerformanceMetrics,
                             model_name: str, dataset_name: str) -> List[str]:
        """ì‹œê°í™” ìƒì„±"""
        
        viz_paths = []
        
        # 1. ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ
        dashboard_path = self._create_performance_dashboard(
            metrics, model_name, dataset_name
        )
        viz_paths.append(dashboard_path)
        
        # 2. ì‹ ë¢°ë„ ë¶„í¬ ì°¨íŠ¸
        confidence_path = self._create_confidence_distribution_chart(
            inference_results, model_name, dataset_name
        )
        viz_paths.append(confidence_path)
        
        # 3. í´ë˜ìŠ¤ë³„ ê²€ì¶œ ì°¨íŠ¸
        class_path = self._create_class_distribution_chart(
            metrics, model_name, dataset_name
        )
        viz_paths.append(class_path)
        
        # 4. ì„±ëŠ¥ ì‹œê³„ì—´ ì°¨íŠ¸
        performance_path = self._create_performance_timeline_chart(
            inference_results, model_name, dataset_name
        )
        viz_paths.append(performance_path)
        
        return viz_paths
    
    def _create_performance_dashboard(self, metrics: PerformanceMetrics,
                                    model_name: str, dataset_name: str) -> str:
        """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        
        # Plotly ì„œë¸Œí”Œë¡¯ìœ¼ë¡œ ëŒ€ì‹œë³´ë“œ êµ¬ì„±
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                'ì „ì²´ ì„±ëŠ¥ ì§€í‘œ',
                'ì‹ ë¢°ë„ ë¶„í¬', 
                'í´ë˜ìŠ¤ë³„ ê²€ì¶œ ìˆ˜',
                'ì²˜ë¦¬ ì„±ëŠ¥'
            ],
            specs=[
                [{"type": "indicator"}, {"type": "bar"}],
                [{"type": "pie"}, {"type": "scatter"}]
            ]
        )
        
        # 1. ì „ì²´ ì„±ëŠ¥ ì§€í‘œ (ê²Œì´ì§€)
        fig.add_trace(
            go.Indicator(
                mode="gauge+number+delta",
                value=metrics.map_50 * 100,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "mAP@0.5 (%)"},
                gauge={
                    'axis': {'range': [None, 100]},
                    'bar': {'color': self.colors['primary']},
                    'steps': [
                        {'range': [0, 50], 'color': "lightgray"},
                        {'range': [50, 80], 'color': "gray"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': 90
                    }
                }
            ),
            row=1, col=1
        )
        
        # 2. ì‹ ë¢°ë„ ë¶„í¬ (ë°” ì°¨íŠ¸)
        confidence_ranges = list(metrics.confidence_distribution.keys())
        confidence_counts = list(metrics.confidence_distribution.values())
        
        fig.add_trace(
            go.Bar(
                x=confidence_ranges,
                y=confidence_counts,
                name="ì‹ ë¢°ë„ ë¶„í¬",
                marker_color=self.colors['secondary']
            ),
            row=1, col=2
        )
        
        # 3. í´ë˜ìŠ¤ë³„ ê²€ì¶œ ìˆ˜ (íŒŒì´ ì°¨íŠ¸)
        if metrics.class_metrics:
            class_names = list(metrics.class_metrics.keys())
            class_counts = [metrics.class_metrics[name]['count'] for name in class_names]
            
            fig.add_trace(
                go.Pie(
                    labels=class_names,
                    values=class_counts,
                    name="í´ë˜ìŠ¤ ë¶„í¬"
                ),
                row=2, col=1
            )
        
        # 4. ì²˜ë¦¬ ì„±ëŠ¥ (ìŠ¤ìºí„°)
        performance_metrics = [
            metrics.avg_inference_time * 1000,  # msë¡œ ë³€í™˜
            metrics.throughput_fps,
            metrics.memory_usage_mb
        ]
        performance_labels = ['ì¶”ë¡  ì‹œê°„ (ms)', 'FPS', 'ë©”ëª¨ë¦¬ (MB)']
        
        fig.add_trace(
            go.Scatter(
                x=performance_labels,
                y=performance_metrics,
                mode='markers+lines',
                name="ì„±ëŠ¥ ì§€í‘œ",
                marker=dict(size=15, color=self.colors['accent'])
            ),
            row=2, col=2
        )
        
        # ë ˆì´ì•„ì›ƒ ì„¤ì •
        fig.update_layout(
            title=f"ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ: {model_name} - {dataset_name}",
            height=800,
            showlegend=False,
            template="plotly_white"
        )
        
        # ì €ì¥
        dashboard_path = self.output_dir / f"dashboard_{model_name}_{dataset_name}.html"
        fig.write_html(str(dashboard_path))
        
        return str(dashboard_path)
    
    def _create_confidence_distribution_chart(self, inference_results: List[Dict],
                                            model_name: str, dataset_name: str) -> str:
        """ì‹ ë¢°ë„ ë¶„í¬ ì°¨íŠ¸ ìƒì„±"""
        
        # ëª¨ë“  ê²€ì¶œì—ì„œ ì‹ ë¢°ë„ ì¶”ì¶œ
        confidences = []
        for result in inference_results:
            for detection in result.get('detections', []):
                confidences.append(detection.get('confidence', 0))
        
        if not confidences:
            return ""
        
        # Plotly íˆìŠ¤í† ê·¸ë¨
        fig = go.Figure()
        
        fig.add_trace(
            go.Histogram(
                x=confidences,
                nbinsx=50,
                name="ì‹ ë¢°ë„ ë¶„í¬",
                marker_color=self.colors['primary'],
                opacity=0.7
            )
        )
        
        # í†µê³„ì„  ì¶”ê°€
        mean_conf = np.mean(confidences)
        fig.add_vline(
            x=mean_conf,
            line_dash="dash",
            line_color="red",
            annotation_text=f"í‰ê· : {mean_conf:.3f}"
        )
        
        fig.update_layout(
            title=f"ì‹ ë¢°ë„ ë¶„í¬: {model_name} - {dataset_name}",
            xaxis_title="ì‹ ë¢°ë„",
            yaxis_title="ë¹ˆë„",
            template="plotly_white"
        )
        
        # ì €ì¥
        confidence_path = self.output_dir / f"confidence_dist_{model_name}_{dataset_name}.html"
        fig.write_html(str(confidence_path))
        
        return str(confidence_path)
    
    def _create_class_distribution_chart(self, metrics: PerformanceMetrics,
                                       model_name: str, dataset_name: str) -> str:
        """í´ë˜ìŠ¤ë³„ ê²€ì¶œ ì°¨íŠ¸ ìƒì„±"""
        
        if not metrics.class_metrics:
            return ""
        
        class_names = list(metrics.class_metrics.keys())
        class_counts = [metrics.class_metrics[name]['count'] for name in class_names]
        class_percentages = [metrics.class_metrics[name]['percentage'] for name in class_names]
        
        # Plotly ë°” ì°¨íŠ¸
        fig = go.Figure()
        
        fig.add_trace(
            go.Bar(
                x=class_names,
                y=class_counts,
                text=[f"{p:.1f}%" for p in class_percentages],
                textposition='auto',
                name="ê²€ì¶œ ìˆ˜",
                marker_color=self.colors['accent']
            )
        )
        
        fig.update_layout(
            title=f"í´ë˜ìŠ¤ë³„ ê²€ì¶œ ë¶„í¬: {model_name} - {dataset_name}",
            xaxis_title="í´ë˜ìŠ¤",
            yaxis_title="ê²€ì¶œ ìˆ˜",
            template="plotly_white"
        )
        
        # ì €ì¥
        class_path = self.output_dir / f"class_dist_{model_name}_{dataset_name}.html"
        fig.write_html(str(class_path))
        
        return str(class_path)
    
    def _create_performance_timeline_chart(self, inference_results: List[Dict],
                                         model_name: str, dataset_name: str) -> str:
        """ì„±ëŠ¥ ì‹œê³„ì—´ ì°¨íŠ¸ ìƒì„±"""
        
        # ì²˜ë¦¬ ì‹œê°„ ì¶”ì¶œ
        processing_times = []
        image_indices = []
        
        for i, result in enumerate(inference_results):
            if 'processing_time' in result:
                processing_times.append(result['processing_time'] * 1000)  # msë¡œ ë³€í™˜
                image_indices.append(i)
        
        if not processing_times:
            return ""
        
        # Plotly ë¼ì¸ ì°¨íŠ¸
        fig = go.Figure()
        
        fig.add_trace(
            go.Scatter(
                x=image_indices,
                y=processing_times,
                mode='lines+markers',
                name="ì²˜ë¦¬ ì‹œê°„",
                line=dict(color=self.colors['secondary'], width=2),
                marker=dict(size=6)
            )
        )
        
        # í‰ê· ì„  ì¶”ê°€
        avg_time = np.mean(processing_times)
        fig.add_hline(
            y=avg_time,
            line_dash="dash",
            line_color="red",
            annotation_text=f"í‰ê· : {avg_time:.1f}ms"
        )
        
        fig.update_layout(
            title=f"ì²˜ë¦¬ ì‹œê°„ ì¶”ì´: {model_name} - {dataset_name}",
            xaxis_title="ì´ë¯¸ì§€ ìˆœì„œ",
            yaxis_title="ì²˜ë¦¬ ì‹œê°„ (ms)",
            template="plotly_white"
        )
        
        # ì €ì¥
        timeline_path = self.output_dir / f"performance_timeline_{model_name}_{dataset_name}.html"
        fig.write_html(str(timeline_path))
        
        return str(timeline_path)
    
    def _create_comparison_visualizations(self, comparison_data: Dict[str, AnalysisReport],
                                        dataset_name: str) -> List[str]:
        """ëª¨ë¸ ë¹„êµ ì‹œê°í™” ìƒì„±"""
        
        viz_paths = []
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
        comparison_path = self._create_model_comparison_chart(comparison_data, dataset_name)
        viz_paths.append(comparison_path)
        
        return viz_paths
    
    def _create_model_comparison_chart(self, comparison_data: Dict[str, AnalysisReport],
                                     dataset_name: str) -> str:
        """ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        
        if len(comparison_data) < 2:
            return ""
        
        model_names = list(comparison_data.keys())
        
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ
        map_scores = [report.overall_metrics.map_50 * 100 for report in comparison_data.values()]
        inference_times = [report.overall_metrics.avg_inference_time * 1000 for report in comparison_data.values()]
        throughputs = [report.overall_metrics.throughput_fps for report in comparison_data.values()]
        
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig = make_subplots(
            rows=1, cols=3,
            subplot_titles=['mAP@0.5 (%)', 'ì¶”ë¡  ì‹œê°„ (ms)', 'ì²˜ë¦¬ëŸ‰ (FPS)']
        )
        
        # mAP ë¹„êµ
        fig.add_trace(
            go.Bar(
                x=model_names,
                y=map_scores,
                name="mAP@0.5",
                marker_color=self.colors['primary']
            ),
            row=1, col=1
        )
        
        # ì¶”ë¡  ì‹œê°„ ë¹„êµ
        fig.add_trace(
            go.Bar(
                x=model_names,
                y=inference_times,
                name="ì¶”ë¡  ì‹œê°„",
                marker_color=self.colors['secondary']
            ),
            row=1, col=2
        )
        
        # ì²˜ë¦¬ëŸ‰ ë¹„êµ
        fig.add_trace(
            go.Bar(
                x=model_names,
                y=throughputs,
                name="ì²˜ë¦¬ëŸ‰",
                marker_color=self.colors['accent']
            ),
            row=1, col=3
        )
        
        fig.update_layout(
            title=f"ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ: {dataset_name}",
            height=500,
            showlegend=False,
            template="plotly_white"
        )
        
        # ì €ì¥
        comparison_path = self.output_dir / f"model_comparison_{dataset_name}.html"
        fig.write_html(str(comparison_path))
        
        return str(comparison_path)
    
    def _generate_recommendations(self, metrics: PerformanceMetrics,
                                detailed_analysis: Dict[str, Any]) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì •í™•ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if metrics.map_50 < 0.8:
            recommendations.append(
                "ğŸ¯ ì •í™•ë„ ê°œì„  í•„ìš”: mAP@0.5ê°€ 80% ë¯¸ë§Œì…ë‹ˆë‹¤. "
                "ë°ì´í„° ì¦ê°•ì´ë‚˜ ëª¨ë¸ í¬ê¸° ì¦ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if metrics.avg_inference_time > 0.5:
            recommendations.append(
                "âš¡ ì¶”ë¡  ì†ë„ ê°œì„  í•„ìš”: í‰ê·  ì¶”ë¡  ì‹œê°„ì´ 500msë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. "
                "ëª¨ë¸ ê²½ëŸ‰í™”ë‚˜ í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if metrics.avg_confidence < 0.7:
            recommendations.append(
                "ğŸ” ì‹ ë¢°ë„ ê°œì„  í•„ìš”: í‰ê·  ì‹ ë¢°ë„ê°€ 70% ë¯¸ë§Œì…ë‹ˆë‹¤. "
                "í›ˆë ¨ ë°ì´í„° í’ˆì§ˆ ì ê²€ì´ë‚˜ ì„ê³„ê°’ ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # ê²€ì¶œ ë°€ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if detailed_analysis.get('image_analysis', {}).get('detection_coverage', 0) < 80:
            recommendations.append(
                "ğŸ“Š ê²€ì¶œ ì»¤ë²„ë¦¬ì§€ ê°œì„  í•„ìš”: ê²€ì¶œë˜ì§€ ì•Šì€ ì´ë¯¸ì§€ê°€ ë§ìŠµë‹ˆë‹¤. "
                "ì„ê³„ê°’ ë‚®ì¶”ê¸°ë‚˜ ë°ì´í„° ê· í˜• ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append(
                "âœ… ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤. "
                "ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ì •ê¸°ì ì¸ ì¬í›ˆë ¨ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
            )
        
        return recommendations
    
    def _generate_comparison_report(self, comparison_data: Dict[str, AnalysisReport],
                                  viz_paths: List[str], dataset_name: str) -> AnalysisReport:
        """ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_accuracy_model = max(
            comparison_data.keys(),
            key=lambda k: comparison_data[k].overall_metrics.map_50
        )
        
        best_speed_model = min(
            comparison_data.keys(),
            key=lambda k: comparison_data[k].overall_metrics.avg_inference_time
        )
        
        # ë¹„êµ ë¶„ì„
        comparison_analysis = {
            'model_count': len(comparison_data),
            'best_accuracy_model': best_accuracy_model,
            'best_speed_model': best_speed_model,
            'performance_summary': {}
        }
        
        for model_name, report in comparison_data.items():
            comparison_analysis['performance_summary'][model_name] = {
                'map_50': report.overall_metrics.map_50,
                'inference_time': report.overall_metrics.avg_inference_time,
                'throughput': report.overall_metrics.throughput_fps,
                'total_detections': report.overall_metrics.total_detections
            }
        
        # ë¹„êµ ê¶Œì¥ì‚¬í•­
        comparison_recommendations = [
            f"ğŸ† ìµœê³  ì •í™•ë„: {best_accuracy_model} "
            f"(mAP@0.5: {comparison_data[best_accuracy_model].overall_metrics.map_50:.3f})",
            f"âš¡ ìµœê³  ì†ë„: {best_speed_model} "
            f"(ì¶”ë¡  ì‹œê°„: {comparison_data[best_speed_model].overall_metrics.avg_inference_time*1000:.1f}ms)"
        ]
        
        # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
        comparison_report = AnalysisReport(
            dataset_name=dataset_name,
            model_name="Model_Comparison",
            analysis_timestamp=pd.Timestamp.now().isoformat(),
            detailed_analysis=comparison_analysis,
            visualization_paths=viz_paths,
            recommendations=comparison_recommendations
        )
        
        return comparison_report
    
    def _save_analysis_report(self, report: AnalysisReport):
        """ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥"""
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        report_path = self.output_dir / f"analysis_report_{report.model_name}_{report.dataset_name}.json"
        
        # ë¦¬í¬íŠ¸ ë°ì´í„° ì¤€ë¹„
        report_data = {
            'dataset_name': report.dataset_name,
            'model_name': report.model_name,
            'analysis_timestamp': report.analysis_timestamp,
            'overall_metrics': {
                'map_50': report.overall_metrics.map_50,
                'map_75': report.overall_metrics.map_75,
                'map_50_95': report.overall_metrics.map_50_95,
                'precision': report.overall_metrics.precision,
                'recall': report.overall_metrics.recall,
                'f1_score': report.overall_metrics.f1_score,
                'avg_inference_time': report.overall_metrics.avg_inference_time,
                'throughput_fps': report.overall_metrics.throughput_fps,
                'memory_usage_mb': report.overall_metrics.memory_usage_mb,
                'avg_confidence': report.overall_metrics.avg_confidence,
                'total_detections': report.overall_metrics.total_detections,
                'detection_density': report.overall_metrics.detection_density,
                'class_metrics': report.overall_metrics.class_metrics,
                'confidence_distribution': report.overall_metrics.confidence_distribution
            },
            'detailed_analysis': report.detailed_analysis,
            'visualization_paths': report.visualization_paths,
            'recommendations': report.recommendations
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_markdown_report(report)
        
        self.logger.info(f"ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")
    
    def _generate_markdown_report(self, report: AnalysisReport):
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        md_path = self.output_dir / f"report_{report.model_name}_{report.dataset_name}.md"
        
        markdown_content = f"""# ğŸ“Š ê²°ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸

**ëª¨ë¸**: {report.model_name}  
**ë°ì´í„°ì…‹**: {report.dataset_name}  
**ë¶„ì„ ì¼ì‹œ**: {report.analysis_timestamp}

## ğŸ¯ ì „ì²´ ì„±ëŠ¥ ìš”ì•½

| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| mAP@0.5 | {report.overall_metrics.map_50:.3f} |
| mAP@0.75 | {report.overall_metrics.map_75:.3f} |
| mAP@0.5:0.95 | {report.overall_metrics.map_50_95:.3f} |
| ì •ë°€ë„ | {report.overall_metrics.precision:.3f} |
| ì¬í˜„ìœ¨ | {report.overall_metrics.recall:.3f} |
| F1 ì ìˆ˜ | {report.overall_metrics.f1_score:.3f} |

## âš¡ ì„±ëŠ¥ ì§€í‘œ

| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| í‰ê·  ì¶”ë¡  ì‹œê°„ | {report.overall_metrics.avg_inference_time*1000:.1f} ms |
| ì²˜ë¦¬ëŸ‰ | {report.overall_metrics.throughput_fps:.2f} FPS |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | {report.overall_metrics.memory_usage_mb:.1f} MB |

## ğŸ” ê²€ì¶œ í†µê³„

| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| ì´ ê²€ì¶œ ìˆ˜ | {report.overall_metrics.total_detections:,} |
| í‰ê·  ì‹ ë¢°ë„ | {report.overall_metrics.avg_confidence:.3f} |
| ê²€ì¶œ ë°€ë„ | {report.overall_metrics.detection_density:.2f} ê²€ì¶œ/ì´ë¯¸ì§€ |

## ğŸ“ˆ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥

"""
        
        # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ í‘œ
        if report.overall_metrics.class_metrics:
            markdown_content += "| í´ë˜ìŠ¤ | ê²€ì¶œ ìˆ˜ | ë¹„ìœ¨ (%) |\n"
            markdown_content += "|--------|---------|----------|\n"
            
            for class_name, metrics in report.overall_metrics.class_metrics.items():
                markdown_content += f"| {class_name} | {metrics['count']:,} | {metrics['percentage']:.1f} |\n"
        
        # ê¶Œì¥ì‚¬í•­
        markdown_content += "\n## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n\n"
        for i, recommendation in enumerate(report.recommendations, 1):
            markdown_content += f"{i}. {recommendation}\n"
        
        # ì‹œê°í™” ë§í¬
        if report.visualization_paths:
            markdown_content += "\n## ğŸ“Š ì‹œê°í™” ìë£Œ\n\n"
            for viz_path in report.visualization_paths:
                viz_name = Path(viz_path).stem
                markdown_content += f"- [{viz_name}]({viz_path})\n"
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Nong-View ê²°ê³¼ ë¶„ì„ ì‹œìŠ¤í…œ")
    
    parser.add_argument(
        '--results-dir',
        type=str,
        required=True,
        help='ì¶”ë¡  ê²°ê³¼ ë””ë ‰í† ë¦¬'
    )
    
    parser.add_argument(
        '--ground-truth-dir',
        type=str,
        default=None,
        help='Ground Truth ë¼ë²¨ ë””ë ‰í† ë¦¬'
    )
    
    parser.add_argument(
        '--model-name',
        type=str,
        default="Unknown",
        help='ëª¨ë¸ ì´ë¦„'
    )
    
    parser.add_argument(
        '--dataset-name',
        type=str,
        default="Unknown",
        help='ë°ì´í„°ì…‹ ì´ë¦„'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default=None,
        help='ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬'
    )
    
    parser.add_argument(
        '--compare-models',
        nargs='+',
        default=None,
        help='ë¹„êµí•  ëª¨ë¸ë“¤ì˜ ê²°ê³¼ ë””ë ‰í† ë¦¬ë“¤ (model_name:results_dir í˜•ì‹)'
    )
    
    args = parser.parse_args()
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = ResultsAnalyzer(output_dir=args.output_dir)
    
    logger.info("=" * 60)
    logger.info("ğŸ“Š Nong-View ê²°ê³¼ ë¶„ì„ ì‹œì‘")
    logger.info("=" * 60)
    
    try:
        if args.compare_models:
            # ëª¨ë¸ ë¹„êµ ë¶„ì„
            model_results = {}
            for model_spec in args.compare_models:
                if ':' in model_spec:
                    model_name, results_dir = model_spec.split(':', 1)
                    model_results[model_name] = results_dir
                else:
                    logger.error(f"ì˜ëª»ëœ ëª¨ë¸ ì§€ì • í˜•ì‹: {model_spec}")
                    continue
            
            logger.info(f"ëª¨ë¸ ë¹„êµ ë¶„ì„: {list(model_results.keys())}")
            
            report = analyzer.compare_models(
                model_results,
                args.ground_truth_dir,
                args.dataset_name
            )
            
        else:
            # ë‹¨ì¼ ëª¨ë¸ ë¶„ì„
            logger.info(f"ë‹¨ì¼ ëª¨ë¸ ë¶„ì„: {args.model_name}")
            
            report = analyzer.analyze_inference_results(
                args.results_dir,
                args.ground_truth_dir,
                args.model_name,
                args.dataset_name
            )
        
        logger.info("=" * 60)
        logger.info("âœ… ê²°ê³¼ ë¶„ì„ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ ê²€ì¶œ ìˆ˜: {report.overall_metrics.total_detections:,}")
        logger.info(f"ğŸ¯ í‰ê·  ì‹ ë¢°ë„: {report.overall_metrics.avg_confidence:.3f}")
        logger.info(f"âš¡ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {report.overall_metrics.avg_inference_time*1000:.1f}ms")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise


if __name__ == "__main__":
    main()