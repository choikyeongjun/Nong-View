#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Nong-View Best Performance - ìµœì í™”ëœ ì¶”ë¡  ì‹œìŠ¤í…œ

ê¸°ì¡´ ì‹¤ì œ ìŠ¤í¬ë¦½íŠ¸ì˜ ê²€ì¦ëœ ê¸°ëŠ¥ í†µí•©:
- large_scale_crop_inference.py: ì™„ì „í•œ íŒŒì´í”„ë¼ì¸, ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
- inf.py: EdgeEnhancedInference, ê°€ì¥ìë¦¬ ê°œì„  ì•Œê³ ë¦¬ì¦˜
- upgrade_inf.py: ì§€ì˜¤ë©”íŠ¸ë¦¬ í¬ë¡­ ë°©ì‹

í˜ì‹ ì  ê°œì„  ì‚¬í•­:
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ìµœì í™” (FP16, í…ì„œ ìœµí•©)
- ì ì‘ì  ë°°ì¹˜ ì²˜ë¦¬
- ì§€ëŠ¥í˜• í›„ì²˜ë¦¬ (NMS ìµœì í™”)
- ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ì§€ì›

ë‹´ë‹¹: Claude Sonnet (Data Processing & Integration)
ê°œë°œ ë‚ ì§œ: 2025-10-28
"""

import os
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

import cv2
import numpy as np
import pandas as pd
from PIL import Image
import torch
import rasterio
from tqdm import tqdm
import geopandas as gpd
from shapely.geometry import box, Polygon
from ultralytics import YOLO
import psutil

# ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
import sys
sys.path.append(str(Path(__file__).parent.parent))
from configs.best_config import CONFIG, ModelType, logger

@dataclass
class InferenceResult:
    """ì¶”ë¡  ê²°ê³¼ í´ë˜ìŠ¤"""
    image_path: str
    detections: List[Dict] = field(default_factory=list)
    processing_time: float = 0.0
    preprocessing_time: float = 0.0
    inference_time: float = 0.0
    postprocessing_time: float = 0.0
    memory_usage: float = 0.0
    confidence_stats: Dict[str, float] = field(default_factory=dict)
    
@dataclass
class BatchInferenceStats:
    """ë°°ì¹˜ ì¶”ë¡  í†µê³„"""
    total_images: int = 0
    successful_images: int = 0
    failed_images: int = 0
    total_detections: int = 0
    average_confidence: float = 0.0
    total_processing_time: float = 0.0
    throughput_fps: float = 0.0
    peak_memory_mb: float = 0.0
    gpu_utilization: float = 0.0

class OptimizedInferenceEngine:
    """ìµœì í™”ëœ ì¶”ë¡  ì—”ì§„ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str = None, device: str = None, 
                 enable_half_precision: bool = True, batch_size: int = None):
        
        self.device = device or CONFIG.inference.device
        self.enable_half_precision = enable_half_precision and CONFIG.inference.half_precision
        self.batch_size = batch_size or CONFIG.inference.batch_size
        self.logger = logging.getLogger(__name__)
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self._load_model(model_path)
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self._optimize_model()
        
        # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.memory_monitor = MemoryMonitor()
        self.performance_tracker = PerformanceTracker()
        
        # í›„ì²˜ë¦¬ ì—”ì§„
        self.postprocessor = IntelligentPostprocessor()
        
        self.logger.info(f"ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ: {self.device}, ë°°ì¹˜í¬ê¸°: {self.batch_size}")
    
    def _load_model(self, model_path: str = None) -> YOLO:
        """ëª¨ë¸ ë¡œë“œ ë° ì´ˆê¸°í™”"""
        if model_path and Path(model_path).exists():
            self.logger.info(f"ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ë¡œë“œ: {model_path}")
            model = YOLO(model_path)
        else:
            # ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            default_model = ModelType.YOLO11S.value + ".pt"
            self.logger.info(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ: {default_model}")
            model = YOLO(default_model)
        
        # ëª¨ë¸ì„ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        model.to(self.device)
        
        return model
    
    def _optimize_model(self):
        """ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”"""
        # Half precision ì„¤ì •
        if self.enable_half_precision and self.device == "cuda":
            self.model.model.half()
            self.logger.info("FP16 ë°˜ì •ë°€ë„ í™œì„±í™”")
        
        # ëª¨ë¸ ì›Œë°ì—… (ì²« ì¶”ë¡  ìµœì í™”)
        self._warmup_model()
        
        # ì¶”ë¡  ì„¤ì • ìµœì í™”
        self.inference_kwargs = {
            'conf': CONFIG.inference.conf_threshold,
            'iou': CONFIG.inference.iou_threshold,
            'max_det': CONFIG.inference.max_det,
            'agnostic_nms': CONFIG.inference.agnostic_nms,
            'verbose': False,
            'device': self.device
        }
        
        if self.enable_half_precision:
            self.inference_kwargs['half'] = True
    
    def _warmup_model(self):
        """ëª¨ë¸ ì›Œë°ì—… (ì²« ì¶”ë¡  ì§€ì—° ìµœì†Œí™”)"""
        try:
            dummy_input = torch.zeros((1, 3, 640, 640))
            if self.device == "cuda":
                dummy_input = dummy_input.cuda()
            
            if self.enable_half_precision and self.device == "cuda":
                dummy_input = dummy_input.half()
            
            # ì›Œë°ì—… ì¶”ë¡  (3íšŒ)
            with torch.no_grad():
                for _ in range(3):
                    _ = self.model.predict(dummy_input, **self.inference_kwargs)
            
            self.logger.info("ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    def infer_single_image(self, image_path: str, 
                          enhance_edges: bool = True,
                          save_results: bool = True,
                          output_dir: str = None) -> InferenceResult:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ """
        start_time = time.time()
        
        result = InferenceResult(image_path=str(image_path))
        
        try:
            # 1. ì „ì²˜ë¦¬
            preprocess_start = time.time()
            image, original_shape = self._preprocess_image(image_path)
            result.preprocessing_time = time.time() - preprocess_start
            
            # 2. ì¶”ë¡ 
            inference_start = time.time()
            predictions = self.model.predict(
                image, 
                imgsz=CONFIG.inference.image_size,
                **self.inference_kwargs
            )
            result.inference_time = time.time() - inference_start
            
            # 3. í›„ì²˜ë¦¬
            postprocess_start = time.time()
            detections = self._postprocess_predictions(
                predictions, original_shape, enhance_edges
            )
            result.postprocessing_time = time.time() - postprocess_start
            
            # 4. ê²°ê³¼ ì €ì¥
            result.detections = detections
            result.processing_time = time.time() - start_time
            result.memory_usage = self.memory_monitor.get_current_usage()
            result.confidence_stats = self._calculate_confidence_stats(detections)
            
            # 5. ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
            if save_results and output_dir:
                self._save_inference_result(result, output_dir)
            
            self.logger.debug(f"ì¶”ë¡  ì™„ë£Œ: {Path(image_path).name} ({result.processing_time:.3f}ì´ˆ)")
            
        except Exception as e:
            self.logger.error(f"ì¶”ë¡  ì‹¤íŒ¨ {image_path}: {e}")
            result.processing_time = time.time() - start_time
        
        return result
    
    def infer_batch_images(self, image_paths: List[str],
                          output_dir: str = None,
                          max_workers: int = None,
                          enhance_edges: bool = True) -> Tuple[List[InferenceResult], BatchInferenceStats]:
        """ë°°ì¹˜ ì´ë¯¸ì§€ ì¶”ë¡ """
        
        if not image_paths:
            return [], BatchInferenceStats()
        
        self.logger.info(f"ë°°ì¹˜ ì¶”ë¡  ì‹œì‘: {len(image_paths)}ê°œ ì´ë¯¸ì§€")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
        
        # ì›Œì»¤ ìˆ˜ ìë™ ê²°ì •
        if max_workers is None:
            max_workers = min(4, len(image_paths), CONFIG.hardware.cpu_workers // 2)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
        start_time = time.time()
        results = []
        stats = BatchInferenceStats()
        
        # ì ì‘ì  ë°°ì¹˜ í¬ê¸° ê²°ì •
        adaptive_batch_size = self._calculate_adaptive_batch_size(image_paths[:5])
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            batches = [image_paths[i:i + adaptive_batch_size] 
                      for i in range(0, len(image_paths), adaptive_batch_size)]
            
            futures = []
            for batch in batches:
                future = executor.submit(
                    self._process_batch,
                    batch, output_dir, enhance_edges
                )
                futures.append(future)
            
            # ê²°ê³¼ ìˆ˜ì§‘ (ì§„í–‰ìƒí™© í‘œì‹œ)
            for future in tqdm(as_completed(futures), total=len(futures), desc="ë°°ì¹˜ ì²˜ë¦¬"):
                try:
                    batch_results = future.result()
                    results.extend(batch_results)
                except Exception as e:
                    self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # í†µê³„ ê³„ì‚°
        total_time = time.time() - start_time
        stats = self._calculate_batch_stats(results, total_time)
        
        self.logger.info(f"ë°°ì¹˜ ì¶”ë¡  ì™„ë£Œ: {stats.successful_images}/{stats.total_images} ì„±ê³µ")
        self.logger.info(f"ì²˜ë¦¬ ì†ë„: {stats.throughput_fps:.2f} FPS")
        
        return results, stats
    
    def _preprocess_image(self, image_path: str) -> Tuple[np.ndarray, Tuple[int, int]]:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        image_path = Path(image_path)
        
        # ì´ë¯¸ì§€ ë¡œë“œ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        if image_path.suffix.lower() in ['.tif', '.tiff']:
            # GeoTIFF ì²˜ë¦¬
            with rasterio.open(image_path) as src:
                image = src.read()
                if len(image.shape) == 3:
                    image = np.transpose(image, (1, 2, 0))
                if image.shape[-1] > 3:
                    image = image[:, :, :3]  # RGBë§Œ ì‚¬ìš©
                # ì •ê·œí™”
                if image.max() > 255:
                    image = (image / image.max() * 255).astype(np.uint8)
        else:
            # ì¼ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬
            image = cv2.imread(str(image_path))
            if image is None:
                raise ValueError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        original_shape = image.shape[:2]  # (height, width)
        
        return image, original_shape
    
    def _postprocess_predictions(self, predictions, original_shape: Tuple[int, int], 
                               enhance_edges: bool = True) -> List[Dict]:
        """ì˜ˆì¸¡ ê²°ê³¼ í›„ì²˜ë¦¬"""
        detections = []
        
        if not predictions or len(predictions) == 0:
            return detections
        
        pred = predictions[0]  # ì²« ë²ˆì§¸ ê²°ê³¼ (ë‹¨ì¼ ì´ë¯¸ì§€)
        
        if pred.boxes is None or len(pred.boxes) == 0:
            return detections
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ë° ì •ë³´ ì¶”ì¶œ
        boxes = pred.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2
        confidences = pred.boxes.conf.cpu().numpy()
        class_ids = pred.boxes.cls.cpu().numpy().astype(int)
        
        # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
        class_names = pred.names
        
        for i, (box, conf, cls_id) in enumerate(zip(boxes, confidences, class_ids)):
            x1, y1, x2, y2 = box
            
            detection = {
                'bbox': [float(x1), float(y1), float(x2), float(y2)],
                'confidence': float(conf),
                'class_id': int(cls_id),
                'class_name': class_names[cls_id] if cls_id in class_names else f'class_{cls_id}',
                'area': float((x2 - x1) * (y2 - y1))
            }
            
            # ê°€ì¥ìë¦¬ ê°œì„  (ì˜µì…˜)
            if enhance_edges:
                detection = self.postprocessor.enhance_detection_edges(
                    detection, original_shape
                )
            
            detections.append(detection)
        
        # ì¤‘ë³µ ì œê±° ë° ìµœì í™”
        if enhance_edges:
            detections = self.postprocessor.optimize_detections(detections)
        
        return detections
    
    def _process_batch(self, image_paths: List[str], output_dir: str = None, 
                      enhance_edges: bool = True) -> List[InferenceResult]:
        """ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬"""
        results = []
        
        for image_path in image_paths:
            try:
                result = self.infer_single_image(
                    image_path, enhance_edges, 
                    save_results=bool(output_dir), 
                    output_dir=output_dir
                )
                results.append(result)
                
            except Exception as e:
                self.logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ {image_path}: {e}")
                # ì‹¤íŒ¨í•œ ê²°ê³¼ë„ ê¸°ë¡
                failed_result = InferenceResult(image_path=str(image_path))
                results.append(failed_result)
        
        return results
    
    def _calculate_adaptive_batch_size(self, sample_images: List[str]) -> int:
        """ì ì‘ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if not sample_images:
            return self.batch_size
        
        # ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        initial_memory = self.memory_monitor.get_current_usage()
        
        try:
            # ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            sample_result = self.infer_single_image(sample_images[0], save_results=False)
            memory_per_image = self.memory_monitor.get_current_usage() - initial_memory
            
            # ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            available_memory = self.memory_monitor.get_available_memory()
            safe_batch_size = max(1, int(available_memory * 0.7 / memory_per_image))
            
            # ì„¤ì •ëœ ìµœëŒ€ê°’ìœ¼ë¡œ ì œí•œ
            adaptive_batch_size = min(safe_batch_size, self.batch_size, len(sample_images))
            
            self.logger.info(f"ì ì‘ì  ë°°ì¹˜ í¬ê¸°: {adaptive_batch_size} (ë©”ëª¨ë¦¬ ê¸°ë°˜)")
            
            return adaptive_batch_size
            
        except Exception as e:
            self.logger.warning(f"ì ì‘ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return self.batch_size
    
    def _calculate_confidence_stats(self, detections: List[Dict]) -> Dict[str, float]:
        """ì‹ ë¢°ë„ í†µê³„ ê³„ì‚°"""
        if not detections:
            return {}
        
        confidences = [det['confidence'] for det in detections]
        
        return {
            'mean': float(np.mean(confidences)),
            'std': float(np.std(confidences)),
            'min': float(np.min(confidences)),
            'max': float(np.max(confidences)),
            'median': float(np.median(confidences))
        }
    
    def _calculate_batch_stats(self, results: List[InferenceResult], 
                             total_time: float) -> BatchInferenceStats:
        """ë°°ì¹˜ í†µê³„ ê³„ì‚°"""
        stats = BatchInferenceStats()
        
        stats.total_images = len(results)
        stats.successful_images = len([r for r in results if r.detections])
        stats.failed_images = stats.total_images - stats.successful_images
        stats.total_detections = sum(len(r.detections) for r in results)
        stats.total_processing_time = total_time
        stats.throughput_fps = stats.total_images / total_time if total_time > 0 else 0
        stats.peak_memory_mb = self.memory_monitor.get_peak_usage()
        
        # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
        all_confidences = []
        for result in results:
            for det in result.detections:
                all_confidences.append(det['confidence'])
        
        if all_confidences:
            stats.average_confidence = float(np.mean(all_confidences))
        
        return stats
    
    def _save_inference_result(self, result: InferenceResult, output_dir: str):
        """ì¶”ë¡  ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_dir)
        image_name = Path(result.image_path).stem
        
        # JSON ê²°ê³¼ ì €ì¥
        json_path = output_path / f"{image_name}_result.json"
        result_data = {
            'image_path': result.image_path,
            'detections': result.detections,
            'processing_time': result.processing_time,
            'confidence_stats': result.confidence_stats,
            'metadata': {
                'model_type': type(self.model).__name__,
                'device': self.device,
                'timestamp': pd.Timestamp.now().isoformat()
            }
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        # ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ (ì˜µì…˜)
        if CONFIG.inference.visualize:
            self._save_visualization(result, output_path)
    
    def _save_visualization(self, result: InferenceResult, output_path: Path):
        """ê²°ê³¼ ì‹œê°í™” ì €ì¥"""
        try:
            image = cv2.imread(result.image_path)
            if image is None:
                return
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            for detection in result.detections:
                x1, y1, x2, y2 = [int(x) for x in detection['bbox']]
                confidence = detection['confidence']
                class_name = detection['class_name']
                
                # ë°”ìš´ë”© ë°•ìŠ¤
                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ë¼ë²¨
                label = f"{class_name}: {confidence:.2f}"
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                cv2.rectangle(image, (x1, y1 - label_size[1] - 10), 
                            (x1 + label_size[0], y1), (0, 255, 0), -1)
                cv2.putText(image, label, (x1, y1 - 5), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
            
            # ì €ì¥
            image_name = Path(result.image_path).stem
            vis_path = output_path / f"{image_name}_visualization.jpg"
            cv2.imwrite(str(vis_path), image)
            
        except Exception as e:
            self.logger.error(f"ì‹œê°í™” ì €ì¥ ì‹¤íŒ¨: {e}")


class IntelligentPostprocessor:
    """ì§€ëŠ¥í˜• í›„ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def enhance_detection_edges(self, detection: Dict, original_shape: Tuple[int, int]) -> Dict:
        """ê²€ì¶œ ê°€ì¥ìë¦¬ ê°œì„  (inf.py ì•Œê³ ë¦¬ì¦˜ ì ìš©)"""
        # ê°€ì¥ìë¦¬ í™•ì¥ ë¡œì§ (ê¸°ì¡´ inf.pyì˜ expand_edge_masks ë¡œì§)
        x1, y1, x2, y2 = detection['bbox']
        confidence = detection['confidence']
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ í™•ì¥ ê³„ìˆ˜
        expansion_factor = max(0.02, min(0.1, (1.0 - confidence) * 0.05))
        
        width = x2 - x1
        height = y2 - y1
        
        # í™•ì¥ ì ìš©
        expand_x = width * expansion_factor
        expand_y = height * expansion_factor
        
        new_x1 = max(0, x1 - expand_x)
        new_y1 = max(0, y1 - expand_y)
        new_x2 = min(original_shape[1], x2 + expand_x)  # width
        new_y2 = min(original_shape[0], y2 + expand_y)  # height
        
        detection['bbox'] = [new_x1, new_y1, new_x2, new_y2]
        detection['enhanced'] = True
        
        return detection
    
    def optimize_detections(self, detections: List[Dict]) -> List[Dict]:
        """ê²€ì¶œ ê²°ê³¼ ìµœì í™” (ì¤‘ë³µ ì œê±°, ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§)"""
        if not detections:
            return detections
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ì •ë ¬
        detections.sort(key=lambda x: x['confidence'], reverse=True)
        
        # ê°œì„ ëœ NMS (IoU + ë©´ì  ê³ ë ¤)
        optimized = []
        for detection in detections:
            is_redundant = False
            
            for existing in optimized:
                iou = self._calculate_iou(detection['bbox'], existing['bbox'])
                
                # IoU ì„ê³„ê°’ + í´ë˜ìŠ¤ ì¼ì¹˜ í™•ì¸
                if (iou > 0.3 and 
                    detection['class_id'] == existing['class_id']):
                    is_redundant = True
                    break
            
            if not is_redundant:
                optimized.append(detection)
        
        return optimized
    
    def _calculate_iou(self, box1: List[float], box2: List[float]) -> float:
        """IoU ê³„ì‚°"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # êµì§‘í•© ì˜ì—­
        x1_inter = max(x1_1, x1_2)
        y1_inter = max(y1_1, y1_2)
        x2_inter = min(x2_1, x2_2)
        y2_inter = min(y2_1, y2_2)
        
        if x2_inter <= x1_inter or y2_inter <= y1_inter:
            return 0.0
        
        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
        
        # í•©ì§‘í•© ì˜ì—­
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = area1 + area2 - inter_area
        
        return inter_area / union_area if union_area > 0 else 0.0


class MemoryMonitor:
    """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.peak_usage = 0.0
    
    def get_current_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        process = psutil.Process()
        usage_mb = process.memory_info().rss / 1024 / 1024
        self.peak_usage = max(self.peak_usage, usage_mb)
        return usage_mb
    
    def get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (MB)"""
        memory = psutil.virtual_memory()
        return memory.available / 1024 / 1024
    
    def get_peak_usage(self) -> float:
        """ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        return self.peak_usage


class PerformanceTracker:
    """ì„±ëŠ¥ ì¶”ì  í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.metrics = {}
    
    def track_inference_time(self, func):
        """ì¶”ë¡  ì‹œê°„ ì¶”ì  ë°ì½”ë ˆì´í„°"""
        def wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            elapsed_time = time.time() - start_time
            
            func_name = func.__name__
            if func_name not in self.metrics:
                self.metrics[func_name] = []
            self.metrics[func_name].append(elapsed_time)
            
            return result
        return wrapper
    
    def get_performance_summary(self) -> Dict[str, Dict[str, float]]:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        summary = {}
        for func_name, times in self.metrics.items():
            summary[func_name] = {
                'mean': np.mean(times),
                'std': np.std(times),
                'min': np.min(times),
                'max': np.max(times),
                'count': len(times)
            }
        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Nong-View ìµœì í™”ëœ ì¶”ë¡  ì‹œìŠ¤í…œ")
    
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ ë˜ëŠ” ë””ë ‰í† ë¦¬'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        default=None,
        help='YOLO ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸ê°’: YOLOv11s)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=None,
        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´)'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        choices=['cpu', 'cuda'],
        default=None,
        help='ì¶”ë¡  ë””ë°”ì´ìŠ¤'
    )
    
    parser.add_argument(
        '--disable-edge-enhancement',
        action='store_true',
        help='ê°€ì¥ìë¦¬ ê°œì„  ë¹„í™œì„±í™”'
    )
    
    parser.add_argument(
        '--workers',
        type=int,
        default=None,
        help='ë³‘ë ¬ ì›Œì»¤ ìˆ˜'
    )
    
    args = parser.parse_args()
    
    # ì…ë ¥ ê²½ë¡œ í™•ì¸
    input_path = Path(args.input)
    if not input_path.exists():
        logger.error(f"ì…ë ¥ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path}")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = args.output or str(Path("results") / "inference_output")
    
    # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
    engine = OptimizedInferenceEngine(
        model_path=args.model,
        device=args.device,
        batch_size=args.batch_size
    )
    
    logger.info("=" * 60)
    logger.info("ğŸš€ Nong-View ìµœì í™”ëœ ì¶”ë¡  ì‹œì‘")
    logger.info("=" * 60)
    
    try:
        if input_path.is_file():
            # ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ 
            logger.info(f"ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ : {input_path}")
            result = engine.infer_single_image(
                str(input_path),
                enhance_edges=not args.disable_edge_enhancement,
                save_results=True,
                output_dir=output_dir
            )
            
            logger.info(f"ì¶”ë¡  ì™„ë£Œ: {len(result.detections)}ê°œ ê²€ì¶œ")
            logger.info(f"ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
            
        else:
            # ë°°ì¹˜ ì´ë¯¸ì§€ ì¶”ë¡ 
            image_extensions = {'.jpg', '.jpeg', '.png', '.tif', '.tiff'}
            image_paths = []
            
            for ext in image_extensions:
                image_paths.extend(input_path.glob(f"*{ext}"))
                image_paths.extend(input_path.glob(f"*{ext.upper()}"))
            
            logger.info(f"ë°°ì¹˜ ì¶”ë¡ : {len(image_paths)}ê°œ ì´ë¯¸ì§€")
            
            results, stats = engine.infer_batch_images(
                [str(p) for p in image_paths],
                output_dir=output_dir,
                max_workers=args.workers,
                enhance_edges=not args.disable_edge_enhancement
            )
            
            logger.info(f"ë°°ì¹˜ ì¶”ë¡  ì™„ë£Œ:")
            logger.info(f"  - ì„±ê³µ: {stats.successful_images}/{stats.total_images}")
            logger.info(f"  - ì´ ê²€ì¶œ: {stats.total_detections}ê°œ")
            logger.info(f"  - ì²˜ë¦¬ ì†ë„: {stats.throughput_fps:.2f} FPS")
            logger.info(f"  - í‰ê·  ì‹ ë¢°ë„: {stats.average_confidence:.3f}")
        
        logger.info("=" * 60)
        logger.info("âœ… ì¶”ë¡  ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
        raise


if __name__ == "__main__":
    main()