#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Nong-View Best Performance - Data Utilities
ë°ì´í„° ì²˜ë¦¬ ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

ë‹´ë‹¹: Claude Sonnet (Data Processing & Integration)
ê°œë°œ ë‚ ì§œ: 2025-10-28
"""

import os
import json
import yaml
import cv2
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from collections import Counter, defaultdict
import logging
from dataclasses import dataclass, asdict
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import albumentations as A
from albumentations.pytorch import ToTensorV2
import rasterio
from PIL import Image
import shutil
import random

# ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
import sys
sys.path.append(str(Path(__file__).parent.parent))
from configs.best_config import CONFIG, DatasetType, logger

@dataclass
class ImageInfo:
    """ì´ë¯¸ì§€ ì •ë³´ í´ë˜ìŠ¤"""
    filename: str
    filepath: str
    width: int
    height: int
    channels: int
    file_size: int
    format: str
    has_labels: bool = False
    label_count: int = 0
    class_distribution: Dict[str, int] = None
    quality_score: float = 0.0
    
    def __post_init__(self):
        if self.class_distribution is None:
            self.class_distribution = {}

@dataclass  
class DatasetStats:
    """ë°ì´í„°ì…‹ í†µê³„ í´ë˜ìŠ¤"""
    total_images: int
    total_labels: int
    class_distribution: Dict[str, int]
    image_size_distribution: Dict[str, int]
    split_distribution: Dict[str, int]
    quality_metrics: Dict[str, float]
    
class DataQualityAnalyzer:
    """ë°ì´í„° í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_image_quality(self, image_path: str) -> float:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-1)"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            if str(image_path).lower().endswith(('.tif', '.tiff')):
                with rasterio.open(image_path) as src:
                    image = src.read()
                    if len(image.shape) == 3:
                        image = np.transpose(image, (1, 2, 0))
                    if image.shape[-1] > 3:
                        image = image[:, :, :3]
            else:
                image = cv2.imread(str(image_path))
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            scores = []
            
            # 1. ëª…ë„ ë¶„í¬ ì ìˆ˜ (0.3 ê°€ì¤‘ì¹˜)
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
            hist_norm = hist.flatten() / hist.sum()
            
            # íˆìŠ¤í† ê·¸ë¨ ì—”íŠ¸ë¡œí”¼ (ë‹¤ì–‘ì„±)
            entropy = -np.sum(hist_norm * np.log2(hist_norm + 1e-8))
            brightness_score = min(entropy / 8.0, 1.0)  # 8bit ê¸°ì¤€ ì •ê·œí™”
            scores.append(('brightness', brightness_score, 0.3))
            
            # 2. ëŒ€ë¹„ ì ìˆ˜ (0.25 ê°€ì¤‘ì¹˜)  
            contrast = gray.std()
            contrast_score = min(contrast / 64.0, 1.0)  # ê²½í—˜ì  ìµœëŒ€ê°’
            scores.append(('contrast', contrast_score, 0.25))
            
            # 3. ì„ ëª…ë„ ì ìˆ˜ (0.25 ê°€ì¤‘ì¹˜)
            laplacian = cv2.Laplacian(gray, cv2.CV_64F)
            sharpness = laplacian.var()
            sharpness_score = min(sharpness / 1000.0, 1.0)  # ê²½í—˜ì  ìµœëŒ€ê°’
            scores.append(('sharpness', sharpness_score, 0.25))
            
            # 4. ë…¸ì´ì¦ˆ ì ìˆ˜ (0.2 ê°€ì¤‘ì¹˜)
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ í›„ ì°¨ì´ë¡œ ë…¸ì´ì¦ˆ ì¶”ì •
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
            noise = np.mean(np.abs(gray.astype(float) - blurred.astype(float)))
            noise_score = max(0, 1.0 - noise / 25.0)  # ë‚®ì€ ë…¸ì´ì¦ˆê°€ ì¢‹ìŒ
            scores.append(('noise', noise_score, 0.2))
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            total_score = sum(score * weight for _, score, weight in scores)
            
            return total_score
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨ {image_path}: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def detect_outliers(self, image_stats: List[ImageInfo], 
                       method: str = "isolation_forest") -> List[bool]:
        """ì´ìƒì¹˜ íƒì§€"""
        try:
            from sklearn.ensemble import IsolationForest
            from sklearn.preprocessing import StandardScaler
            
            # íŠ¹ì„± ë²¡í„° ìƒì„±
            features = []
            for stat in image_stats:
                feature = [
                    stat.width,
                    stat.height, 
                    stat.channels,
                    stat.file_size,
                    stat.label_count,
                    stat.quality_score
                ]
                features.append(feature)
            
            features = np.array(features)
            
            # í‘œì¤€í™”
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)
            
            # ì´ìƒì¹˜ íƒì§€
            if method == "isolation_forest":
                detector = IsolationForest(contamination=0.1, random_state=CONFIG.data.random_seed)
                outliers = detector.fit_predict(features_scaled) == -1
            else:
                # ë‹¨ìˆœ í†µê³„ì  ë°©ë²•
                z_scores = np.abs(stats.zscore(features_scaled, axis=0))
                outliers = np.any(z_scores > 3, axis=1)
            
            return outliers.tolist()
            
        except Exception as e:
            self.logger.error(f"ì´ìƒì¹˜ íƒì§€ ì‹¤íŒ¨: {e}")
            return [False] * len(image_stats)

class DatasetProcessor:
    """ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ë¶„í•  í´ë˜ìŠ¤"""
    
    def __init__(self, dataset_type: DatasetType):
        self.dataset_type = dataset_type
        self.dataset_config = CONFIG.get_dataset_config(dataset_type)
        self.dataset_path = Path(CONFIG.data.namwon_data_path) / self.dataset_config["path"]
        self.logger = logging.getLogger(__name__)
        self.quality_analyzer = DataQualityAnalyzer()
    
    def scan_dataset(self) -> Tuple[List[ImageInfo], DatasetStats]:
        """ë°ì´í„°ì…‹ ì „ì²´ ìŠ¤ìº” ë° ë¶„ì„"""
        self.logger.info(f"ë°ì´í„°ì…‹ ìŠ¤ìº” ì‹œì‘: {self.dataset_type.value}")
        
        image_infos = []
        class_counter = Counter()
        size_counter = Counter()
        total_labels = 0
        
        # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ íƒìƒ‰
        for split in ['train', 'val', 'test']:
            images_dir = self.dataset_path / 'images' / split
            labels_dir = self.dataset_path / 'labels' / split
            
            if not images_dir.exists():
                self.logger.warning(f"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì—†ìŒ: {images_dir}")
                continue
            
            # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬
            for img_file in images_dir.glob('*'):
                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tif', '.tiff']:
                    try:
                        # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
                        info = self._extract_image_info(img_file, labels_dir, split)
                        image_infos.append(info)
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        total_labels += info.label_count
                        for class_name, count in info.class_distribution.items():
                            class_counter[class_name] += count
                        
                        size_key = f"{info.width}x{info.height}"
                        size_counter[size_key] += 1
                        
                    except Exception as e:
                        self.logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ {img_file}: {e}")
        
        # í’ˆì§ˆ ë¶„ì„
        quality_scores = [info.quality_score for info in image_infos]
        quality_metrics = {
            'mean_quality': np.mean(quality_scores),
            'std_quality': np.std(quality_scores),
            'min_quality': np.min(quality_scores),
            'max_quality': np.max(quality_scores)
        }
        
        # ë¶„í•  í†µê³„
        split_counter = Counter()
        for info in image_infos:
            # íŒŒì¼ ê²½ë¡œì—ì„œ ë¶„í•  ì¶”ì¶œ
            if '/train/' in info.filepath:
                split_counter['train'] += 1
            elif '/val/' in info.filepath:
                split_counter['val'] += 1
            elif '/test/' in info.filepath:
                split_counter['test'] += 1
        
        # ë°ì´í„°ì…‹ í†µê³„ ìƒì„±
        stats = DatasetStats(
            total_images=len(image_infos),
            total_labels=total_labels,
            class_distribution=dict(class_counter),
            image_size_distribution=dict(size_counter),
            split_distribution=dict(split_counter),
            quality_metrics=quality_metrics
        )
        
        self.logger.info(f"ìŠ¤ìº” ì™„ë£Œ: {len(image_infos)}ê°œ ì´ë¯¸ì§€, {total_labels}ê°œ ë¼ë²¨")
        return image_infos, stats
    
    def _extract_image_info(self, img_path: Path, labels_dir: Path, split: str) -> ImageInfo:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ"""
        # ê¸°ë³¸ ì´ë¯¸ì§€ ì •ë³´
        file_size = img_path.stat().st_size
        
        # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´
        if img_path.suffix.lower() in ['.tif', '.tiff']:
            # GeoTIFF ì²˜ë¦¬
            with rasterio.open(img_path) as src:
                width, height = src.width, src.height
                channels = src.count
                img_format = 'TIFF'
        else:
            # ì¼ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬
            with Image.open(img_path) as img:
                width, height = img.size
                channels = len(img.getbands())
                img_format = img.format
        
        # ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
        label_path = labels_dir / f"{img_path.stem}.txt"
        has_labels = label_path.exists()
        label_count = 0
        class_distribution = {}
        
        if has_labels:
            try:
                with open(label_path, 'r') as f:
                    lines = f.readlines()
                    label_count = len(lines)
                    
                    # í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°
                    class_ids = []
                    for line in lines:
                        parts = line.strip().split()
                        if parts:
                            class_ids.append(int(parts[0]))
                    
                    # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
                    classes = self.dataset_config["classes"]
                    for class_id in class_ids:
                        if class_id < len(classes):
                            class_name = classes[class_id]
                            class_distribution[class_name] = class_distribution.get(class_name, 0) + 1
                            
            except Exception as e:
                self.logger.error(f"ë¼ë²¨ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {label_path}: {e}")
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self.quality_analyzer.analyze_image_quality(img_path)
        
        return ImageInfo(
            filename=img_path.name,
            filepath=str(img_path),
            width=width,
            height=height,
            channels=channels,
            file_size=file_size,
            format=img_format,
            has_labels=has_labels,
            label_count=label_count,
            class_distribution=class_distribution,
            quality_score=quality_score
        )
    
    def create_balanced_split(self, image_infos: List[ImageInfo], 
                            train_ratio: float = 0.8,
                            val_ratio: float = 0.1,
                            test_ratio: float = 0.1) -> Dict[str, List[ImageInfo]]:
        """ê· í˜•ì¡íŒ ë°ì´í„° ë¶„í•  ìƒì„±"""
        self.logger.info("ê· í˜•ì¡íŒ ë°ì´í„° ë¶„í•  ìƒì„± ì¤‘...")
        
        # ë¶„í•  ë¹„ìœ¨ ê²€ì¦
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "ë¶„í•  ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤"
        
        # ê³„ì¸µí™”ë¥¼ ìœ„í•œ ë ˆì´ë¸” ìƒì„±
        stratify_labels = []
        for info in image_infos:
            if info.class_distribution:
                # ì£¼ìš” í´ë˜ìŠ¤ (ê°€ì¥ ë§ì€ ê°ì²´ë¥¼ ê°€ì§„ í´ë˜ìŠ¤)
                dominant_class = max(info.class_distribution, key=info.class_distribution.get)
                stratify_labels.append(dominant_class)
            else:
                stratify_labels.append('no_label')
        
        try:
            # 1ì°¨ ë¶„í• : train vs (val + test)
            train_indices, temp_indices = train_test_split(
                range(len(image_infos)),
                test_size=(val_ratio + test_ratio),
                stratify=stratify_labels,
                random_state=CONFIG.data.random_seed
            )
            
            # 2ì°¨ ë¶„í• : val vs test
            temp_labels = [stratify_labels[i] for i in temp_indices]
            val_size = val_ratio / (val_ratio + test_ratio)
            
            val_indices, test_indices = train_test_split(
                temp_indices,
                test_size=(1 - val_size),
                stratify=temp_labels,
                random_state=CONFIG.data.random_seed
            )
            
        except ValueError as e:
            self.logger.warning(f"ê³„ì¸µí™” ë¶„í•  ì‹¤íŒ¨, ëœë¤ ë¶„í•  ì‚¬ìš©: {e}")
            # ê³„ì¸µí™” ë¶ˆê°€ëŠ¥ ì‹œ ëœë¤ ë¶„í• 
            indices = list(range(len(image_infos)))
            random.Random(CONFIG.data.random_seed).shuffle(indices)
            
            n_train = int(len(indices) * train_ratio)
            n_val = int(len(indices) * val_ratio)
            
            train_indices = indices[:n_train]
            val_indices = indices[n_train:n_train + n_val]
            test_indices = indices[n_train + n_val:]
        
        # ê²°ê³¼ êµ¬ì„±
        splits = {
            'train': [image_infos[i] for i in train_indices],
            'val': [image_infos[i] for i in val_indices],
            'test': [image_infos[i] for i in test_indices]
        }
        
        # ë¶„í•  ê²°ê³¼ ë¡œê·¸
        for split_name, split_data in splits.items():
            self.logger.info(f"{split_name}: {len(split_data)}ê°œ ì´ë¯¸ì§€ "
                           f"({len(split_data)/len(image_infos)*100:.1f}%)")
        
        return splits

class DataAugmentation:
    """ë°ì´í„° ì¦ê°• í´ë˜ìŠ¤"""
    
    def __init__(self, dataset_type: DatasetType):
        self.dataset_type = dataset_type
        self.logger = logging.getLogger(__name__)
    
    def get_training_transforms(self, image_size: int = 640) -> A.Compose:
        """í›ˆë ¨ìš© ë°ì´í„° ì¦ê°• ë³€í™˜"""
        
        # ê¸°ë³¸ ì¦ê°• ì„¤ì •
        aug_config = CONFIG.training.augmentation
        
        transforms = [
            # í¬ê¸° ì¡°ì •
            A.LongestMaxSize(max_size=image_size),
            A.PadIfNeeded(
                min_height=image_size, 
                min_width=image_size,
                border_mode=cv2.BORDER_CONSTANT,
                value=0
            ),
            
            # ìƒ‰ìƒ ë³€í™˜
            A.HueSaturationValue(
                hue_shift_limit=int(aug_config["hsv_h"] * 180),
                sat_shift_limit=int(aug_config["hsv_s"] * 255),
                val_shift_limit=int(aug_config["hsv_v"] * 255),
                p=0.7
            ),
            
            # ê¸°í•˜í•™ì  ë³€í™˜
            A.ShiftScaleRotate(
                shift_limit=aug_config["translate"],
                scale_limit=aug_config["scale"],
                rotate_limit=aug_config["degrees"],
                border_mode=cv2.BORDER_CONSTANT,
                p=0.8
            ),
            
            # í”Œë¦½
            A.HorizontalFlip(p=aug_config["fliplr"]),
            A.VerticalFlip(p=aug_config["flipud"]),
            
            # í’ˆì§ˆ ê°œì„ 
            A.OneOf([
                A.GaussNoise(var_limit=(10, 50), p=0.5),
                A.ISONoise(color_shift=(0.01, 0.05), p=0.5),
            ], p=0.3),
            
            A.OneOf([
                A.Blur(blur_limit=3, p=0.5),
                A.MotionBlur(blur_limit=3, p=0.5),
            ], p=0.2),
            
            A.RandomBrightnessContrast(
                brightness_limit=0.2,
                contrast_limit=0.2,
                p=0.5
            ),
            
            # ì •ê·œí™” ë° í…ì„œ ë³€í™˜
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ]
        
        return A.Compose(transforms, bbox_params=A.BboxParams(
            format='yolo',
            label_fields=['class_labels']
        ))
    
    def get_validation_transforms(self, image_size: int = 640) -> A.Compose:
        """ê²€ì¦ìš© ë³€í™˜ (ì¦ê°• ì—†ìŒ)"""
        transforms = [
            A.LongestMaxSize(max_size=image_size),
            A.PadIfNeeded(
                min_height=image_size,
                min_width=image_size, 
                border_mode=cv2.BORDER_CONSTANT,
                value=0
            ),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ]
        
        return A.Compose(transforms, bbox_params=A.BboxParams(
            format='yolo',
            label_fields=['class_labels']  
        ))

def export_dataset_yaml(dataset_type: DatasetType, output_path: Path, 
                       splits: Dict[str, List[ImageInfo]]) -> Path:
    """YOLO í˜•ì‹ dataset.yaml íŒŒì¼ ìƒì„±"""
    
    dataset_config = CONFIG.get_dataset_config(dataset_type)
    
    yaml_content = {
        'path': str(output_path.absolute()),
        'train': 'images/train',
        'val': 'images/val', 
        'test': 'images/test',
        'nc': len(dataset_config["classes"]),
        'names': dataset_config["classes"],
        
        # ë©”íƒ€ë°ì´í„°
        'description': f'Nong-View Best Performance - {dataset_type.value}',
        'version': CONFIG.version,
        'created': pd.Timestamp.now().isoformat(),
        
        # í†µê³„
        'dataset_stats': {
            'total_images': sum(len(split) for split in splits.values()),
            'train_images': len(splits.get('train', [])),
            'val_images': len(splits.get('val', [])),
            'test_images': len(splits.get('test', [])),
        }
    }
    
    yaml_path = output_path / 'dataset.yaml'
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"dataset.yaml ìƒì„± ì™„ë£Œ: {yaml_path}")
    return yaml_path

def save_dataset_stats(stats: DatasetStats, output_path: Path) -> Path:
    """ë°ì´í„°ì…‹ í†µê³„ ì €ì¥"""
    stats_path = output_path / 'dataset_stats.json'
    
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(asdict(stats), f, indent=2, ensure_ascii=False)
    
    logger.info(f"ë°ì´í„°ì…‹ í†µê³„ ì €ì¥ ì™„ë£Œ: {stats_path}")
    return stats_path

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ”§ Data Utils í…ŒìŠ¤íŠ¸")
    
    # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ê°„ë‹¨í•œ ìŠ¤ìº” í…ŒìŠ¤íŠ¸
    for dataset_type in DatasetType:
        try:
            processor = DatasetProcessor(dataset_type)
            print(f"\nğŸ“Š {dataset_type.value} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸
            if processor.dataset_path.exists():
                print(f"  âœ… ê²½ë¡œ í™•ì¸: {processor.dataset_path}")
                
                # ê°„ë‹¨í•œ ìŠ¤ìº” í…ŒìŠ¤íŠ¸
                image_infos, stats = processor.scan_dataset()
                print(f"  ğŸ“ˆ ì´ë¯¸ì§€: {stats.total_images}ê°œ")
                print(f"  ğŸ·ï¸ ë¼ë²¨: {stats.total_labels}ê°œ")
                print(f"  ğŸ¯ í´ë˜ìŠ¤: {list(stats.class_distribution.keys())}")
                print(f"  â­ í‰ê·  í’ˆì§ˆ: {stats.quality_metrics['mean_quality']:.3f}")
            else:
                print(f"  âŒ ê²½ë¡œ ì—†ìŒ: {processor.dataset_path}")
                
        except Exception as e:
            print(f"  âŒ ì—ëŸ¬: {e}")
    
    print("\nâœ… Data Utils í…ŒìŠ¤íŠ¸ ì™„ë£Œ")